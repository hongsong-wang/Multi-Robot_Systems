<!DOCTYPE html>
<html>
<head>
<title>Paper collected by Wang</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<style type="text/css">


/* BODY
=============================================================================*/

body {
  font-family: Helvetica, arial, freesans, clean, sans-serif;
  font-size: 14px;
  line-height: 1.6;
  color: #333;
  background-color: #fff;
  padding: 20px;
  max-width: 960px;
  margin: 0 auto;
}

span#pid {
  color:red;
  
}
span#filename{
  font-style: oblique;
}

span#title{
  font-family: Times New Roman, freesans, clean, sans-serif;
  font-style: italic;
  font-size: 20px;
  border:1px solid #B50;
}
span#abs{
  font-family: Times New Roman, freesans, clean, sans-serif;
  font-style: oblique;
  font-size: 18px;
}
</style>
</head>
<body>

</p></br></br><div id='section'>Paperid: <span id='pid'>1, <a href='https://arxiv.org/pdf/2510.10379.pdf' target='_blank'>https://arxiv.org/pdf/2510.10379.pdf</a></span>   <span><a href='https://github.com/therohangupta/robot-fleet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Rohan Gupta, Trevor Asbery, Zain Merchant, Abrar Anwar, Jesse Thomason
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.10379">RobotFleet: An Open-Source Framework for Centralized Multi-Robot Task Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Coordinating heterogeneous robot fleets to achieve multiple goals is challenging in multi-robot systems. We introduce an open-source and extensible framework for centralized multi-robot task planning and scheduling that leverages LLMs to enable fleets of heterogeneous robots to accomplish multiple tasks. RobotFleet provides abstractions for planning, scheduling, and execution across robots deployed as containerized services to simplify fleet scaling and management. The framework maintains a shared declarative world state and two-way communication for task execution and replanning. By modularizing each layer of the autonomy stack and using LLMs for open-world reasoning, RobotFleet lowers the barrier to building scalable multi-robot systems. The code can be found here: https://github.com/therohangupta/robot-fleet.
<div id='section'>Paperid: <span id='pid'>2, <a href='https://arxiv.org/pdf/2509.18053.pdf' target='_blank'>https://arxiv.org/pdf/2509.18053.pdf</a></span>   <span><a href='https://eddyhkchiu.github.io/v2vgot.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hsu-kuang Chiu, Ryo Hachiuma, Chien-Yi Wang, Yu-Chiang Frank Wang, Min-Hung Chen, Stephen F. Smith
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.18053">V2V-GoT: Vehicle-to-Vehicle Cooperative Autonomous Driving with Multimodal Large Language Models and Graph-of-Thoughts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current state-of-the-art autonomous vehicles could face safety-critical situations when their local sensors are occluded by large nearby objects on the road. Vehicle-to-vehicle (V2V) cooperative autonomous driving has been proposed as a means of addressing this problem, and one recently introduced framework for cooperative autonomous driving has further adopted an approach that incorporates a Multimodal Large Language Model (MLLM) to integrate cooperative perception and planning processes. However, despite the potential benefit of applying graph-of-thoughts reasoning to the MLLM, this idea has not been considered by previous cooperative autonomous driving research. In this paper, we propose a novel graph-of-thoughts framework specifically designed for MLLM-based cooperative autonomous driving. Our graph-of-thoughts includes our proposed novel ideas of occlusion-aware perception and planning-aware prediction. We curate the V2V-GoT-QA dataset and develop the V2V-GoT model for training and testing the cooperative driving graph-of-thoughts. Our experimental results show that our method outperforms other baselines in cooperative perception, prediction, and planning tasks. Our project website: https://eddyhkchiu.github.io/v2vgot.github.io/ .
<div id='section'>Paperid: <span id='pid'>3, <a href='https://arxiv.org/pdf/2509.18053.pdf' target='_blank'>https://arxiv.org/pdf/2509.18053.pdf</a></span>   <span><a href='https://eddyhkchiu.github.io/v2vgot.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hsu-kuang Chiu, Ryo Hachiuma, Chien-Yi Wang, Yu-Chiang Frank Wang, Min-Hung Chen, Stephen F. Smith
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.18053">V2V-GoT: Vehicle-to-Vehicle Cooperative Autonomous Driving with Multimodal Large Language Models and Graph-of-Thoughts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current state-of-the-art autonomous vehicles could face safety-critical situations when their local sensors are occluded by large nearby objects on the road. Vehicle-to-vehicle (V2V) cooperative autonomous driving has been proposed as a means of addressing this problem, and one recently introduced framework for cooperative autonomous driving has further adopted an approach that incorporates a Multimodal Large Language Model (MLLM) to integrate cooperative perception and planning processes. However, despite the potential benefit of applying graph-of-thoughts reasoning to the MLLM, this idea has not been considered by previous cooperative autonomous driving research. In this paper, we propose a novel graph-of-thoughts framework specifically designed for MLLM-based cooperative autonomous driving. Our graph-of-thoughts includes our proposed novel ideas of occlusion-aware perception and planning-aware prediction. We curate the V2V-GoT-QA dataset and develop the V2V-GoT model for training and testing the cooperative driving graph-of-thoughts. Our experimental results show that our method outperforms other baselines in cooperative perception, prediction, and planning tasks. Our project website: https://eddyhkchiu.github.io/v2vgot.github.io/ .
<div id='section'>Paperid: <span id='pid'>4, <a href='https://arxiv.org/pdf/2509.03704.pdf' target='_blank'>https://arxiv.org/pdf/2509.03704.pdf</a></span>   <span><a href='https://github.com/ucla-mobility/QuantV2X' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Seth Z. Zhao, Huizhi Zhang, Zhaowei Li, Juntong Peng, Anthony Chui, Zewei Zhou, Zonglin Meng, Hao Xiang, Zhiyu Huang, Fujia Wang, Ran Tian, Chenfeng Xu, Bolei Zhou, Jiaqi Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03704">QuantV2X: A Fully Quantized Multi-Agent System for Cooperative Perception</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cooperative perception through Vehicle-to-Everything (V2X) communication offers significant potential for enhancing vehicle perception by mitigating occlusions and expanding the field of view. However, past research has predominantly focused on improving accuracy metrics without addressing the crucial system-level considerations of efficiency, latency, and real-world deployability. Noticeably, most existing systems rely on full-precision models, which incur high computational and transmission costs, making them impractical for real-time operation in resource-constrained environments. In this paper, we introduce \textbf{QuantV2X}, the first fully quantized multi-agent system designed specifically for efficient and scalable deployment of multi-modal, multi-agent V2X cooperative perception. QuantV2X introduces a unified end-to-end quantization strategy across both neural network models and transmitted message representations that simultaneously reduces computational load and transmission bandwidth. Remarkably, despite operating under low-bit constraints, QuantV2X achieves accuracy comparable to full-precision systems. More importantly, when evaluated under deployment-oriented metrics, QuantV2X reduces system-level latency by 3.2$\times$ and achieves a +9.5 improvement in mAP30 over full-precision baselines. Furthermore, QuantV2X scales more effectively, enabling larger and more capable models to fit within strict memory budgets. These results highlight the viability of a fully quantized multi-agent intermediate fusion system for real-world deployment. The system will be publicly released to promote research in this field: https://github.com/ucla-mobility/QuantV2X.
<div id='section'>Paperid: <span id='pid'>5, <a href='https://arxiv.org/pdf/2509.03704.pdf' target='_blank'>https://arxiv.org/pdf/2509.03704.pdf</a></span>   <span><a href='https://github.com/ucla-mobility/QuantV2X' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Seth Z. Zhao, Huizhi Zhang, Zhaowei Li, Juntong Peng, Anthony Chui, Zewei Zhou, Zonglin Meng, Hao Xiang, Zhiyu Huang, Fujia Wang, Ran Tian, Chenfeng Xu, Bolei Zhou, Jiaqi Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03704">QuantV2X: A Fully Quantized Multi-Agent System for Cooperative Perception</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cooperative perception through Vehicle-to-Everything (V2X) communication offers significant potential for enhancing vehicle perception by mitigating occlusions and expanding the field of view. However, past research has predominantly focused on improving accuracy metrics without addressing the crucial system-level considerations of efficiency, latency, and real-world deployability. Noticeably, most existing systems rely on full-precision models, which incur high computational and transmission costs, making them impractical for real-time operation in resource-constrained environments. In this paper, we introduce \textbf{QuantV2X}, the first fully quantized multi-agent system designed specifically for efficient and scalable deployment of multi-modal, multi-agent V2X cooperative perception. QuantV2X introduces a unified end-to-end quantization strategy across both neural network models and transmitted message representations that simultaneously reduces computational load and transmission bandwidth. Remarkably, despite operating under low-bit constraints, QuantV2X achieves accuracy comparable to full-precision systems. More importantly, when evaluated under deployment-oriented metrics, QuantV2X reduces system-level latency by 3.2$\times$ and achieves a +9.5 improvement in mAP30 over full-precision baselines. Furthermore, QuantV2X scales more effectively, enabling larger and more capable models to fit within strict memory budgets. These results highlight the viability of a fully quantized multi-agent intermediate fusion system for real-world deployment. The system will be publicly released to promote research in this field: https://github.com/ucla-mobility/QuantV2X.
<div id='section'>Paperid: <span id='pid'>6, <a href='https://arxiv.org/pdf/2508.14387.pdf' target='_blank'>https://arxiv.org/pdf/2508.14387.pdf</a></span>   <span><a href='https://tcxm.github.io/DEXTER-LLM/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxiao Zhu, Junfeng Chen, Xintong Zhang, Meng Guo, Zhongkui Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.14387">DEXTER-LLM: Dynamic and Explainable Coordination of Multi-Robot Systems in Unknown Environments via Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Online coordination of multi-robot systems in open and unknown environments faces significant challenges, particularly when semantic features detected during operation dynamically trigger new tasks. Recent large language model (LLMs)-based approaches for scene reasoning and planning primarily focus on one-shot, end-to-end solutions in known environments, lacking both dynamic adaptation capabilities for online operation and explainability in the processes of planning. To address these issues, a novel framework (DEXTER-LLM) for dynamic task planning in unknown environments, integrates four modules: (i) a mission comprehension module that resolves partial ordering of tasks specified by natural languages or linear temporal logic formulas (LTL); (ii) an online subtask generator based on LLMs that improves the accuracy and explainability of task decomposition via multi-stage reasoning; (iii) an optimal subtask assigner and scheduler that allocates subtasks to robots via search-based optimization; and (iv) a dynamic adaptation and human-in-the-loop verification module that implements multi-rate, event-based updates for both subtasks and their assignments, to cope with new features and tasks detected online. The framework effectively combines LLMs' open-world reasoning capabilities with the optimality of model-based assignment methods, simultaneously addressing the critical issue of online adaptability and explainability. Experimental evaluations demonstrate exceptional performances, with 100% success rates across all scenarios, 160 tasks and 480 subtasks completed on average (3 times the baselines), 62% less queries to LLMs during adaptation, and superior plan quality (2 times higher) for compound tasks. Project page at https://tcxm.github.io/DEXTER-LLM/
<div id='section'>Paperid: <span id='pid'>7, <a href='https://arxiv.org/pdf/2508.07657.pdf' target='_blank'>https://arxiv.org/pdf/2508.07657.pdf</a></span>   <span><a href='https://zl-tian.github.io/MoRoCo/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhuoli Tian, Yuyang Zhang, Jinsheng Wei, Meng Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.07657">MoRoCo: Multi-operator-robot Coordination, Interaction and Exploration under Restricted Communication</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fleets of autonomous robots are increasingly deployed alongside multiple human operators to explore unknown environments, identify salient features, and perform complex tasks in scenarios such as subterranean exploration, reconnaissance, and search-and-rescue missions. In these contexts, communication is often severely limited to short-range exchanges via ad-hoc networks, posing challenges to coordination. While recent studies have addressed multi-robot exploration under communication constraints, they largely overlook the essential role of human operators and their real-time interaction with robotic teams. Operators may demand timely updates on the exploration progress and robot status, reprioritize or cancel tasks dynamically, or request live video feeds and control access. Conversely, robots may seek human confirmation for anomalous events or require help recovering from motion or planning failures. To enable such bilateral, context-aware interactions under restricted communication, this work proposes MoRoCo, a unified framework for online coordination and exploration in multi-operator, multi-robot systems. MoRoCo enables the team to adaptively switch among three coordination modes: spread mode for parallelized exploration with intermittent data sharing, migrate mode for coordinated relocation, and chain mode for maintaining high-bandwidth connectivity through multi-hop links. These transitions are managed through distributed algorithms via only local communication. Extensive large-scale human-in-the-loop simulations and hardware experiments validate the necessity of incorporating human robot interactions and demonstrate that MoRoCo enables efficient, reliable coordination under limited communication, marking a significant step toward robust human-in-the-loop multi-robot autonomy in challenging environments.
<div id='section'>Paperid: <span id='pid'>8, <a href='https://arxiv.org/pdf/2507.19239.pdf' target='_blank'>https://arxiv.org/pdf/2507.19239.pdf</a></span>   <span><a href='https://github.com/zhongjiaru/CoopTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaru Zhong, Jiahao Wang, Jiahui Xu, Xiaofan Li, Zaiqing Nie, Haibao Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.19239">CoopTrack: Exploring End-to-End Learning for Efficient Cooperative Sequential Perception</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cooperative perception aims to address the inherent limitations of single-vehicle autonomous driving systems through information exchange among multiple agents. Previous research has primarily focused on single-frame perception tasks. However, the more challenging cooperative sequential perception tasks, such as cooperative 3D multi-object tracking, have not been thoroughly investigated. Therefore, we propose CoopTrack, a fully instance-level end-to-end framework for cooperative tracking, featuring learnable instance association, which fundamentally differs from existing approaches. CoopTrack transmits sparse instance-level features that significantly enhance perception capabilities while maintaining low transmission costs. Furthermore, the framework comprises two key components: Multi-Dimensional Feature Extraction, and Cross-Agent Association and Aggregation, which collectively enable comprehensive instance representation with semantic and motion features, and adaptive cross-agent association and fusion based on a feature graph. Experiments on both the V2X-Seq and Griffin datasets demonstrate that CoopTrack achieves excellent performance. Specifically, it attains state-of-the-art results on V2X-Seq, with 39.0\% mAP and 32.8\% AMOTA. The project is available at https://github.com/zhongjiaru/CoopTrack.
<div id='section'>Paperid: <span id='pid'>9, <a href='https://arxiv.org/pdf/2507.17519.pdf' target='_blank'>https://arxiv.org/pdf/2507.17519.pdf</a></span>   <span><a href='https://github.com/konskara/TerraPlan' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kostas Karakontis, Thanos Petsanis, Athanasios Ch. Kapoutsis, Pavlos Ch. Kapoutsis, Elias B. Kosmatopoulos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.17519">Terrain-Aware Adaptation for Two-Dimensional UAV Path Planners</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-UAV Coverage Path Planning (mCPP) algorithms in popular commercial software typically treat a Region of Interest (RoI) only as a 2D plane, ignoring important3D structure characteristics. This leads to incomplete 3Dreconstructions, especially around occluded or vertical surfaces. In this paper, we propose a modular algorithm that can extend commercial two-dimensional path planners to facilitate terrain-aware planning by adjusting altitude and camera orientations. To demonstrate it, we extend the well-known DARP (Divide Areas for Optimal Multi-Robot Coverage Path Planning) algorithm and produce DARP-3D. We present simulation results in multiple 3D environments and a real-world flight test using DJI hardware. Compared to baseline, our approach consistently captures improved 3D reconstructions, particularly in areas with significant vertical features. An open-source implementation of the algorithm is available here:https://github.com/konskara/TerraPlan
<div id='section'>Paperid: <span id='pid'>10, <a href='https://arxiv.org/pdf/2507.17130.pdf' target='_blank'>https://arxiv.org/pdf/2507.17130.pdf</a></span>   <span><a href='https://github.com/sparolab/MARSCalib' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Seokhwan Jeong, Hogyun Kim, Younggun Cho
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.17130">MARSCalib: Multi-robot, Automatic, Robust, Spherical Target-based Extrinsic Calibration in Field and Extraterrestrial Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a novel spherical target-based LiDAR-camera extrinsic calibration method designed for outdoor environments with multi-robot systems, considering both target and sensor corruption. The method extracts the 2D ellipse center from the image and the 3D sphere center from the pointcloud, which are then paired to compute the transformation matrix. Specifically, the image is first decomposed using the Segment Anything Model (SAM). Then, a novel algorithm extracts an ellipse from a potentially corrupted sphere, and the extracted center of ellipse is corrected for errors caused by the perspective projection model. For the LiDAR pointcloud, points on the sphere tend to be highly noisy due to the absence of flat regions. To accurately extract the sphere from these noisy measurements, we apply a hierarchical weighted sum to the accumulated pointcloud. Through experiments, we demonstrated that the sphere can be robustly detected even under both types of corruption, outperforming other targets. We evaluated our method using three different types of LiDARs (spinning, solid-state, and non-repetitive) with cameras positioned in three different locations. Furthermore, we validated the robustness of our method to target corruption by experimenting with spheres subjected to various types of degradation. These experiments were conducted in both a planetary test and a field environment. Our code is available at https://github.com/sparolab/MARSCalib.
<div id='section'>Paperid: <span id='pid'>11, <a href='https://arxiv.org/pdf/2506.23514.pdf' target='_blank'>https://arxiv.org/pdf/2506.23514.pdf</a></span>   <span><a href='https://github.com/herolab-uga/MGPRL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sai Krishna Ghanta, Ramviyas Parasuraman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.23514">MGPRL: Distributed Multi-Gaussian Processes for Wi-Fi-based Multi-Robot Relative Localization in Large Indoor Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Relative localization is a crucial capability for multi-robot systems operating in GPS-denied environments. Existing approaches for multi-robot relative localization often depend on costly or short-range sensors like cameras and LiDARs. Consequently, these approaches face challenges such as high computational overhead (e.g., map merging) and difficulties in disjoint environments. To address this limitation, this paper introduces MGPRL, a novel distributed framework for multi-robot relative localization using convex-hull of multiple Wi-Fi access points (AP). To accomplish this, we employ co-regionalized multi-output Gaussian Processes for efficient Radio Signal Strength Indicator (RSSI) field prediction and perform uncertainty-aware multi-AP localization, which is further coupled with weighted convex hull-based alignment for robust relative pose estimation. Each robot predicts the RSSI field of the environment by an online scan of APs in its environment, which are utilized for position estimation of multiple APs. To perform relative localization, each robot aligns the convex hull of its predicted AP locations with that of the neighbor robots. This approach is well-suited for devices with limited computational resources and operates solely on widely available Wi-Fi RSSI measurements without necessitating any dedicated pre-calibration or offline fingerprinting. We rigorously evaluate the performance of the proposed MGPRL in ROS simulations and demonstrate it with real-world experiments, comparing it against multiple state-of-the-art approaches. The results showcase that MGPRL outperforms existing methods in terms of localization accuracy and computational efficiency. Finally, we open source MGPRL as a ROS package https://github.com/herolab-uga/MGPRL.
<div id='section'>Paperid: <span id='pid'>12, <a href='https://arxiv.org/pdf/2506.01538.pdf' target='_blank'>https://arxiv.org/pdf/2506.01538.pdf</a></span>   <span><a href='https://windylab.github.io/LAMARL/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Guobin Zhu, Rui Zhou, Wenkang Ji, Shiyu Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.01538">LAMARL: LLM-Aided Multi-Agent Reinforcement Learning for Cooperative Policy Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Although Multi-Agent Reinforcement Learning (MARL) is effective for complex multi-robot tasks, it suffers from low sample efficiency and requires iterative manual reward tuning. Large Language Models (LLMs) have shown promise in single-robot settings, but their application in multi-robot systems remains largely unexplored. This paper introduces a novel LLM-Aided MARL (LAMARL) approach, which integrates MARL with LLMs, significantly enhancing sample efficiency without requiring manual design. LAMARL consists of two modules: the first module leverages LLMs to fully automate the generation of prior policy and reward functions. The second module is MARL, which uses the generated functions to guide robot policy training effectively. On a shape assembly benchmark, both simulation and real-world experiments demonstrate the unique advantages of LAMARL. Ablation studies show that the prior policy improves sample efficiency by an average of 185.9% and enhances task completion, while structured prompts based on Chain-of-Thought (CoT) and basic APIs improve LLM output success rates by 28.5%-67.5%. Videos and code are available at https://windylab.github.io/LAMARL/
<div id='section'>Paperid: <span id='pid'>13, <a href='https://arxiv.org/pdf/2505.17576.pdf' target='_blank'>https://arxiv.org/pdf/2505.17576.pdf</a></span>   <span><a href='https://arpg.github.io/cumulti' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Doncey Albin, Miles Mena, Annika Thomas, Harel Biggie, Xuefei Sun, Dusty Woods, Steve McGuire, Christoffer Heckman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.17576">CU-Multi: A Dataset for Multi-Robot Data Association</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-robot systems (MRSs) are valuable for tasks such as search and rescue due to their ability to coordinate over shared observations. A central challenge in these systems is aligning independently collected perception data across space and time, i.e., multi-robot data association. While recent advances in collaborative SLAM (C-SLAM), map merging, and inter-robot loop closure detection have significantly progressed the field, evaluation strategies still predominantly rely on splitting a single trajectory from single-robot SLAM datasets into multiple segments to simulate multiple robots. Without careful consideration to how a single trajectory is split, this approach will fail to capture realistic pose-dependent variation in observations of a scene inherent to multi-robot systems. To address this gap, we present CU-Multi, a multi-robot dataset collected over multiple days at two locations on the University of Colorado Boulder campus. Using a single robotic platform, we generate four synchronized runs with aligned start times and deliberate percentages of trajectory overlap. CU-Multi includes RGB-D, GPS with accurate geospatial heading, and semantically annotated LiDAR data. By introducing controlled variations in trajectory overlap and dense lidar annotations, CU-Multi offers a compelling alternative for evaluating methods in multi-robot data association. Instructions on accessing the dataset, support code, and the latest updates are publicly available at https://arpg.github.io/cumulti
<div id='section'>Paperid: <span id='pid'>14, <a href='https://arxiv.org/pdf/2505.08230.pdf' target='_blank'>https://arxiv.org/pdf/2505.08230.pdf</a></span>   <span><a href='https://sparolab.github.io/research/skid_slam/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hogyun Kim, Jiwon Choi, Juwon Kim, Geonmo Yang, Dongjin Cho, Hyungtae Lim, Younggun Cho
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.08230">SKiD-SLAM: Robust, Lightweight, and Distributed Multi-Robot LiDAR SLAM in Resource-Constrained Field Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Distributed LiDAR SLAM is crucial for achieving efficient robot autonomy and improving the scalability of mapping. However, two issues need to be considered when applying it in field environments: one is resource limitation, and the other is inter/intra-robot association. The resource limitation issue arises when the data size exceeds the processing capacity of the network or memory, especially when utilizing communication systems or onboard computers in the field. The inter/intra-robot association issue occurs due to the narrow convergence region of ICP under large viewpoint differences, triggering many false positive loops and ultimately resulting in an inconsistent global map for multi-robot systems. To tackle these problems, we propose a distributed LiDAR SLAM framework designed for versatile field applications, called SKiD-SLAM. Extending our previous work that solely focused on lightweight place recognition and fast and robust global registration, we present a multi-robot mapping framework that focuses on robust and lightweight inter-robot loop closure in distributed LiDAR SLAM. Through various environmental experiments, we demonstrate that our method is more robust and lightweight compared to other state-of-the-art distributed SLAM approaches, overcoming resource limitation and inter/intra-robot association issues. Also, we validated the field applicability of our approach through mapping experiments in real-world planetary emulation terrain and cave environments, which are in-house datasets. Our code will be available at https://sparolab.github.io/research/skid_slam/.
<div id='section'>Paperid: <span id='pid'>15, <a href='https://arxiv.org/pdf/2505.06771.pdf' target='_blank'>https://arxiv.org/pdf/2505.06771.pdf</a></span>   <span><a href='https://github.com/GT-STAR-Lab/JaxRobotarium' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/GT-STAR-Lab/JaxRobotarium' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shalin Anand Jain, Jiazhen Liu, Siva Kailas, Harish Ravichandar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.06771">JaxRobotarium: Training and Deploying Multi-Robot Policies in 10 Minutes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning (MARL) has emerged as a promising solution for learning complex and scalable coordination behaviors in multi-robot systems. However, established MARL platforms (e.g., SMAC and MPE) lack robotics relevance and hardware deployment, leaving multi-robot learning researchers to develop bespoke environments and hardware testbeds dedicated to the development and evaluation of their individual contributions. The Multi-Agent RL Benchmark and Learning Environment for the Robotarium (MARBLER) is an exciting recent step in providing a standardized robotics-relevant platform for MARL, by bridging the Robotarium testbed with existing MARL software infrastructure. However, MARBLER lacks support for parallelization and GPU/TPU execution, making the platform prohibitively slow compared to modern MARL environments and hindering adoption. We contribute JaxRobotarium, a Jax-powered end-to-end simulation, learning, deployment, and benchmarking platform for the Robotarium. JaxRobotarium enables rapid training and deployment of multi-robot RL (MRRL) policies with realistic robot dynamics and safety constraints, supporting parallelization and hardware acceleration. Our generalizable learning interface integrates easily with SOTA MARL libraries (e.g., JaxMARL). In addition, JaxRobotarium includes eight standardized coordination scenarios, including four novel scenarios that bring established MARL benchmark tasks (e.g., RWARE and Level-Based Foraging) to a robotics setting. We demonstrate that JaxRobotarium retains high simulation fidelity while achieving dramatic speedups over baseline (20x in training and 150x in simulation), and provides an open-access sim-to-real evaluation pipeline through the Robotarium testbed, accelerating and democratizing access to multi-robot learning research and evaluation. Our code is available at https://github.com/GT-STAR-Lab/JaxRobotarium.
<div id='section'>Paperid: <span id='pid'>16, <a href='https://arxiv.org/pdf/2503.23875.pdf' target='_blank'>https://arxiv.org/pdf/2503.23875.pdf</a></span>   <span><a href='https://github.com/WindyLab/GenSwarm' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenkang Ji, Huaben Chen, Mingyang Chen, Guobin Zhu, Lufeng Xu, Roderich GroÃ, Rui Zhou, Ming Cao, Shiyu Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.23875">GenSwarm: Scalable Multi-Robot Code-Policy Generation and Deployment via Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The development of control policies for multi-robot systems traditionally follows a complex and labor-intensive process, often lacking the flexibility to adapt to dynamic tasks. This has motivated research on methods to automatically create control policies. However, these methods require iterative processes of manually crafting and refining objective functions, thereby prolonging the development cycle. This work introduces \textit{GenSwarm}, an end-to-end system that leverages large language models to automatically generate and deploy control policies for multi-robot tasks based on simple user instructions in natural language. As a multi-language-agent system, GenSwarm achieves zero-shot learning, enabling rapid adaptation to altered or unseen tasks. The white-box nature of the code policies ensures strong reproducibility and interpretability. With its scalable software and hardware architectures, GenSwarm supports efficient policy deployment on both simulated and real-world multi-robot systems, realizing an instruction-to-execution end-to-end functionality that could prove valuable for robotics specialists and non-specialists alike.The code of the proposed GenSwarm system is available online: https://github.com/WindyLab/GenSwarm.
<div id='section'>Paperid: <span id='pid'>17, <a href='https://arxiv.org/pdf/2503.12395.pdf' target='_blank'>https://arxiv.org/pdf/2503.12395.pdf</a></span>   <span><a href='https://github.com/ApricityZ/TERL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Heng Zhang, Guoxiang Zhao, Xiaoqiang Ren
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.12395">TERL: Large-Scale Multi-Target Encirclement Using Transformer-Enhanced Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pursuit-evasion (PE) problem is a critical challenge in multi-robot systems (MRS). While reinforcement learning (RL) has shown its promise in addressing PE tasks, research has primarily focused on single-target pursuit, with limited exploration of multi-target encirclement, particularly in large-scale settings. This paper proposes a Transformer-Enhanced Reinforcement Learning (TERL) framework for large-scale multi-target encirclement. By integrating a transformer-based policy network with target selection, TERL enables robots to adaptively prioritize targets and safely coordinate robots. Results show that TERL outperforms existing RL-based methods in terms of encirclement success rate and task completion time, while maintaining good performance in large-scale scenarios. Notably, TERL, trained on small-scale scenarios (15 pursuers, 4 targets), generalizes effectively to large-scale settings (80 pursuers, 20 targets) without retraining, achieving a 100% success rate. The code and demonstration video are available at https://github.com/ApricityZ/TERL.
<div id='section'>Paperid: <span id='pid'>18, <a href='https://arxiv.org/pdf/2503.12122.pdf' target='_blank'>https://arxiv.org/pdf/2503.12122.pdf</a></span>   <span><a href='https://yanoyoshiki.github.io/ICCO/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yoshiki Yano, Kazuki Shibata, Maarten Kokshoorn, Takamitsu Matsubara
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.12122">ICCO: Learning an Instruction-conditioned Coordinator for Language-guided Task-aligned Multi-robot Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in Large Language Models (LLMs) have permitted the development of language-guided multi-robot systems, which allow robots to execute tasks based on natural language instructions. However, achieving effective coordination in distributed multi-agent environments remains challenging due to (1) misalignment between instructions and task requirements and (2) inconsistency in robot behaviors when they independently interpret ambiguous instructions. To address these challenges, we propose Instruction-Conditioned Coordinator (ICCO), a Multi-Agent Reinforcement Learning (MARL) framework designed to enhance coordination in language-guided multi-robot systems. ICCO consists of a Coordinator agent and multiple Local Agents, where the Coordinator generates Task-Aligned and Consistent Instructions (TACI) by integrating language instructions with environmental states, ensuring task alignment and behavioral consistency. The Coordinator and Local Agents are jointly trained to optimize a reward function that balances task efficiency and instruction following. A Consistency Enhancement Term is added to the learning objective to maximize mutual information between instructions and robot behaviors, further improving coordination. Simulation and real-world experiments validate the effectiveness of ICCO in achieving language-guided task-aligned multi-robot control. The demonstration can be found at https://yanoyoshiki.github.io/ICCO/.
<div id='section'>Paperid: <span id='pid'>19, <a href='https://arxiv.org/pdf/2503.11461.pdf' target='_blank'>https://arxiv.org/pdf/2503.11461.pdf</a></span>   <span><a href='https://wyd0817.github.io/project-mrs-cwc/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Runze Xiao, Yongdong Wang, Yusuke Tsunoda, Koichi Osuka, Hajime Asama
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.11461">MRS-CWC: A Weakly Constrained Multi-Robot System with Controllable Constraint Stiffness for Mobility and Navigation in Unknown 3D Rough Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Navigating unknown three-dimensional (3D) rugged environments is challenging for multi-robot systems. Traditional discrete systems struggle with rough terrain due to limited individual mobility, while modular systems--where rigid, controllable constraints link robot units--improve traversal but suffer from high control complexity and reduced flexibility. To address these limitations, we propose the Multi-Robot System with Controllable Weak Constraints (MRS-CWC), where robot units are connected by constraints with dynamically adjustable stiffness. This adaptive mechanism softens or stiffens in real-time during environmental interactions, ensuring a balance between flexibility and mobility. We formulate the system's dynamics and control model and evaluate MRS-CWC against six baseline methods and an ablation variant in a benchmark dataset with 100 different simulation terrains. Results show that MRS-CWC achieves the highest navigation completion rate and ranks second in success rate, efficiency, and energy cost in the highly rugged terrain group, outperforming all baseline methods without relying on environmental modeling, path planning, or complex control. Even where MRS-CWC ranks second, its performance is only slightly behind a more complex ablation variant with environmental modeling and path planning. Finally, we develop a physical prototype and validate its feasibility in a constructed rugged environment. For videos, simulation benchmarks, and code, please visit https://wyd0817.github.io/project-mrs-cwc/.
<div id='section'>Paperid: <span id='pid'>20, <a href='https://arxiv.org/pdf/2503.06983.pdf' target='_blank'>https://arxiv.org/pdf/2503.06983.pdf</a></span>   <span><a href='https://github.com/wang-jh18-SVM/Griffin' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiahao Wang, Xiangyu Cao, Jiaru Zhong, Yuner Zhang, Haibao Yu, Lei He, Shaobing Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.06983">Griffin: Aerial-Ground Cooperative Detection and Tracking Dataset and Benchmark</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite significant advancements, autonomous driving systems continue to struggle with occluded objects and long-range detection due to the inherent limitations of single-perspective sensing. Aerial-ground cooperation offers a promising solution by integrating UAVs' aerial views with ground vehicles' local observations. However, progress in this emerging field has been hindered by the absence of public datasets and standardized evaluation benchmarks. To address this gap, this paper presents a comprehensive solution for aerial-ground cooperative 3D perception through three key contributions: (1) Griffin, a large-scale multi-modal dataset featuring over 200 dynamic scenes (30k+ frames) with varied UAV altitudes (20-60m), diverse weather conditions, and occlusion-aware 3D annotations, enhanced by CARLA-AirSim co-simulation for realistic UAV dynamics; (2) A unified benchmarking framework for aerial-ground cooperative detection and tracking tasks, including protocols for evaluating communication efficiency, latency tolerance, and altitude adaptability; (3) AGILE, an instance-level intermediate fusion baseline that dynamically aligns cross-view features through query-based interaction, achieving an advantageous balance between communication overhead and perception accuracy. Extensive experiments prove the effectiveness of aerial-ground cooperative perception and demonstrate the direction of further research. The dataset and codes are available at https://github.com/wang-jh18-SVM/Griffin.
<div id='section'>Paperid: <span id='pid'>21, <a href='https://arxiv.org/pdf/2502.09980.pdf' target='_blank'>https://arxiv.org/pdf/2502.09980.pdf</a></span>   <span><a href='https://eddyhkchiu.github.io/v2vllm.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hsu-kuang Chiu, Ryo Hachiuma, Chien-Yi Wang, Stephen F. Smith, Yu-Chiang Frank Wang, Min-Hung Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.09980">V2V-LLM: Vehicle-to-Vehicle Cooperative Autonomous Driving with Multi-Modal Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current autonomous driving vehicles rely mainly on their individual sensors to understand surrounding scenes and plan for future trajectories, which can be unreliable when the sensors are malfunctioning or occluded. To address this problem, cooperative perception methods via vehicle-to-vehicle (V2V) communication have been proposed, but they have tended to focus on perception tasks like detection or tracking. How those approaches contribute to overall cooperative planning performance is still under-explored. Inspired by recent progress using Large Language Models (LLMs) to build autonomous driving systems, we propose a novel problem setting that integrates a Multi-Modal LLM into cooperative autonomous driving, with the proposed Vehicle-to-Vehicle Question-Answering (V2V-QA) dataset and benchmark. We also propose our baseline method Vehicle-to-Vehicle Multi-Modal Large Language Model (V2V-LLM), which uses an LLM to fuse perception information from multiple connected autonomous vehicles (CAVs) and answer various types of driving-related questions: grounding, notable object identification, and planning. Experimental results show that our proposed V2V-LLM can be a promising unified model architecture for performing various tasks in cooperative autonomous driving, and outperforms other baseline methods that use different fusion approaches. Our work also creates a new research direction that can improve the safety of future autonomous driving systems. The code and data will be released to the public to facilitate open-source research in this field. Our project website: https://eddyhkchiu.github.io/v2vllm.github.io/ .
<div id='section'>Paperid: <span id='pid'>22, <a href='https://arxiv.org/pdf/2501.16803.pdf' target='_blank'>https://arxiv.org/pdf/2501.16803.pdf</a></span>   <span><a href='https://github.com/LantaoLi/RG-Attn' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lantao Li, Kang Yang, Wenqi Zhang, Xiaoxue Wang, Chen Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.16803">RG-Attn: Radian Glue Attention for Multi-modality Multi-agent Cooperative Perception</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cooperative perception enhances autonomous driving by leveraging Vehicle-to-Everything (V2X) communication for multi-agent sensor fusion. However, most existing methods rely on single-modal data sharing, limiting fusion performance, particularly in heterogeneous sensor settings involving both LiDAR and cameras across vehicles and roadside units (RSUs). To address this, we propose Radian Glue Attention (RG-Attn), a lightweight and generalizable cross-modal fusion module that unifies intra-agent and inter-agent fusion via transformation-based coordinate alignment and a unified sampling/inversion strategy. RG-Attn efficiently aligns features through a radian-based attention constraint, operating column-wise on geometrically consistent regions to reduce overhead and preserve spatial coherence, thereby enabling accurate and robust fusion. Building upon RG-Attn, we propose three cooperative architectures. The first, Paint-To-Puzzle (PTP), prioritizes communication efficiency but assumes all agents have LiDAR, optionally paired with cameras. The second, Co-Sketching-Co-Coloring (CoS-CoCo), offers maximal flexibility, supporting any sensor setup (e.g., LiDAR-only, camera-only, or both) and enabling strong cross-modal generalization for real-world deployment. The third, Pyramid-RG-Attn Fusion (PRGAF), aims for peak detection accuracy with the highest computational overhead. Extensive evaluations on simulated and real-world datasets show our framework delivers state-of-the-art detection accuracy with high flexibility and efficiency. GitHub Link: https://github.com/LantaoLi/RG-Attn
<div id='section'>Paperid: <span id='pid'>23, <a href='https://arxiv.org/pdf/2501.16803.pdf' target='_blank'>https://arxiv.org/pdf/2501.16803.pdf</a></span>   <span><a href='https://github.com/LantaoLi/RG-Attn' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lantao Li, Kang Yang, Wenqi Zhang, Xiaoxue Wang, Chen Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.16803">RG-Attn: Radian Glue Attention for Multi-modality Multi-agent Cooperative Perception</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cooperative perception enhances autonomous driving by leveraging Vehicle-to-Everything (V2X) communication for multi-agent sensor fusion. However, most existing methods rely on single-modal data sharing, limiting fusion performance, particularly in heterogeneous sensor settings involving both LiDAR and cameras across vehicles and roadside units (RSUs). To address this, we propose Radian Glue Attention (RG-Attn), a lightweight and generalizable cross-modal fusion module that unifies intra-agent and inter-agent fusion via transformation-based coordinate alignment and a unified sampling/inversion strategy. RG-Attn efficiently aligns features through a radian-based attention constraint, operating column-wise on geometrically consistent regions to reduce overhead and preserve spatial coherence, thereby enabling accurate and robust fusion. Building upon RG-Attn, we propose three cooperative architectures. The first, Paint-To-Puzzle (PTP), prioritizes communication efficiency but assumes all agents have LiDAR, optionally paired with cameras. The second, Co-Sketching-Co-Coloring (CoS-CoCo), offers maximal flexibility, supporting any sensor setup (e.g., LiDAR-only, camera-only, or both) and enabling strong cross-modal generalization for real-world deployment. The third, Pyramid-RG-Attn Fusion (PRGAF), aims for peak detection accuracy with the highest computational overhead. Extensive evaluations on simulated and real-world datasets show our framework delivers state-of-the-art detection accuracy with high flexibility and efficiency. GitHub Link: https://github.com/LantaoLi/RG-Attn
<div id='section'>Paperid: <span id='pid'>24, <a href='https://arxiv.org/pdf/2412.18381.pdf' target='_blank'>https://arxiv.org/pdf/2412.18381.pdf</a></span>   <span><a href='https://github.com/efc-robot/MR-COGraphs' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiuyi Gu, Zhaocheng Ye, Jincheng Yu, Jiahao Tang, Tinghao Yi, Yuhan Dong, Jian Wang, Jinqiang Cui, Xinlei Chen, Yu Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.18381">MR-COGraphs: Communication-efficient Multi-Robot Open-vocabulary Mapping System via 3D Scene Graphs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Collaborative perception in unknown environments is crucial for multi-robot systems. With the emergence of foundation models, robots can now not only perceive geometric information but also achieve open-vocabulary scene understanding. However, existing map representations that support open-vocabulary queries often involve large data volumes, which becomes a bottleneck for multi-robot transmission in communication-limited environments. To address this challenge, we develop a method to construct a graph-structured 3D representation called COGraph, where nodes represent objects with semantic features and edges capture their spatial adjacency relationships. Before transmission, a data-driven feature encoder is applied to compress the feature dimensions of the COGraph. Upon receiving COGraphs from other robots, the semantic features of each node are recovered using a decoder. We also propose a feature-based approach for place recognition and translation estimation, enabling the merging of local COGraphs into a unified global map. We validate our framework on two realistic datasets and the real-world environment. The results demonstrate that, compared to existing baselines for open-vocabulary map construction, our framework reduces the data volume by over 80\% while maintaining mapping and query performance without compromise. For more details, please visit our website at https://github.com/efc-robot/MR-COGraphs.
<div id='section'>Paperid: <span id='pid'>25, <a href='https://arxiv.org/pdf/2412.18292.pdf' target='_blank'>https://arxiv.org/pdf/2412.18292.pdf</a></span>   <span><a href='https://github.com/FrankZxShen/MCoCoNav.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhixuan Shen, Haonan Luo, Kexun Chen, Fengmao Lv, Tianrui Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.18292">Enhancing Multi-Robot Semantic Navigation Through Multimodal Chain-of-Thought Score Collaboration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding how humans cooperatively utilize semantic knowledge to explore unfamiliar environments and decide on navigation directions is critical for house service multi-robot systems. Previous methods primarily focused on single-robot centralized planning strategies, which severely limited exploration efficiency. Recent research has considered decentralized planning strategies for multiple robots, assigning separate planning models to each robot, but these approaches often overlook communication costs. In this work, we propose Multimodal Chain-of-Thought Co-Navigation (MCoCoNav), a modular approach that utilizes multimodal Chain-of-Thought to plan collaborative semantic navigation for multiple robots. MCoCoNav combines visual perception with Vision Language Models (VLMs) to evaluate exploration value through probabilistic scoring, thus reducing time costs and achieving stable outputs. Additionally, a global semantic map is used as a communication bridge, minimizing communication overhead while integrating observational results. Guided by scores that reflect exploration trends, robots utilize this map to assess whether to explore new frontier points or revisit history nodes. Experiments on HM3D_v0.2 and MP3D demonstrate the effectiveness of our approach. Our code is available at https://github.com/FrankZxShen/MCoCoNav.git.
<div id='section'>Paperid: <span id='pid'>26, <a href='https://arxiv.org/pdf/2412.09782.pdf' target='_blank'>https://arxiv.org/pdf/2412.09782.pdf</a></span>   <span><a href='https://ucd-dare.github.io/eidrive.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanchu Zhou, Edward Xie, Wei Shao, Dechen Gao, Michelle Dong, Junshan Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.09782">EI-Drive: A Platform for Cooperative Perception with Realistic Communication Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The growing interest in autonomous driving calls for realistic simulation platforms capable of accurately simulating cooperative perception process in realistic traffic scenarios. Existing studies for cooperative perception often have not accounted for transmission latency and errors in real-world environments. To address this gap, we introduce EI-Drive, an edge-AI based autonomous driving simulation platform that integrates advanced cooperative perception with more realistic communication models. Built on the CARLA framework, EI-Drive features new modules for cooperative perception while taking into account transmission latency and errors, providing a more realistic platform for evaluating cooperative perception algorithms. In particular, the platform enables vehicles to fuse data from multiple sources, improving situational awareness and safety in complex environments. With its modular design, EI-Drive allows for detailed exploration of sensing, perception, planning, and control in various cooperative driving scenarios. Experiments using EI-Drive demonstrate significant improvements in vehicle safety and performance, particularly in scenarios with complex traffic flow and network conditions. All code and documents are accessible on our GitHub page: \url{https://ucd-dare.github.io/eidrive.github.io/}.
<div id='section'>Paperid: <span id='pid'>27, <a href='https://arxiv.org/pdf/2411.14775.pdf' target='_blank'>https://arxiv.org/pdf/2411.14775.pdf</a></span>   <span><a href='https://github.com/vision3d-lab/CSE_Dataset' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Harin Park, Inha Lee, Minje Kim, Hyungyu Park, Kyungdon Joo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.14775">A Benchmark Dataset for Collaborative SLAM in Service Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As service environments have become diverse, they have started to demand complicated tasks that are difficult for a single robot to complete. This change has led to an interest in multiple robots instead of a single robot. C-SLAM, as a fundamental technique for multiple service robots, needs to handle diverse challenges such as homogeneous scenes and dynamic objects to ensure that robots operate smoothly and perform their tasks safely. However, existing C-SLAM datasets do not include the various indoor service environments with the aforementioned challenges. To close this gap, we introduce a new multi-modal C-SLAM dataset for multiple service robots in various indoor service environments, called C-SLAM dataset in Service Environments (CSE). We use the NVIDIA Isaac Sim to generate data in various indoor service environments with the challenges that may occur in real-world service environments. By using simulation, we can provide accurate and precisely time-synchronized sensor data, such as stereo RGB, stereo depth, IMU, and ground truth (GT) poses. We configure three common indoor service environments (Hospital, Office, and Warehouse), each of which includes various dynamic objects that perform motions suitable to each environment. In addition, we drive three robots to mimic the actions of real service robots. Through these factors, we generate a more realistic C-SLAM dataset for multiple service robots. We demonstrate our dataset by evaluating diverse state-of-the-art single-robot SLAM and multi-robot SLAM methods. Our dataset is available at https://github.com/vision3d-lab/CSE_Dataset.
<div id='section'>Paperid: <span id='pid'>28, <a href='https://arxiv.org/pdf/2411.13340.pdf' target='_blank'>https://arxiv.org/pdf/2411.13340.pdf</a></span>   <span><a href='https://github.com/chensiweiTHU/WHALES' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yinsong Wang, Siwei Chen, Ziyi Song, Sheng Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.13340">WHALES: A Multi-Agent Scheduling Dataset for Enhanced Cooperation in Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cooperative perception research is hindered by the limited availability of datasets that capture the complexity of real-world Vehicle-to-Everything (V2X) interactions, particularly under dynamic communication constraints. To address this gap, we introduce WHALES (Wireless enhanced Autonomous vehicles with Large number of Engaged agents), the first large-scale V2X dataset explicitly designed to benchmark communication-aware agent scheduling and scalable cooperative perception. WHALES introduces a new benchmark that enables state-of-the-art (SOTA) research in communication-aware cooperative perception, featuring an average of 8.4 cooperative agents per scene and 2.01 million annotated 3D objects across diverse traffic scenarios. It incorporates detailed communication metadata to emulate real-world communication bottlenecks, enabling rigorous evaluation of scheduling strategies. To further advance the field, we propose the Coverage-Aware Historical Scheduler (CAHS), a novel scheduling baseline that selects agents based on historical viewpoint coverage, improving perception performance over existing SOTA methods. WHALES bridges the gap between simulated and real-world V2X challenges, providing a robust framework for exploring perception-scheduling co-design, cross-data generalization, and scalability limits. The WHALES dataset and code are available at https://github.com/chensiweiTHU/WHALES.
<div id='section'>Paperid: <span id='pid'>29, <a href='https://arxiv.org/pdf/2411.10962.pdf' target='_blank'>https://arxiv.org/pdf/2411.10962.pdf</a></span>   <span><a href='https://github.com/yanglei18/V2X-Radar' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lei Yang, Xinyu Zhang, Chen Wang, Jun Li, Jiaqi Ma, Zhiying Song, Tong Zhao, Ziying Song, Li Wang, Mo Zhou, Yang Shen, Kai Wu, Chen Lv
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.10962">V2X-Radar: A Multi-modal Dataset with 4D Radar for Cooperative Perception</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modern autonomous vehicle perception systems often struggle with occlusions and limited perception range. Previous studies have demonstrated the effectiveness of cooperative perception in extending the perception range and overcoming occlusions, thereby enhancing the safety of autonomous driving. In recent years, a series of cooperative perception datasets have emerged; however, these datasets primarily focus on cameras and LiDAR, neglecting 4D Radar, a sensor used in single-vehicle autonomous driving to provide robust perception in adverse weather conditions. In this paper, to bridge the gap created by the absence of 4D Radar datasets in cooperative perception, we present V2X-Radar, the first large-scale, real-world multi-modal dataset featuring 4D Radar. V2X-Radar dataset is collected using a connected vehicle platform and an intelligent roadside unit equipped with 4D Radar, LiDAR, and multi-view cameras. The collected data encompasses sunny and rainy weather conditions, spanning daytime, dusk, and nighttime, as well as various typical challenging scenarios. The dataset consists of 20K LiDAR frames, 40K camera images, and 20K 4D Radar data, including 350K annotated boxes across five categories. To support various research domains, we have established V2X-Radar-C for cooperative perception, V2X-Radar-I for roadside perception, and V2X-Radar-V for single-vehicle perception. Furthermore, we provide comprehensive benchmarks across these three sub-datasets. We will release all datasets and benchmark codebase at http://openmpd.com/column/V2X-Radar and https://github.com/yanglei18/V2X-Radar.
<div id='section'>Paperid: <span id='pid'>30, <a href='https://arxiv.org/pdf/2411.09022.pdf' target='_blank'>https://arxiv.org/pdf/2411.09022.pdf</a></span>   <span><a href='https://wyd0817.github.io/project-dart-llm/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yongdong Wang, Runze Xiao, Jun Younes Louhi Kasahara, Ryosuke Yajima, Keiji Nagatani, Atsushi Yamashita, Hajime Asama
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.09022">DART-LLM: Dependency-Aware Multi-Robot Task Decomposition and Execution using Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) have demonstrated promising reasoning capabilities in robotics; however, their application in multi-robot systems remains limited, particularly in handling task dependencies. This paper introduces DART-LLM, a novel framework that employs Directed Acyclic Graphs (DAGs) to model task dependencies, enabling the decomposition of natural language instructions into well-coordinated subtasks for multi-robot execution. DART-LLM comprises four key components: a Question-Answering (QA) LLM module for dependency-aware task decomposition, a Breakdown Function module for robot assignment, an Actuation module for execution, and a Vision-Language Model (VLM)-based object detector for environmental perception, achieving end-to-end task execution. Experimental results across three task complexity levels demonstrate that DART-LLM achieves state-of-the-art performance, significantly outperforming the baseline across all evaluation metrics. Among the tested models, DeepSeek-r1-671B achieves the highest success rate, whereas Llama-3.1-8B exhibits superior response time reliability. Ablation studies further confirm that explicit dependency modeling notably enhances the performance of smaller models, facilitating efficient deployment on resource-constrained platforms. Please refer to the project website https://wyd0817.github.io/project-dart-llm/ for videos and code.
<div id='section'>Paperid: <span id='pid'>31, <a href='https://arxiv.org/pdf/2411.02624.pdf' target='_blank'>https://arxiv.org/pdf/2411.02624.pdf</a></span>   <span><a href='https://github.com/NingMingHao/MVSLab-IndoorCooperativePerception' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Minghao Ning, Yaodong Cui, Yufeng Yang, Shucheng Huang, Zhenan Liu, Ahmad Reza Alghooneh, Ehsan Hashemi, Amir Khajepour
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.02624">Enhancing Indoor Mobility with Connected Sensor Nodes: A Real-Time, Delay-Aware Cooperative Perception Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a novel real-time, delay-aware cooperative perception system designed for intelligent mobility platforms operating in dynamic indoor environments. The system contains a network of multi-modal sensor nodes and a central node that collectively provide perception services to mobility platforms. The proposed Hierarchical Clustering Considering the Scanning Pattern and Ground Contacting Feature based Lidar Camera Fusion improve intra-node perception for crowded environment. The system also features delay-aware global perception to synchronize and aggregate data across nodes. To validate our approach, we introduced the Indoor Pedestrian Tracking dataset, compiled from data captured by two indoor sensor nodes. Our experiments, compared to baselines, demonstrate significant improvements in detection accuracy and robustness against delays. The dataset is available in the repository: https://github.com/NingMingHao/MVSLab-IndoorCooperativePerception
<div id='section'>Paperid: <span id='pid'>32, <a href='https://arxiv.org/pdf/2410.01398.pdf' target='_blank'>https://arxiv.org/pdf/2410.01398.pdf</a></span>   <span><a href='https://github.com/BrendanxP/CSI-Simulation-Framework' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Brendan Dijkstra, Ninad Jadhav, Alex Sloot, Matteo Marcantoni, Bayu Jayawardhana, Stephanie Gil, Bahar Haghighat
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.01398">WiFi-CSI Sensing and Bearing Estimation in Multi-Robot Systems: An Open-Source Simulation Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Development and testing of multi-robot systems employing wireless signal-based sensing requires access to suitable hardware, such as channel monitoring WiFi transceivers, which can pose significant limitations. The WiFi Sensor for Robotics (WSR) toolbox, introduced by Jadhav et al. in 2022, provides a novel solution by using WiFi Channel State Information (CSI) to compute relative bearing between robots. The toolbox leverages the amplitude and phase of WiFi signals and creates virtual antenna arrays by exploiting the motion of mobile robots, eliminating the need for physical antenna arrays. However, the WSR toolbox's reliance on an obsoleting WiFi transceiver hardware has limited its operability and accessibility, hindering broader application and development of relevant tools. We present an open-source simulation framework that replicates the WSR toolbox's capabilities using Gazebo and Matlab. By simulating WiFi-CSI data collection, our framework emulates the behavior of mobile robots equipped with the WSR toolbox, enabling precise bearing estimation without physical hardware. We validate the framework through experiments with both simulated and real Turtlebot3 robots, showing a close match between the obtained CSI data and the resulting bearing estimates. This work provides a virtual environment for developing and testing WiFi-CSI-based multi-robot localization without relying on physical hardware. All code and experimental setup information are publicly available at https://github.com/BrendanxP/CSI-Simulation-Framework
<div id='section'>Paperid: <span id='pid'>33, <a href='https://arxiv.org/pdf/2409.16967.pdf' target='_blank'>https://arxiv.org/pdf/2409.16967.pdf</a></span>   <span><a href='https://github.com/AccGen99/marl_ipp' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Apoorva Vashisth, Manav Kulshrestha, Damon Conover, Aniket Bera
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.16967">Scalable Multi-Robot Informative Path Planning for Target Mapping via Deep Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous robots are widely utilized for mapping and exploration tasks due to their cost-effectiveness. Multi-robot systems offer scalability and efficiency, especially in terms of the number of robots deployed in more complex environments. These tasks belong to the set of Multi-Robot Informative Path Planning (MRIPP) problems. In this paper, we propose a deep reinforcement learning approach for the MRIPP problem. We aim to maximize the number of discovered stationary targets in an unknown 3D environment while operating under resource constraints (such as path length). Here, each robot aims to maximize discovered targets, avoid unknown static obstacles, and prevent inter-robot collisions while operating under communication and resource constraints. We utilize the centralized training and decentralized execution paradigm to train a single policy neural network. A key aspect of our approach is our coordination graph that prioritizes visiting regions not yet explored by other robots. Our learned policy can be copied onto any number of robots for deployment in more complex environments not seen during training. Our approach outperforms state-of-the-art approaches by at least 26.2% in terms of the number of discovered targets while requiring a planning time of less than 2 sec per step. We present results for more complex environments with up to 64 robots and compare success rates against baseline planners. Our code and trained model are available at - https://github.com/AccGen99/marl_ipp
<div id='section'>Paperid: <span id='pid'>34, <a href='https://arxiv.org/pdf/2409.15146.pdf' target='_blank'>https://arxiv.org/pdf/2409.15146.pdf</a></span>   <span><a href='https://github.com/MrKeee/COHERENT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kehui Liu, Zixin Tang, Dong Wang, Zhigang Wang, Xuelong Li, Bin Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.15146">COHERENT: Collaboration of Heterogeneous Multi-Robot System with Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Leveraging the powerful reasoning capabilities of large language models (LLMs), recent LLM-based robot task planning methods yield promising results. However, they mainly focus on single or multiple homogeneous robots on simple tasks. Practically, complex long-horizon tasks always require collaboration among multiple heterogeneous robots especially with more complex action spaces, which makes these tasks more challenging. To this end, we propose COHERENT, a novel LLM-based task planning framework for collaboration of heterogeneous multi-robot systems including quadrotors, robotic dogs, and robotic arms. Specifically, a Proposal-Execution-Feedback-Adjustment (PEFA) mechanism is designed to decompose and assign actions for individual robots, where a centralized task assigner makes a task planning proposal to decompose the complex task into subtasks, and then assigns subtasks to robot executors. Each robot executor selects a feasible action to implement the assigned subtask and reports self-reflection feedback to the task assigner for plan adjustment. The PEFA loops until the task is completed. Moreover, we create a challenging heterogeneous multi-robot task planning benchmark encompassing 100 complex long-horizon tasks. The experimental results show that our work surpasses the previous methods by a large margin in terms of success rate and execution efficiency. The experimental videos, code, and benchmark are released at https://github.com/MrKeee/COHERENT.
<div id='section'>Paperid: <span id='pid'>35, <a href='https://arxiv.org/pdf/2409.10699.pdf' target='_blank'>https://arxiv.org/pdf/2409.10699.pdf</a></span>   <span><a href='https://taco-group.github.io/CoMamba/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinlong Li, Xinyu Liu, Baolu Li, Runsheng Xu, Jiachen Li, Hongkai Yu, Zhengzhong Tu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.10699">CoMamba: Real-time Cooperative Perception Unlocked with State Space Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cooperative perception systems play a vital role in enhancing the safety and efficiency of vehicular autonomy. Although recent studies have highlighted the efficacy of vehicle-to-everything (V2X) communication techniques in autonomous driving, a significant challenge persists: how to efficiently integrate multiple high-bandwidth features across an expanding network of connected agents such as vehicles and infrastructure. In this paper, we introduce CoMamba, a novel cooperative 3D detection framework designed to leverage state-space models for real-time onboard vehicle perception. Compared to prior state-of-the-art transformer-based models, CoMamba enjoys being a more scalable 3D model using bidirectional state space models, bypassing the quadratic complexity pain-point of attention mechanisms. Through extensive experimentation on V2X/V2V datasets, CoMamba achieves superior performance compared to existing methods while maintaining real-time processing capabilities. The proposed framework not only enhances object detection accuracy but also significantly reduces processing time, making it a promising solution for next-generation cooperative perception systems in intelligent transportation networks.
<div id='section'>Paperid: <span id='pid'>36, <a href='https://arxiv.org/pdf/2409.06531.pdf' target='_blank'>https://arxiv.org/pdf/2409.06531.pdf</a></span>   <span><a href='https://github.com/wuuya1/RangeTAP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Gang Xu, Yuchen Wu, Sheng Tao, Yifan Yang, Tao Liu, Tao Huang, Huifeng Wu, Yong Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.06531">Multi-robot Task Allocation and Path Planning with Maximum Range Constraints</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This letter presents a novel multi-robot task allocation and path planning method that considers robots' maximum range constraints in large-sized workspaces, enabling robots to complete the assigned tasks within their range limits. Firstly, we developed a fast path planner to solve global paths efficiently. Subsequently, we propose an innovative auction-based approach that integrates our path planner into the auction phase for reward computation while considering the robots' range limits. This method accounts for extra obstacle-avoiding travel distances rather than ideal straight-line distances, resolving the coupling between task allocation and path planning. Additionally, to avoid redundant computations during iterations, we implemented a lazy auction strategy to speed up the convergence of the task allocation. Finally, we validated the proposed method's effectiveness and application potential through extensive simulation and real-world experiments. The implementation code for our method will be available at https://github.com/wuuya1/RangeTAP.
<div id='section'>Paperid: <span id='pid'>37, <a href='https://arxiv.org/pdf/2409.04980.pdf' target='_blank'>https://arxiv.org/pdf/2409.04980.pdf</a></span>   <span><a href='https://github.com/RadetzkyLi/Multi-V2X' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Rongsong Li, Xin Pei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.04980">Multi-V2X: A Large Scale Multi-modal Multi-penetration-rate Dataset for Cooperative Perception</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cooperative perception through vehicle-to-everything (V2X) has garnered significant attention in recent years due to its potential to overcome occlusions and enhance long-distance perception. Great achievements have been made in both datasets and algorithms. However, existing real-world datasets are limited by the presence of few communicable agents, while synthetic datasets typically cover only vehicles. More importantly, the penetration rate of connected and autonomous vehicles (CAVs) , a critical factor for the deployment of cooperative perception technologies, has not been adequately addressed. To tackle these issues, we introduce Multi-V2X, a large-scale, multi-modal, multi-penetration-rate dataset for V2X perception. By co-simulating SUMO and CARLA, we equip a substantial number of cars and roadside units (RSUs) in simulated towns with sensor suites, and collect comprehensive sensing data. Datasets with specified CAV penetration rates can be obtained by masking some equipped cars as normal vehicles. In total, our Multi-V2X dataset comprises 549k RGB frames, 146k LiDAR frames, and 4,219k annotated 3D bounding boxes across six categories. The highest possible CAV penetration rate reaches 86.21%, with up to 31 agents in communication range, posing new challenges in selecting agents to collaborate with. We provide comprehensive benchmarks for cooperative 3D object detection tasks. Our data and code are available at https://github.com/RadetzkyLi/Multi-V2X .
<div id='section'>Paperid: <span id='pid'>38, <a href='https://arxiv.org/pdf/2408.16530.pdf' target='_blank'>https://arxiv.org/pdf/2408.16530.pdf</a></span>   <span><a href='https://github.com/Fishsoup0/Autonomous-Driving-Perception' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Wang, Shaohua Wang, Yicheng Li, Mingchun Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.16530">A Comprehensive Review of 3D Object Detection in Autonomous Driving: Technological Advances and Future Directions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, 3D object perception has become a crucial component in the development of autonomous driving systems, providing essential environmental awareness. However, as perception tasks in autonomous driving evolve, their variants have increased, leading to diverse insights from industry and academia. Currently, there is a lack of comprehensive surveys that collect and summarize these perception tasks and their developments from a broader perspective. This review extensively summarizes traditional 3D object detection methods, focusing on camera-based, LiDAR-based, and fusion detection techniques. We provide a comprehensive analysis of the strengths and limitations of each approach, highlighting advancements in accuracy and robustness. Furthermore, we discuss future directions, including methods to improve accuracy such as temporal perception, occupancy grids, and end-to-end learning frameworks. We also explore cooperative perception methods that extend the perception range through collaborative communication. By providing a holistic view of the current state and future developments in 3D object perception, we aim to offer a more comprehensive understanding of perception tasks for autonomous driving. Additionally, we have established an active repository to provide continuous updates on the latest advancements in this field, accessible at: https://github.com/Fishsoup0/Autonomous-Driving-Perception.
<div id='section'>Paperid: <span id='pid'>39, <a href='https://arxiv.org/pdf/2408.11241.pdf' target='_blank'>https://arxiv.org/pdf/2408.11241.pdf</a></span>   <span><a href='https://github.com/ucla-mobility/CooPre' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Seth Z. Zhao, Hao Xiang, Chenfeng Xu, Xin Xia, Bolei Zhou, Jiaqi Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.11241">CooPre: Cooperative Pretraining for V2X Cooperative Perception</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing Vehicle-to-Everything (V2X) cooperative perception methods rely on accurate multi-agent 3D annotations. Nevertheless, it is time-consuming and expensive to collect and annotate real-world data, especially for V2X systems. In this paper, we present a self-supervised learning framwork for V2X cooperative perception, which utilizes the vast amount of unlabeled 3D V2X data to enhance the perception performance. Specifically, multi-agent sensing information is aggregated to form a holistic view and a novel proxy task is formulated to reconstruct the LiDAR point clouds across multiple connected agents to better reason multi-agent spatial correlations. Besides, we develop a V2X bird-eye-view (BEV) guided masking strategy which effectively allows the model to pay attention to 3D features across heterogeneous V2X agents (i.e., vehicles and infrastructure) in the BEV space. Noticeably, such a masking strategy effectively pretrains the 3D encoder with a multi-agent LiDAR point cloud reconstruction objective and is compatible with mainstream cooperative perception backbones. Our approach, validated through extensive experiments on representative datasets (i.e., V2X-Real, V2V4Real, and OPV2V) and multiple state-of-the-art cooperative perception methods (i.e., AttFuse, F-Cooper, and V2X-ViT), leads to a performance boost across all V2X settings. Notably, CooPre achieves a 4% mAP improvement on V2X-Real dataset and surpasses baseline performance using only 50% of the training data, highlighting its data efficiency. Additionally, we demonstrate the framework's powerful performance in cross-domain transferability and robustness under challenging scenarios. The code will be made publicly available at https://github.com/ucla-mobility/CooPre.
<div id='section'>Paperid: <span id='pid'>40, <a href='https://arxiv.org/pdf/2407.03825.pdf' target='_blank'>https://arxiv.org/pdf/2407.03825.pdf</a></span>   <span><a href='https://github.com/YuanYunshuang/CoSense3D' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunshuang Yuan, Monika Sester
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.03825">StreamLTS: Query-based Temporal-Spatial LiDAR Fusion for Cooperative Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cooperative perception via communication among intelligent traffic agents has great potential to improve the safety of autonomous driving. However, limited communication bandwidth, localization errors and asynchronized capturing time of sensor data, all introduce difficulties to the data fusion of different agents. To some extend, previous works have attempted to reduce the shared data size, mitigate the spatial feature misalignment caused by localization errors and communication delay. However, none of them have considered the asynchronized sensor ticking times, which can lead to dynamic object misplacement of more than one meter during data fusion. In this work, we propose Time-Aligned COoperative Object Detection (TA-COOD), for which we adapt widely used dataset OPV2V and DairV2X with considering asynchronous LiDAR sensor ticking times and build an efficient fully sparse framework with modeling the temporal information of individual objects with query-based techniques. The experiment results confirmed the superior efficiency of our fully sparse framework compared to the state-of-the-art dense models. More importantly, they show that the point-wise observation timestamps of the dynamic objects are crucial for accurate modeling the object temporal context and the predictability of their time-related locations. The official code is available at \url{https://github.com/YuanYunshuang/CoSense3D}.
<div id='section'>Paperid: <span id='pid'>41, <a href='https://arxiv.org/pdf/2405.08345.pdf' target='_blank'>https://arxiv.org/pdf/2405.08345.pdf</a></span>   <span><a href='https://github.com/KunSong-L/Distributed-Multi-Robot-Topological-Map' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kun Song, Gaoming Chen, Wenhang Liu, Zhenhua Xiong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.08345">Multi-Robot Rendezvous in Unknown Environment with Limited Communication</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Rendezvous aims at gathering all robots at a specific location, which is an important collaborative behavior for multi-robot systems. However, in an unknown environment, it is challenging to achieve rendezvous. Previous researches mainly focus on special scenarios where communication is not allowed and each robot executes a random searching strategy, which is highly time-consuming, especially in large-scale environments. In this work, we focus on rendezvous in unknown environments where communication is available. We divide this task into two steps: rendezvous based environment exploration with relative pose (RP) estimation and rendezvous point selection. A new strategy called partitioned and incomplete exploration for rendezvous (PIER) is proposed to efficiently explore the unknown environment, where lightweight topological maps are constructed and shared among robots for RP estimation with very few communications. Then, a rendezvous point selection algorithm based on the merged topological map is proposed for efficient rendezvous for multi-robot systems. The effectiveness of the proposed methods is validated in both simulations and real-world experiments.
<div id='section'>Paperid: <span id='pid'>42, <a href='https://arxiv.org/pdf/2405.01107.pdf' target='_blank'>https://arxiv.org/pdf/2405.01107.pdf</a></span>   <span><a href='https://proroklab.github.io/CoViS-Net/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jan Blumenkamp, Steven Morad, Jennifer Gielis, Amanda Prorok
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.01107">CoViS-Net: A Cooperative Visual Spatial Foundation Model for Multi-Robot Applications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous robot operation in unstructured environments is often underpinned by spatial understanding through vision. Systems composed of multiple concurrently operating robots additionally require access to frequent, accurate and reliable pose estimates. In this work, we propose CoViS-Net, a decentralized visual spatial foundation model that learns spatial priors from data, enabling pose estimation as well as spatial comprehension. Our model is fully decentralized, platform-agnostic, executable in real-time using onboard compute, and does not require existing networking infrastructure. CoViS-Net provides relative pose estimates and a local bird's-eye-view (BEV) representation, even without camera overlap between robots (in contrast to classical methods). We demonstrate its use in a multi-robot formation control task across various real-world settings. We provide code, models and supplementary material online. https://proroklab.github.io/CoViS-Net/
<div id='section'>Paperid: <span id='pid'>43, <a href='https://arxiv.org/pdf/2403.16015.pdf' target='_blank'>https://arxiv.org/pdf/2403.16015.pdf</a></span>   <span><a href='https://github.com/ziyanx02/multiagent-quadruped-environment' target='_blank'>  GitHub</a></span> <span><a href='https://ziyanx02.github.io/multiagent-quadruped-environment/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyan Xiong, Bo Chen, Shiyu Huang, Wei-Wei Tu, Zhaofeng He, Yang Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.16015">MQE: Unleashing the Power of Interaction with Multi-agent Quadruped Environment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The advent of deep reinforcement learning (DRL) has significantly advanced the field of robotics, particularly in the control and coordination of quadruped robots. However, the complexity of real-world tasks often necessitates the deployment of multi-robot systems capable of sophisticated interaction and collaboration. To address this need, we introduce the Multi-agent Quadruped Environment (MQE), a novel platform designed to facilitate the development and evaluation of multi-agent reinforcement learning (MARL) algorithms in realistic and dynamic scenarios. MQE emphasizes complex interactions between robots and objects, hierarchical policy structures, and challenging evaluation scenarios that reflect real-world applications. We present a series of collaborative and competitive tasks within MQE, ranging from simple coordination to complex adversarial interactions, and benchmark state-of-the-art MARL algorithms. Our findings indicate that hierarchical reinforcement learning can simplify task learning, but also highlight the need for advanced algorithms capable of handling the intricate dynamics of multi-agent interactions. MQE serves as a stepping stone towards bridging the gap between simulation and practical deployment, offering a rich environment for future research in multi-agent systems and robot learning. For open-sourced code and more details of MQE, please refer to https://ziyanx02.github.io/multiagent-quadruped-environment/ .
<div id='section'>Paperid: <span id='pid'>44, <a href='https://arxiv.org/pdf/2403.13311.pdf' target='_blank'>https://arxiv.org/pdf/2403.13311.pdf</a></span>   <span><a href='https://github.com/reso1/MCFS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingtao Tang, Hang Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.13311">Multi-Robot Connected Fermat Spiral Coverage</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce the Multi-Robot Connected Fermat Spiral (MCFS), a novel algorithmic framework for Multi-Robot Coverage Path Planning (MCPP) that adapts Connected Fermat Spiral (CFS) from the computer graphics community to multi-robot coordination for the first time. MCFS uniquely enables the orchestration of multiple robots to generate coverage paths that contour around arbitrarily shaped obstacles, a feature that is notably lacking in traditional methods. Our framework not only enhances area coverage and optimizes task performance, particularly in terms of makespan, for workspaces rich in irregular obstacles but also addresses the challenges of path continuity and curvature critical for non-holonomic robots by generating smooth paths without decomposing the workspace. MCFS solves MCPP by constructing a graph of isolines and transforming MCPP into a combinatorial optimization problem, aiming to minimize the makespan while covering all vertices. Our contributions include developing a unified CFS version for scalable and adaptable MCPP, extending it to MCPP with novel optimization techniques for cost reduction and path continuity and smoothness, and demonstrating through extensive experiments that MCFS outperforms existing MCPP methods in makespan, path curvature, coverage ratio, and overlapping ratio. Our research marks a significant step in MCPP, showcasing the fusion of computer graphics and automated planning principles to advance the capabilities of multi-robot systems in complex environments. Our code is available at https://github.com/reso1/MCFS.
<div id='section'>Paperid: <span id='pid'>45, <a href='https://arxiv.org/pdf/2403.10145.pdf' target='_blank'>https://arxiv.org/pdf/2403.10145.pdf</a></span>   <span><a href='https://github.com/AIR-THU/DAIR-RCooper' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruiyang Hao, Siqi Fan, Yingru Dai, Zhenlin Zhang, Chenxi Li, Yuntian Wang, Haibao Yu, Wenxian Yang, Jirui Yuan, Zaiqing Nie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.10145">RCooper: A Real-world Large-scale Dataset for Roadside Cooperative Perception</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The value of roadside perception, which could extend the boundaries of autonomous driving and traffic management, has gradually become more prominent and acknowledged in recent years. However, existing roadside perception approaches only focus on the single-infrastructure sensor system, which cannot realize a comprehensive understanding of a traffic area because of the limited sensing range and blind spots. Orienting high-quality roadside perception, we need Roadside Cooperative Perception (RCooper) to achieve practical area-coverage roadside perception for restricted traffic areas. Rcooper has its own domain-specific challenges, but further exploration is hindered due to the lack of datasets. We hence release the first real-world, large-scale RCooper dataset to bloom the research on practical roadside cooperative perception, including detection and tracking. The manually annotated dataset comprises 50k images and 30k point clouds, including two representative traffic scenes (i.e., intersection and corridor). The constructed benchmarks prove the effectiveness of roadside cooperation perception and demonstrate the direction of further research. Codes and dataset can be accessed at: https://github.com/AIR-THU/DAIR-RCooper.
<div id='section'>Paperid: <span id='pid'>46, <a href='https://arxiv.org/pdf/2403.01316.pdf' target='_blank'>https://arxiv.org/pdf/2403.01316.pdf</a></span>   <span><a href='https://tum-traffic-dataset.github.io/tumtraf-v2x' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Walter Zimmer, Gerhard Arya Wardana, Suren Sritharan, Xingcheng Zhou, Rui Song, Alois C. Knoll
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.01316">TUMTraf V2X Cooperative Perception Dataset</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cooperative perception offers several benefits for enhancing the capabilities of autonomous vehicles and improving road safety. Using roadside sensors in addition to onboard sensors increases reliability and extends the sensor range. External sensors offer higher situational awareness for automated vehicles and prevent occlusions. We propose CoopDet3D, a cooperative multi-modal fusion model, and TUMTraf-V2X, a perception dataset, for the cooperative 3D object detection and tracking task. Our dataset contains 2,000 labeled point clouds and 5,000 labeled images from five roadside and four onboard sensors. It includes 30k 3D boxes with track IDs and precise GPS and IMU data. We labeled eight categories and covered occlusion scenarios with challenging driving maneuvers, like traffic violations, near-miss events, overtaking, and U-turns. Through multiple experiments, we show that our CoopDet3D camera-LiDAR fusion model achieves an increase of +14.36 3D mAP compared to a vehicle camera-LiDAR fusion model. Finally, we make our dataset, model, labeling tool, and dev-kit publicly available on our website: https://tum-traffic-dataset.github.io/tumtraf-v2x.
<div id='section'>Paperid: <span id='pid'>47, <a href='https://arxiv.org/pdf/2310.18622.pdf' target='_blank'>https://arxiv.org/pdf/2310.18622.pdf</a></span>   <span><a href='https://github.com/lunjohnzhang/warehouse_env_gen_nca_public' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yulun Zhang, Matthew C. Fontaine, Varun Bhatt, Stefanos Nikolaidis, Jiaoyang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.18622">Arbitrarily Scalable Environment Generators via Neural Cellular Automata</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We study the problem of generating arbitrarily large environments to improve the throughput of multi-robot systems. Prior work proposes Quality Diversity (QD) algorithms as an effective method for optimizing the environments of automated warehouses. However, these approaches optimize only relatively small environments, falling short when it comes to replicating real-world warehouse sizes. The challenge arises from the exponential increase in the search space as the environment size increases. Additionally, the previous methods have only been tested with up to 350 robots in simulations, while practical warehouses could host thousands of robots. In this paper, instead of optimizing environments, we propose to optimize Neural Cellular Automata (NCA) environment generators via QD algorithms. We train a collection of NCA generators with QD algorithms in small environments and then generate arbitrarily large environments from the generators at test time. We show that NCA environment generators maintain consistent, regularized patterns regardless of environment size, significantly enhancing the scalability of multi-robot systems in two different domains with up to 2,350 robots. Additionally, we demonstrate that our method scales a single-agent reinforcement learning policy to arbitrarily large environments with similar patterns. We include the source code at \url{https://github.com/lunjohnzhang/warehouse_env_gen_nca_public}.
<div id='section'>Paperid: <span id='pid'>48, <a href='https://arxiv.org/pdf/2310.16870.pdf' target='_blank'>https://arxiv.org/pdf/2310.16870.pdf</a></span>   <span><a href='https://github.com/PurdueDigitalTwin/MACP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunsheng Ma, Juanwu Lu, Can Cui, Sicheng Zhao, Xu Cao, Wenqian Ye, Ziran Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.16870">MACP: Efficient Model Adaptation for Cooperative Perception</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vehicle-to-vehicle (V2V) communications have greatly enhanced the perception capabilities of connected and automated vehicles (CAVs) by enabling information sharing to "see through the occlusions", resulting in significant performance improvements. However, developing and training complex multi-agent perception models from scratch can be expensive and unnecessary when existing single-agent models show remarkable generalization capabilities. In this paper, we propose a new framework termed MACP, which equips a single-agent pre-trained model with cooperation capabilities. We approach this objective by identifying the key challenges of shifting from single-agent to cooperative settings, adapting the model by freezing most of its parameters and adding a few lightweight modules. We demonstrate in our experiments that the proposed framework can effectively utilize cooperative observations and outperform other state-of-the-art approaches in both simulated and real-world cooperative perception benchmarks while requiring substantially fewer tunable parameters with reduced communication costs. Our source code is available at https://github.com/PurdueDigitalTwin/MACP.
<div id='section'>Paperid: <span id='pid'>49, <a href='https://arxiv.org/pdf/2310.06603.pdf' target='_blank'>https://arxiv.org/pdf/2310.06603.pdf</a></span>   <span><a href='https://github.com/feeling0414-lab/V2X-AHD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Caizhen He, Hai Wang, Long Chen, Tong Luo, Yingfeng Cai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.06603">V2X-AHD:Vehicle-to-Everything Cooperation Perception via Asymmetric Heterogenous Distillation Network</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Object detection is the central issue of intelligent traffic systems, and recent advancements in single-vehicle lidar-based 3D detection indicate that it can provide accurate position information for intelligent agents to make decisions and plan. Compared with single-vehicle perception, multi-view vehicle-road cooperation perception has fundamental advantages, such as the elimination of blind spots and a broader range of perception, and has become a research hotspot. However, the current perception of cooperation focuses on improving the complexity of fusion while ignoring the fundamental problems caused by the absence of single-view outlines. We propose a multi-view vehicle-road cooperation perception system, vehicle-to-everything cooperative perception (V2X-AHD), in order to enhance the identification capability, particularly for predicting the vehicle's shape. At first, we propose an asymmetric heterogeneous distillation network fed with different training data to improve the accuracy of contour recognition, with multi-view teacher features transferring to single-view student features. While the point cloud data are sparse, we propose Spara Pillar, a spare convolutional-based plug-in feature extraction backbone, to reduce the number of parameters and improve and enhance feature extraction capabilities. Moreover, we leverage the multi-head self-attention (MSA) to fuse the single-view feature, and the lightweight design makes the fusion feature a smooth expression. The results of applying our algorithm to the massive open dataset V2Xset demonstrate that our method achieves the state-of-the-art result. The V2X-AHD can effectively improve the accuracy of 3D object detection and reduce the number of network parameters, according to this study, which serves as a benchmark for cooperative perception. The code for this article is available at https://github.com/feeling0414-lab/V2X-AHD.
<div id='section'>Paperid: <span id='pid'>50, <a href='https://arxiv.org/pdf/2308.10097.pdf' target='_blank'>https://arxiv.org/pdf/2308.10097.pdf</a></span>   <span><a href='https://github.com/abbas-tari/raft.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Abbas Tariverdi, Jim Torresen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.10097">Rafting Towards Consensus: Formation Control of Distributed Dynamical Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we introduce a novel adaptation of the Raft consensus algorithm for achieving emergent formation control in multi-agent systems with a single integrator dynamics. This strategy, dubbed "Rafting," enables robust cooperation between distributed nodes, thereby facilitating the achievement of desired geometric configurations. Our framework takes advantage of the Raft algorithm's inherent fault tolerance and strong consistency guarantees to extend its applicability to distributed formation control tasks. Following the introduction of a decentralized mechanism for aggregating agent states, a synchronization protocol for information exchange and consensus formation is proposed. The Raft consensus algorithm combines leader election, log replication, and state machine application to steer agents toward a common, collaborative goal. A series of detailed simulations validate the efficacy and robustness of our method under various conditions, including partial network failures and disturbances. The outcomes demonstrate the algorithm's potential and open up new possibilities in swarm robotics, autonomous transportation, and distributed computation. The implementation of the algorithms presented in this paper is available at https://github.com/abbas-tari/raft.git.
<div id='section'>Paperid: <span id='pid'>51, <a href='https://arxiv.org/pdf/2308.08862.pdf' target='_blank'>https://arxiv.org/pdf/2308.08862.pdf</a></span>   <span><a href='https://github.com/Dr-Xiaogaren/T2E' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Zhang, Jiaming Chen, Jiyu Cheng, Yibin Li, Simon X. Yang, Wei Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.08862">Nowhere to Go: Benchmarking Multi-robot Collaboration in Target Trapping Environment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Collaboration is one of the most important factors in multi-robot systems. Considering certain real-world applications and to further promote its development, we propose a new benchmark to evaluate multi-robot collaboration in Target Trapping Environment (T2E). In T2E, two kinds of robots (called captor robot and target robot) share the same space. The captors aim to catch the target collaboratively, while the target will try to escape from the trap. Both the trapping and escaping process can use the environment layout to help achieve the corresponding objective, which requires high collaboration between robots and the utilization of the environment. For the benchmark, we present and evaluate multiple learning-based baselines in T2E, and provide insights into regimes of multi-robot collaboration. We also make our benchmark publicly available and encourage researchers from related robotics disciplines to propose, evaluate, and compare their solutions in this benchmark. Our project is released at https://github.com/Dr-Xiaogaren/T2E.
<div id='section'>Paperid: <span id='pid'>52, <a href='https://arxiv.org/pdf/2308.07053.pdf' target='_blank'>https://arxiv.org/pdf/2308.07053.pdf</a></span>   <span><a href='https://github.com/ika-rwth-aachen/robotkube' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bastian Lampe, Lennart Reiher, Lukas Zanger, Timo Woopen, Raphael van Kempen, Lutz Eckstein
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.07053">RobotKube: Orchestrating Large-Scale Cooperative Multi-Robot Systems with Kubernetes and ROS</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modern cyber-physical systems (CPS) such as Cooperative Intelligent Transport Systems (C-ITS) are increasingly defined by the software which operates these systems. In practice, microservice architectures can be employed, which may consist of containerized microservices running in a cluster comprised of robots and supporting infrastructure. These microservices need to be orchestrated dynamically according to ever changing requirements posed at the system. Additionally, these systems are embedded in DevOps processes aiming at continually updating and upgrading both the capabilities of CPS components and of the system as a whole. In this paper, we present RobotKube, an approach to orchestrating containerized microservices for large-scale cooperative multi-robot CPS based on Kubernetes. We describe how to automate the orchestration of software across a CPS, and include the possibility to monitor and selectively store relevant accruing data. In this context, we present two main components of such a system: an event detector capable of, e.g., requesting the deployment of additional applications, and an application manager capable of automatically configuring the required changes in the Kubernetes cluster. By combining the widely adopted Kubernetes platform with the Robot Operating System (ROS), we enable the use of standard tools and practices for developing, deploying, scaling, and monitoring microservices in C-ITS. We demonstrate and evaluate RobotKube in an exemplary and reproducible use case that we make publicly available at https://github.com/ika-rwth-aachen/robotkube .
<div id='section'>Paperid: <span id='pid'>53, <a href='https://arxiv.org/pdf/2307.11514.pdf' target='_blank'>https://arxiv.org/pdf/2307.11514.pdf</a></span>   <span><a href='https://github.com/zllxot/CORE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Binglu Wang, Lei Zhang, Zhaozhong Wang, Yongqiang Zhao, Tianfei Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.11514">CORE: Cooperative Reconstruction for Multi-Agent Perception</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents CORE, a conceptually simple, effective and communication-efficient model for multi-agent cooperative perception. It addresses the task from a novel perspective of cooperative reconstruction, based on two key insights: 1) cooperating agents together provide a more holistic observation of the environment, and 2) the holistic observation can serve as valuable supervision to explicitly guide the model learning how to reconstruct the ideal observation based on collaboration. CORE instantiates the idea with three major components: a compressor for each agent to create more compact feature representation for efficient broadcasting, a lightweight attentive collaboration component for cross-agent message aggregation, and a reconstruction module to reconstruct the observation based on aggregated feature representations. This learning-to-reconstruct idea is task-agnostic, and offers clear and reasonable supervision to inspire more effective collaboration, eventually promoting perception tasks. We validate CORE on OPV2V, a large-scale multi-agent percetion dataset, in two tasks, i.e., 3D object detection and semantic segmentation. Results demonstrate that the model achieves state-of-the-art performance on both tasks, and is more communication-efficient.
<div id='section'>Paperid: <span id='pid'>54, <a href='https://arxiv.org/pdf/2307.03891.pdf' target='_blank'>https://arxiv.org/pdf/2307.03891.pdf</a></span>   <span><a href='https://shubhlohiya.github.io/MARBLER/' target='_blank'>  GitHub</a></span> <span><a href='https://shubhlohiya.github.io/MARBLER/,' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Reza Torbati, Shubham Lohiya, Shivika Singh, Meher Shashwat Nigam, Harish Ravichandar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.03891">MARBLER: An Open Platform for Standardized Evaluation of Multi-Robot Reinforcement Learning Algorithms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Agent Reinforcement Learning (MARL) has enjoyed significant recent progress thanks, in part, to the integration of deep learning techniques for modeling interactions in complex environments. This is naturally starting to benefit multi-robot systems (MRS) in the form of multi-robot RL (MRRL). However, existing infrastructure to train and evaluate policies predominantly focus on the challenges of coordinating virtual agents, and ignore characteristics important to robotic systems. Few platforms support realistic robot dynamics, and fewer still can evaluate Sim2Real performance of learned behavior. To address these issues, we contribute MARBLER: Multi-Agent RL Benchmark and Learning Environment for the Robotarium. MARBLER offers a robust and comprehensive evaluation platform for MRRL by marrying Georgia Tech's Robotarium (which enables rapid deployment on physical MRS) and OpenAI's Gym interface (which facilitates standardized use of modern learning algorithms). MARBLER offers a highly controllable environment with realistic dynamics, including barrier certificate-based obstacle avoidance. It allows anyone across the world to train and deploy MRRL algorithms on a physical testbed with reproducibility. Further, we introduce five novel scenarios inspired by common challenges in MRS and provide support for new custom scenarios. Finally, we use MARBLER to evaluate popular MARL algorithms and provide insights into their suitability for MRRL. In summary, MARBLER can be a valuable tool to the MRS research community by facilitating comprehensive and standardized evaluation of learning algorithms on realistic simulations and physical hardware. Links to our open-source framework and videos of real-world experiments can be found at https://shubhlohiya.github.io/MARBLER/.
<div id='section'>Paperid: <span id='pid'>55, <a href='https://arxiv.org/pdf/2306.06544.pdf' target='_blank'>https://arxiv.org/pdf/2306.06544.pdf</a></span>   <span><a href='https://github.com/andrewnash/Herds-Eye-View' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Andrew Nash, Andrew Vardy, David Churchill
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.06544">Herd's Eye View: Improving Game AI Agent Learning with Collaborative Perception</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a novel perception model named Herd's Eye View (HEV) that adopts a global perspective derived from multiple agents to boost the decision-making capabilities of reinforcement learning (RL) agents in multi-agent environments, specifically in the context of game AI. The HEV approach utilizes cooperative perception to empower RL agents with a global reasoning ability, enhancing their decision-making. We demonstrate the effectiveness of the HEV within simulated game environments and highlight its superior performance compared to traditional ego-centric perception models. This work contributes to cooperative perception and multi-agent reinforcement learning by offering a more realistic and efficient perspective for global coordination and decision-making within game environments. Moreover, our approach promotes broader AI applications beyond gaming by addressing constraints faced by AI in other fields such as robotics. The code is available at https://github.com/andrewnash/Herds-Eye-View
<div id='section'>Paperid: <span id='pid'>56, <a href='https://arxiv.org/pdf/2305.16063.pdf' target='_blank'>https://arxiv.org/pdf/2305.16063.pdf</a></span>   <span><a href='https://github.com/mohsen-raoufi/Kilobots-Individuality-ALife-23/tree/main/Figures' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohsen Raoufi, Pawel Romanczuk, Heiko Hamann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.16063">Individuality in Swarm Robots with the Case Study of Kilobots: Noise, Bug, or Feature?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Inter-individual differences are studied in natural systems, such as fish, bees, and humans, as they contribute to the complexity of both individual and collective behaviors. However, individuality in artificial systems, such as robotic swarms, is undervalued or even overlooked. Agent-specific deviations from the norm in swarm robotics are usually understood as mere noise that can be minimized, for example, by calibration. We observe that robots have consistent deviations and argue that awareness and knowledge of these can be exploited to serve a task. We measure heterogeneity in robot swarms caused by individual differences in how robots act, sense, and oscillate. Our use case is Kilobots and we provide example behaviors where the performance of robots varies depending on individual differences. We show a non-intuitive example of phototaxis with Kilobots where the non-calibrated Kilobots show better performance than the calibrated supposedly ``ideal" one. We measure the inter-individual variations for heterogeneity in sensing and oscillation, too. We briefly discuss how these variations can enhance the complexity of collective behaviors. We suggest that by recognizing and exploring this new perspective on individuality, and hence diversity, in robotic swarms, we can gain a deeper understanding of these systems and potentially unlock new possibilities for their design and implementation of applications.
<div id='section'>Paperid: <span id='pid'>57, <a href='https://arxiv.org/pdf/2305.05938.pdf' target='_blank'>https://arxiv.org/pdf/2305.05938.pdf</a></span>   <span><a href='https://github.com/AIR-THU/DAIR-V2X-Seq' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haibao Yu, Wenxian Yang, Hongzhi Ruan, Zhenwei Yang, Yingjuan Tang, Xu Gao, Xin Hao, Yifeng Shi, Yifeng Pan, Ning Sun, Juan Song, Jirui Yuan, Ping Luo, Zaiqing Nie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.05938">V2X-Seq: A Large-Scale Sequential Dataset for Vehicle-Infrastructure Cooperative Perception and Forecasting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Utilizing infrastructure and vehicle-side information to track and forecast the behaviors of surrounding traffic participants can significantly improve decision-making and safety in autonomous driving. However, the lack of real-world sequential datasets limits research in this area. To address this issue, we introduce V2X-Seq, the first large-scale sequential V2X dataset, which includes data frames, trajectories, vector maps, and traffic lights captured from natural scenery. V2X-Seq comprises two parts: the sequential perception dataset, which includes more than 15,000 frames captured from 95 scenarios, and the trajectory forecasting dataset, which contains about 80,000 infrastructure-view scenarios, 80,000 vehicle-view scenarios, and 50,000 cooperative-view scenarios captured from 28 intersections' areas, covering 672 hours of data. Based on V2X-Seq, we introduce three new tasks for vehicle-infrastructure cooperative (VIC) autonomous driving: VIC3D Tracking, Online-VIC Forecasting, and Offline-VIC Forecasting. We also provide benchmarks for the introduced tasks. Find data, code, and more up-to-date information at \href{https://github.com/AIR-THU/DAIR-V2X-Seq}{https://github.com/AIR-THU/DAIR-V2X-Seq}.
<div id='section'>Paperid: <span id='pid'>58, <a href='https://arxiv.org/pdf/2304.06264.pdf' target='_blank'>https://arxiv.org/pdf/2304.06264.pdf</a></span>   <span><a href='https://github.com/TIERS/uwb-cooperative-mrs-localization' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xianjia Yu, Iacopo Catalano, Paola Torrico MorÃ³n, Sahar Salimpour, Tomi Westerlund, Jorge PeÃ±a Queralta
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.06264">Fusing Odometry, UWB Ranging, and Spatial Detections for Relative Multi-Robot Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This letter presents a cooperative relative multi-robot localization design and experimental study. We propose a flexible Monte Carlo approach leveraging a particle filter to estimate relative states. The estimation can be based on inter-robot Ultra-Wideband (UWB) ranging and onboard odometry alone or dynamically integrated with cooperative spatial object detections from stereo cameras mounted on each robot. The main contributions of this work are as follows. First, we show that a single UWB range is enough to estimate the accurate relative states of two robots when fusing odometry measurements. Second, our experiments also demonstrate that our approach surpasses traditional methods, namely, multilateration, in terms of accuracy. Third, to further increase accuracy, we allow for the integration of cooperative spatial detections. Finally, we show how ROS 2 and Zenoh can be integrated to build a scalable wireless communication solution for multi-robot systems. The experimental validation includes real-time deployment and autonomous navigation based on the relative positioning method. It is worth mentioning that we also address the challenges for UWB-ranging error mitigation for mobile transceivers. The code is available at https://github.com/TIERS/uwb-cooperative-mrs-localization.
<div id='section'>Paperid: <span id='pid'>59, <a href='https://arxiv.org/pdf/2303.07555.pdf' target='_blank'>https://arxiv.org/pdf/2303.07555.pdf</a></span>   <span><a href='https://github.com/gaopeng5/DMGM.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Peng Gao, Qingzhao Zhu, Hongsheng Lu, Chuang Gan, Hao Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.07555">Deep Masked Graph Matching for Correspondence Identification in Collaborative Perception</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Correspondence identification (CoID) is an essential component for collaborative perception in multi-robot systems, such as connected autonomous vehicles. The goal of CoID is to identify the correspondence of objects observed by multiple robots in their own field of view in order for robots to consistently refer to the same objects. CoID is challenging due to perceptual aliasing, object non-covisibility, and noisy sensing. In this paper, we introduce a novel deep masked graph matching approach to enable CoID and address the challenges. Our approach formulates CoID as a graph matching problem and we design a masked neural network to integrate the multimodal visual, spatial, and GPS information to perform CoID. In addition, we design a new technique to explicitly address object non-covisibility caused by occlusion and the vehicle's limited field of view. We evaluate our approach in a variety of street environments using a high-fidelity simulation that integrates the CARLA and SUMO simulators. The experimental results show that our approach outperforms the previous approaches and achieves state-of-the-art CoID performance in connected autonomous driving applications. Our work is available at: https://github.com/gaopeng5/DMGM.git.
<div id='section'>Paperid: <span id='pid'>60, <a href='https://arxiv.org/pdf/2302.02928.pdf' target='_blank'>https://arxiv.org/pdf/2302.02928.pdf</a></span>   <span><a href='https://github.com/YuanYunshuang/GevBEV' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunshuang Yuan, Hao Cheng, Michael Ying Yang, Monika Sester
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.02928">Generating Evidential BEV Maps in Continuous Driving Space</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Safety is critical for autonomous driving, and one aspect of improving safety is to accurately capture the uncertainties of the perception system, especially knowing the unknown. Different from only providing deterministic or probabilistic results, e.g., probabilistic object detection, that only provide partial information for the perception scenario, we propose a complete probabilistic model named GevBEV. It interprets the 2D driving space as a probabilistic Bird's Eye View (BEV) map with point-based spatial Gaussian distributions, from which one can draw evidence as the parameters for the categorical Dirichlet distribution of any new sample point in the continuous driving space. The experimental results show that GevBEV not only provides more reliable uncertainty quantification but also outperforms the previous works on the benchmarks OPV2V and V2V4Real of BEV map interpretation for cooperative perception in simulated and real-world driving scenarios, respectively. A critical factor in cooperative perception is the data transmission size through the communication channels. GevBEV helps reduce communication overhead by selecting only the most important information to share from the learned uncertainty, reducing the average information communicated by 87% with only a slight performance drop. Our code is published at https://github.com/YuanYunshuang/GevBEV.
<div id='section'>Paperid: <span id='pid'>61, <a href='https://arxiv.org/pdf/2301.06230.pdf' target='_blank'>https://arxiv.org/pdf/2301.06230.pdf</a></span>   <span><a href='https://github.com/MISTLab/Swarm-SLAM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Pierre-Yves Lajoie, Giovanni Beltrame
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.06230">Swarm-SLAM : Sparse Decentralized Collaborative Simultaneous Localization and Mapping Framework for Multi-Robot Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Collaborative Simultaneous Localization And Mapping (C-SLAM) is a vital component for successful multi-robot operations in environments without an external positioning system, such as indoors, underground or underwater. In this paper, we introduce Swarm-SLAM, an open-source C-SLAM system that is designed to be scalable, flexible, decentralized, and sparse, which are all key properties in swarm robotics. Our system supports inertial, lidar, stereo, and RGB-D sensing, and it includes a novel inter-robot loop closure prioritization technique that reduces communication and accelerates convergence. We evaluated our ROS-2 implementation on five different datasets, and in a real-world experiment with three robots communicating through an ad-hoc network. Our code is publicly available: https://github.com/MISTLab/Swarm-SLAM
<div id='section'>Paperid: <span id='pid'>62, <a href='https://arxiv.org/pdf/2212.00654.pdf' target='_blank'>https://arxiv.org/pdf/2212.00654.pdf</a></span>   <span><a href='https://github.com/ethz-asl/maplab' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Andrei Cramariuc, Lukas Bernreiter, Florian Tschopp, Marius Fehr, Victor Reijgwart, Juan Nieto, Roland Siegwart, Cesar Cadena
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.00654">maplab 2.0 -- A Modular and Multi-Modal Mapping Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Integration of multiple sensor modalities and deep learning into Simultaneous Localization And Mapping (SLAM) systems are areas of significant interest in current research. Multi-modality is a stepping stone towards achieving robustness in challenging environments and interoperability of heterogeneous multi-robot systems with varying sensor setups. With maplab 2.0, we provide a versatile open-source platform that facilitates developing, testing, and integrating new modules and features into a fully-fledged SLAM system. Through extensive experiments, we show that maplab 2.0's accuracy is comparable to the state-of-the-art on the HILTI 2021 benchmark. Additionally, we showcase the flexibility of our system with three use cases: i) large-scale (approx. 10 km) multi-robot multi-session (23 missions) mapping, ii) integration of non-visual landmarks, and iii) incorporating a semantic object-based loop closure module into the mapping framework. The code is available open-source at https://github.com/ethz-asl/maplab.
<div id='section'>Paperid: <span id='pid'>63, <a href='https://arxiv.org/pdf/2211.15975.pdf' target='_blank'>https://arxiv.org/pdf/2211.15975.pdf</a></span>   <span><a href='https://github.com/PJLab-ADG/PCSim' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyu Cai, Wentao Jiang, Runsheng Xu, Wenquan Zhao, Jiaqi Ma, Si Liu, Yikang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.15975">Analyzing Infrastructure LiDAR Placement with Realistic LiDAR Simulation Library</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, Vehicle-to-Everything(V2X) cooperative perception has attracted increasing attention. Infrastructure sensors play a critical role in this research field; however, how to find the optimal placement of infrastructure sensors is rarely studied. In this paper, we investigate the problem of infrastructure sensor placement and propose a pipeline that can efficiently and effectively find optimal installation positions for infrastructure sensors in a realistic simulated environment. To better simulate and evaluate LiDAR placement, we establish a Realistic LiDAR Simulation library that can simulate the unique characteristics of different popular LiDARs and produce high-fidelity LiDAR point clouds in the CARLA simulator. Through simulating point cloud data in different LiDAR placements, we can evaluate the perception accuracy of these placements using multiple detection models. Then, we analyze the correlation between the point cloud distribution and perception accuracy by calculating the density and uniformity of regions of interest. Experiments show that when using the same number and type of LiDAR, the placement scheme optimized by our proposed method improves the average precision by 15%, compared with the conventional placement scheme in the standard lane scene. We also analyze the correlation between perception performance in the region of interest and LiDAR point cloud distribution and validate that density and uniformity can be indicators of performance. Both the RLS Library and related code will be released at https://github.com/PJLab-ADG/PCSim.
<div id='section'>Paperid: <span id='pid'>64, <a href='https://arxiv.org/pdf/2210.11978.pdf' target='_blank'>https://arxiv.org/pdf/2210.11978.pdf</a></span>   <span><a href='https://github.com/zhongshp/DCL-SLAM.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shipeng Zhong, Yuhua Qi, Zhiqiang Chen, Jin Wu, Hongbo Chen, Ming Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.11978">DCL-SLAM: A Distributed Collaborative LiDAR SLAM Framework for a Robotic Swarm</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To execute collaborative tasks in unknown environments, a robotic swarm needs to establish a global reference frame and locate itself in a shared understanding of the environment. However, it faces many challenges in real-world scenarios, such as the prior information about the environment being absent and poor communication among the team members. This work presents DCL-SLAM, a fully distributed collaborative LiDAR SLAM framework intended for the robotic swarm to simultaneously co-localize in an unknown environment with minimal information exchange. Based on ad-hoc wireless peer-to-peer communication (limited bandwidth and communication range), DCL-SLAM adopts the lightweight LiDAR-Iris descriptor for place recognition and does not require full connectivity among teams. DCL-SLAM includes three main parts: a replaceable single-robot front-end that produces LiDAR odometry results; a distributed loop closure module that detects inter-robot loop closures with keyframes; and a distributed back-end module that adapts distributed pose graph optimizer combined with a pairwise consistent measurement set maximization algorithm to reject spurious inter-robot loop closures. We integrate our proposed framework with diverse open-source LiDAR odometry methods to show its versatility. The proposed system is extensively evaluated on benchmarking datasets and field experiments over various scales and environments. Experimental result shows that DCL-SLAM achieves higher accuracy and lower communication bandwidth than other state-of-art multi-robot SLAM systems. The full source code is available at https://github.com/zhongshp/DCL-SLAM.git.
<div id='section'>Paperid: <span id='pid'>65, <a href='https://arxiv.org/pdf/2208.00759.pdf' target='_blank'>https://arxiv.org/pdf/2208.00759.pdf</a></span>   <span><a href='https://github.com/proroklab/sensor-guided-visual-nav' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jan Blumenkamp, Qingbiao Li, Binyu Wang, Zhe Liu, Amanda Prorok
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2208.00759">See What the Robot Can't See: Learning Cooperative Perception for Visual Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We consider the problem of navigating a mobile robot towards a target in an unknown environment that is endowed with visual sensors, where neither the robot nor the sensors have access to global positioning information and only use first-person-view images. In order to overcome the need for positioning, we train the sensors to encode and communicate relevant viewpoint information to the mobile robot, whose objective it is to use this information to navigate to the target along the shortest path. We overcome the challenge of enabling all the sensors (even those that cannot directly see the target) to predict the direction along the shortest path to the target by implementing a neighborhood-based feature aggregation module using a Graph Neural Network (GNN) architecture. In our experiments, we first demonstrate generalizability to previously unseen environments with various sensor layouts. Our results show that by using communication between the sensors and the robot, we achieve up to 2.0x improvement in SPL (Success weighted by Path Length) when compared to a communication-free baseline. This is done without requiring a global map, positioning data, nor pre-calibration of the sensor network. Second, we perform a zero-shot transfer of our model from simulation to the real world. Laboratory experiments demonstrate the feasibility of our approach in various cluttered environments. Finally, we showcase examples of successful navigation to the target while both the sensor network layout as well as obstacles are dynamically reconfigured as the robot navigates. We provide a video demo, the dataset, trained models, and source code.
  https://www.youtube.com/watch?v=kcmr6RUgucw https://github.com/proroklab/sensor-guided-visual-nav
<div id='section'>Paperid: <span id='pid'>66, <a href='https://arxiv.org/pdf/2203.06464.pdf' target='_blank'>https://arxiv.org/pdf/2203.06464.pdf</a></span>   <span><a href='https://ai4ce.github.io/DeepParticleRobot/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jeremy Shen, Erdong Xiao, Yuchen Liu, Chen Feng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2203.06464">A Deep Reinforcement Learning Environment for Particle Robot Navigation and Object Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Particle robots are novel biologically-inspired robotic systems where locomotion can be achieved collectively and robustly, but not independently. While its control is currently limited to a hand-crafted policy for basic locomotion tasks, such a multi-robot system could be potentially controlled via Deep Reinforcement Learning (DRL) for different tasks more efficiently. However, the particle robot system presents a new set of challenges for DRL differing from existing swarm robotics systems: the low degrees of freedom of each robot and the increased necessity of coordination between robots. We present a 2D particle robot simulator using the OpenAI Gym interface and Pymunk as the physics engine, and introduce new tasks and challenges to research the underexplored applications of DRL in the particle robot system. Moreover, we use Stable-baselines3 to provide a set of benchmarks for the tasks. Current baseline DRL algorithms show signs of achieving the tasks but are yet unable to reach the performance of the hand-crafted policy. Further development of DRL algorithms is necessary in order to accomplish the proposed tasks.
<div id='section'>Paperid: <span id='pid'>67, <a href='https://arxiv.org/pdf/2008.08050.pdf' target='_blank'>https://arxiv.org/pdf/2008.08050.pdf</a></span>   <span><a href='http://github.com/ctu-mrs,' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tomas Baca, Matej Petrlik, Matous Vrba, Vojtech Spurny, Robert Penicka, Daniel Hert, Martin Saska
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2008.08050">The MRS UAV System: Pushing the Frontiers of Reproducible Research, Real-world Deployment, and Education with Autonomous Unmanned Aerial Vehicles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a multirotor Unmanned Aerial Vehicle control (UAV) and estimation system for supporting replicable research through realistic simulations and real-world experiments. We propose a unique multi-frame localization paradigm for estimating the states of a UAV in various frames of reference using multiple sensors simultaneously. The system enables complex missions in GNSS and GNSS-denied environments, including outdoor-indoor transitions and the execution of redundant estimators for backing up unreliable localization sources. Two feedback control designs are presented: one for precise and aggressive maneuvers, and the other for stable and smooth flight with a noisy state estimate. The proposed control and estimation pipeline are constructed without using the Euler/Tait-Bryan angle representation of orientation in 3D. Instead, we rely on rotation matrices and a novel heading-based convention to represent the one free rotational degree-of-freedom in 3D of a standard multirotor helicopter. We provide an actively maintained and well-documented open-source implementation, including realistic simulation of UAV, sensors, and localization systems. The proposed system is the product of years of applied research on multi-robot systems, aerial swarms, aerial manipulation, motion planning, and remote sensing. All our results have been supported by real-world system deployment that shaped the system into the form presented here. In addition, the system was utilized during the participation of our team from the CTU in Prague in the prestigious MBZIRC 2017 and 2020 robotics competitions, and also in the DARPA SubT challenge. Each time, our team was able to secure top places among the best competitors from all over the world. On each occasion, the challenges has motivated the team to improve the system and to gain a great amount of high-quality experience within tight deadlines.
<div id='section'>Paperid: <span id='pid'>68, <a href='https://arxiv.org/pdf/2509.24927.pdf' target='_blank'>https://arxiv.org/pdf/2509.24927.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>An Guo, Shuoxiao Zhang, Enyi Tang, Xinyu Gao, Haomin Pang, Haoxiang Tian, Yanzhou Mu, Wu Wen, Chunrong Fang, Zhenyu Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24927">When Autonomous Vehicle Meets V2X Cooperative Perception: How Far Are We?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the tremendous advancement of deep learning and communication technology, Vehicle-to-Everything (V2X) cooperative perception has the potential to address limitations in sensing distant objects and occlusion for a single-agent perception system. V2X cooperative perception systems are software systems characterized by diverse sensor types and cooperative agents, varying fusion schemes, and operation under different communication conditions. Therefore, their complex composition gives rise to numerous operational challenges. Furthermore, when cooperative perception systems produce erroneous predictions, the types of errors and their underlying causes remain insufficiently explored. To bridge this gap, we take an initial step by conducting an empirical study of V2X cooperative perception. To systematically evaluate the impact of cooperative perception on the ego vehicle's perception performance, we identify and analyze six prevalent error patterns in cooperative perception systems. We further conduct a systematic evaluation of the critical components of these systems through our large-scale study and identify the following key findings: (1) The LiDAR-based cooperation configuration exhibits the highest perception performance; (2) Vehicle-to-infrastructure (V2I) and vehicle-to-vehicle (V2V) communication exhibit distinct cooperative perception performance under different fusion schemes; (3) Increased cooperative perception errors may result in a higher frequency of driving violations; (4) Cooperative perception systems are not robust against communication interference when running online. Our results reveal potential risks and vulnerabilities in critical components of cooperative perception systems. We hope that our findings can better promote the design and repair of cooperative perception systems.
<div id='section'>Paperid: <span id='pid'>69, <a href='https://arxiv.org/pdf/2509.24927.pdf' target='_blank'>https://arxiv.org/pdf/2509.24927.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>An Guo, Shuoxiao Zhang, Enyi Tang, Xinyu Gao, Haomin Pang, Haoxiang Tian, Yanzhou Mu, Wu Wen, Chunrong Fang, Zhenyu Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24927">When Autonomous Vehicle Meets V2X Cooperative Perception: How Far Are We?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the tremendous advancement of deep learning and communication technology, Vehicle-to-Everything (V2X) cooperative perception has the potential to address limitations in sensing distant objects and occlusion for a single-agent perception system. V2X cooperative perception systems are software systems characterized by diverse sensor types and cooperative agents, varying fusion schemes, and operation under different communication conditions. Therefore, their complex composition gives rise to numerous operational challenges. Furthermore, when cooperative perception systems produce erroneous predictions, the types of errors and their underlying causes remain insufficiently explored. To bridge this gap, we take an initial step by conducting an empirical study of V2X cooperative perception. To systematically evaluate the impact of cooperative perception on the ego vehicle's perception performance, we identify and analyze six prevalent error patterns in cooperative perception systems. We further conduct a systematic evaluation of the critical components of these systems through our large-scale study and identify the following key findings: (1) The LiDAR-based cooperation configuration exhibits the highest perception performance; (2) Vehicle-to-infrastructure (V2I) and vehicle-to-vehicle (V2V) communication exhibit distinct cooperative perception performance under different fusion schemes; (3) Increased cooperative perception errors may result in a higher frequency of driving violations; (4) Cooperative perception systems are not robust against communication interference when running online. Our results reveal potential risks and vulnerabilities in critical components of cooperative perception systems. We hope that our findings can better promote the design and repair of cooperative perception systems.
<div id='section'>Paperid: <span id='pid'>70, <a href='https://arxiv.org/pdf/2506.07419.pdf' target='_blank'>https://arxiv.org/pdf/2506.07419.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>An Guo, Xinyu Gao, Chunrong Fang, Haoxiang Tian, Weisong Sun, Yanzhou Mu, Shuncheng Tang, Lei Ma, Zhenyu Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.07419">Generate Realistic Test Scenes for V2X Communication Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurately perceiving complex driving environments is essential for ensuring the safe operation of autonomous vehicles. With the tremendous progress in deep learning and communication technologies, cooperative perception with Vehicle-to-Everything (V2X) technologies has emerged as a solution to overcome the limitations of single-agent perception systems in perceiving distant objects and occlusions. Despite the considerable advancements, V2X cooperative perception systems require thorough testing and continuous enhancement of system performance. Given that V2X driving scenes entail intricate communications with multiple vehicles across various geographic locations, creating V2X test scenes for these systems poses a significant challenge. Moreover, current testing methodologies rely on manual data collection and labeling, which are both time-consuming and costly.
  In this paper, we design and implement V2XGen, an automated testing generation tool for V2X cooperative perception systems. V2XGen utilizes a high-fidelity approach to generate realistic cooperative object instances and strategically place them within the background data in crucial positions. Furthermore, V2XGen adopts a fitness-guided V2X scene generation strategy for the transformed scene generation process and improves testing efficiency. We conduct experiments on V2XGen using multiple cooperative perception systems with different fusion schemes to assess its performance on various tasks. The experimental results demonstrate that V2XGen is capable of generating realistic test scenes and effectively detecting erroneous behaviors in different V2X-oriented driving conditions. Furthermore, the results validate that retraining systems under test with the generated scenes can enhance average detection precision while reducing occlusion and long-range perception errors.
<div id='section'>Paperid: <span id='pid'>71, <a href='https://arxiv.org/pdf/2408.16470.pdf' target='_blank'>https://arxiv.org/pdf/2408.16470.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>An Guo, Xinyu Gao, Zhenyu Chen, Yuan Xiao, Jiakai Liu, Xiuting Ge, Weisong Sun, Chunrong Fang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.16470">CooTest: An Automated Testing Approach for V2X Communication Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Perceiving the complex driving environment precisely is crucial to the safe operation of autonomous vehicles. With the tremendous advancement of deep learning and communication technology, Vehicle-to-Everything (V2X) collaboration has the potential to address limitations in sensing distant objects and occlusion for a single-agent perception system. However, despite spectacular progress, several communication challenges can undermine the effectiveness of multi-vehicle cooperative perception. The low interpretability of Deep Neural Networks (DNNs) and the high complexity of communication mechanisms make conventional testing techniques inapplicable for the cooperative perception of autonomous driving systems (ADS). Besides, the existing testing techniques, depending on manual data collection and labeling, become time-consuming and prohibitively expensive.
  In this paper, we design and implement CooTest, the first automated testing tool of the V2X-oriented cooperative perception module. CooTest devises the V2X-specific metamorphic relation and equips communication and weather transformation operators that can reflect the impact of the various cooperative driving factors to produce transformed scenes. Furthermore, we adopt a V2X-oriented guidance strategy for the transformed scene generation process and improve testing efficiency. We experiment CooTest with multiple cooperative perception models with different fusion schemes to evaluate its performance on different tasks. The experiment results show that CooTest can effectively detect erroneous behaviors under various V2X-oriented driving conditions. Also, the results confirm that CooTest can improve detection average precision and decrease misleading cooperation errors by retraining with the generated scenes.
<div id='section'>Paperid: <span id='pid'>72, <a href='https://arxiv.org/pdf/2302.07433.pdf' target='_blank'>https://arxiv.org/pdf/2302.07433.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Huan Yin, Xuecheng Xu, Sha Lu, Xieyuanli Chen, Rong Xiong, Shaojie Shen, Cyrill Stachniss, Yue Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.07433">A Survey on Global LiDAR Localization: Challenges, Advances and Open Problems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Knowledge about the own pose is key for all mobile robot applications. Thus pose estimation is part of the core functionalities of mobile robots. Over the last two decades, LiDAR scanners have become the standard sensor for robot localization and mapping. This article aims to provide an overview of recent progress and advancements in LiDAR-based global localization. We begin by formulating the problem and exploring the application scope. We then present a review of the methodology, including recent advancements in several topics, such as maps, descriptor extraction, and cross-robot localization. The contents of the article are organized under three themes. The first theme concerns the combination of global place retrieval and local pose estimation. The second theme is upgrading single-shot measurements to sequential ones for sequential global localization. Finally, the third theme focuses on extending single-robot global localization to cross-robot localization in multi-robot systems. We conclude the survey with a discussion of open challenges and promising directions in global LiDAR localization. To our best knowledge, this is the first comprehensive survey on global LiDAR localization for mobile robots.
<div id='section'>Paperid: <span id='pid'>73, <a href='https://arxiv.org/pdf/2408.00606.pdf' target='_blank'>https://arxiv.org/pdf/2408.00606.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tongtong Feng, Xin Wang, Feilin Han, Leping Zhang, Wenwu Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.00606">U2UData: A Large-scale Cooperative Perception Dataset for Swarm UAVs Autonomous Flight</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modern perception systems for autonomous flight are sensitive to occlusion and have limited long-range capability, which is a key bottleneck in improving low-altitude economic task performance. Recent research has shown that the UAV-to-UAV (U2U) cooperative perception system has great potential to revolutionize the autonomous flight industry. However, the lack of a large-scale dataset is hindering progress in this area. This paper presents U2UData, the first large-scale cooperative perception dataset for swarm UAVs autonomous flight. The dataset was collected by three UAVs flying autonomously in the U2USim, covering a 9 km$^2$ flight area. It comprises 315K LiDAR frames, 945K RGB and depth frames, and 2.41M annotated 3D bounding boxes for 3 classes. It also includes brightness, temperature, humidity, smoke, and airflow values covering all flight routes. U2USim is the first real-world mapping swarm UAVs simulation environment. It takes Yunnan Province as the prototype and includes 4 terrains, 7 weather conditions, and 8 sensor types. U2UData introduces two perception tasks: cooperative 3D object detection and cooperative 3D object tracking. This paper provides comprehensive benchmarks of recent cooperative perception algorithms on these tasks.
<div id='section'>Paperid: <span id='pid'>74, <a href='https://arxiv.org/pdf/2311.02948.pdf' target='_blank'>https://arxiv.org/pdf/2311.02948.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiangyong Wen, Yingjian Wang, Xi Zheng, Kaiwei Wang, Chao Xu, Fei Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.02948">Simultaneous Time Synchronization and Mutual Localization for Multi-robot System</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mutual localization stands as a foundational component within various domains of multi-robot systems.
  Nevertheless, in relative pose estimation, time synchronization is usually underappreciated and rarely addressed, although it significantly influences estimation accuracy.
  In this paper, we introduce time synchronization into mutual localization to recover the time offset and relative poses between robots simultaneously.
  Under a constant velocity assumption in a short time, we fuse time offset estimation with our previous bearing-based mutual localization by a novel error representation.
  Based on the error model, we formulate a joint optimization problem and utilize semi-definite relaxation (SDR) to furnish a lossless relaxation.
  By solving the relaxed problem, time synchronization and relative pose estimation can be achieved when time drift between robots is limited.
  To enhance the application range of time offset estimation, we further propose an iterative method to recover the time offset from coarse to fine.
  Comparisons between the proposed method and the existing ones through extensive simulation tests present prominent benefits of time synchronization on mutual localization.
  Moreover, real-world experiments are conducted to show the practicality and robustness.
<div id='section'>Paperid: <span id='pid'>75, <a href='https://arxiv.org/pdf/2306.11259.pdf' target='_blank'>https://arxiv.org/pdf/2306.11259.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Baozhe Zhang, Xinwei Chen, Zhehan Li, Giovanni Beltrame, Chao Xu, Fei Gao, Yanjun Cao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.11259">CoNi-MPC: Cooperative Non-inertial Frame Based Model Predictive Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a novel solution for UAV control in cooperative multi-robot systems, which can be used in various scenarios such as leader-following, landing on a moving base, or specific relative motion with a target. Unlike classical methods that tackle UAV control in the world frame, we directly control the UAV in the target coordinate frame, without making motion assumptions about the target. In detail, we formulate a non-linear model predictive controller of a UAV, referred to as the agent, within a non-inertial frame (i.e., the target frame). The system requires the relative states (pose and velocity), the angular velocity and the accelerations of the target, which can be obtained by relative localization methods and ubiquitous MEMS IMU sensors, respectively. This framework eliminates dependencies that are vital in classical solutions, such as accurate state estimation for both the agent and target, prior knowledge of the target motion model, and continuous trajectory re-planning for some complex tasks. We have performed extensive simulations to investigate the control performance with varying motion characteristics of the target. Furthermore, we conducted real robot experiments, employing either simulated relative pose estimation from motion capture systems indoors or directly from our previous relative pose estimation devices outdoors, to validate the applicability and feasibility of the proposed approach.
<div id='section'>Paperid: <span id='pid'>76, <a href='https://arxiv.org/pdf/2302.01036.pdf' target='_blank'>https://arxiv.org/pdf/2302.01036.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiren Xun, Jian Huang, Zhehan Li, Zhenjun Ying, Yingjian Wang, Chao Xu, Fei Gao, Yanjun Cao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.01036">CREPES: Cooperative RElative Pose Estimation System</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mutual localization plays a crucial role in multi-robot cooperation. CREPES, a novel system that focuses on six degrees of freedom (DOF) relative pose estimation for multi-robot systems, is proposed in this paper. CREPES has a compact hardware design using active infrared (IR) LEDs, an IR fish-eye camera, an ultra-wideband (UWB) module and an inertial measurement unit (IMU). By leveraging IR light communication, the system solves data association between visual detection and UWB ranging. Ranging measurements from the UWB and directional information from the camera offer relative 3-DOF position estimation. Combining the mutual relative position with neighbors and the gravity constraints provided by IMUs, we can estimate the 6-DOF relative pose from a single frame of sensor measurements. In addition, we design an estimator based on the error-state Kalman filter (ESKF) to enhance system accuracy and robustness. When multiple neighbors are available, a Pose Graph Optimization (PGO) algorithm is applied to further improve system accuracy. We conduct enormous experiments to demonstrate CREPES' accuracy between robot pairs and a team of robots, as well as performance under challenging conditions.
<div id='section'>Paperid: <span id='pid'>77, <a href='https://arxiv.org/pdf/2503.02723.pdf' target='_blank'>https://arxiv.org/pdf/2503.02723.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Faryal Batool, Malaika Zafar, Yasheerah Yaqoot, Roohan Ahmed Khan, Muhammad Haris Khan, Aleksey Fedoseev, Dzmitry Tsetserukou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.02723">ImpedanceGPT: VLM-driven Impedance Control of Swarm of Mini-drones for Intelligent Navigation in Dynamic Environment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Swarm robotics plays a crucial role in enabling autonomous operations in dynamic and unpredictable environments. However, a major challenge remains ensuring safe and efficient navigation in environments filled with both dynamic alive (e.g., humans) and dynamic inanimate (e.g., non-living objects) obstacles. In this paper, we propose ImpedanceGPT, a novel system that combines a Vision-Language Model (VLM) with retrieval-augmented generation (RAG) to enable real-time reasoning for adaptive navigation of mini-drone swarms in complex environments.
  The key innovation of ImpedanceGPT lies in the integration of VLM and RAG, which provides the drones with enhanced semantic understanding of their surroundings. This enables the system to dynamically adjust impedance control parameters in response to obstacle types and environmental conditions. Our approach not only ensures safe and precise navigation but also improves coordination between drones in the swarm.
  Experimental evaluations demonstrate the effectiveness of the system. The VLM-RAG framework achieved an obstacle detection and retrieval accuracy of 80 % under optimal lighting. In static environments, drones navigated dynamic inanimate obstacles at 1.4 m/s but slowed to 0.7 m/s with increased separation around humans. In dynamic environments, speed adjusted to 1.0 m/s near hard obstacles, while reducing to 0.6 m/s with higher deflection to safely avoid moving humans.
<div id='section'>Paperid: <span id='pid'>78, <a href='https://arxiv.org/pdf/2503.02723.pdf' target='_blank'>https://arxiv.org/pdf/2503.02723.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Faryal Batool, Yasheerah Yaqoot, Malaika Zafar, Roohan Ahmed Khan, Muhammad Haris Khan, Aleksey Fedoseev, Dzmitry Tsetserukou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.02723">ImpedanceGPT: VLM-driven Impedance Control of Swarm of Mini-drones for Intelligent Navigation in Dynamic Environment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Swarm robotics plays a crucial role in enabling autonomous operations in dynamic and unpredictable environments. However, a major challenge remains ensuring safe and efficient navigation in environments filled with both dynamic alive (e.g., humans) and dynamic inanimate (e.g., non-living objects) obstacles. In this paper, we propose ImpedanceGPT, a novel system that combines a Vision-Language Model (VLM) with retrieval-augmented generation (RAG) to enable real-time reasoning for adaptive navigation of mini-drone swarms in complex environments. The key innovation of ImpedanceGPT lies in the integration of VLM and RAG, which provides the drones with enhanced semantic understanding of their surroundings. This enables the system to dynamically adjust impedance control parameters in response to obstacle types and environmental conditions. Our approach not only ensures safe and precise navigation but also improves coordination between drones in the swarm. Experimental evaluations demonstrate the effectiveness of the system. The VLM-RAG framework achieved an obstacle detection and retrieval accuracy of 80 % under optimal lighting. In static environments, drones navigated dynamic inanimate obstacles at 1.4 m/s but slowed to 0.7 m/s with increased separation around humans. In dynamic environments, speed adjusted to 1.0 m/s near hard obstacles, while reducing to 0.6 m/s with higher deflection to safely avoid moving humans.
<div id='section'>Paperid: <span id='pid'>79, <a href='https://arxiv.org/pdf/2503.02723.pdf' target='_blank'>https://arxiv.org/pdf/2503.02723.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Faryal Batool, Yasheerah Yaqoot, Malaika Zafar, Roohan Ahmed Khan, Muhammad Haris Khan, Aleksey Fedoseev, Dzmitry Tsetserukou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.02723">ImpedanceGPT: VLM-driven Impedance Control of Swarm of Mini-drones for Intelligent Navigation in Dynamic Environment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Swarm robotics plays a crucial role in enabling autonomous operations in dynamic and unpredictable environments. However, a major challenge remains ensuring safe and efficient navigation in environments filled with both dynamic alive (e.g., humans) and dynamic inanimate (e.g., non-living objects) obstacles. In this paper, we propose ImpedanceGPT, a novel system that combines a Vision-Language Model (VLM) with retrieval-augmented generation (RAG) to enable real-time reasoning for adaptive navigation of mini-drone swarms in complex environments. The key innovation of ImpedanceGPT lies in the integration of VLM and RAG, which provides the drones with enhanced semantic understanding of their surroundings. This enables the system to dynamically adjust impedance control parameters in response to obstacle types and environmental conditions. Our approach not only ensures safe and precise navigation but also improves coordination between drones in the swarm. Experimental evaluations demonstrate the effectiveness of the system. The VLM-RAG framework achieved an obstacle detection and retrieval accuracy of 80 % under optimal lighting. In static environments, drones navigated dynamic inanimate obstacles at 1.4 m/s but slowed to 0.7 m/s with increased separation around humans. In dynamic environments, speed adjusted to 1.0 m/s near hard obstacles, while reducing to 0.6 m/s with higher deflection to safely avoid moving humans.
<div id='section'>Paperid: <span id='pid'>80, <a href='https://arxiv.org/pdf/2303.08420.pdf' target='_blank'>https://arxiv.org/pdf/2303.08420.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiyue Guo, Junjie Hu, Hujun Bao, Guofeng Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.08420">Descriptor Distillation for Efficient Multi-Robot SLAM</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Performing accurate localization while maintaining the low-level communication bandwidth is an essential challenge of multi-robot simultaneous localization and mapping (MR-SLAM). In this paper, we tackle this problem by generating a compact yet discriminative feature descriptor with minimum inference time. We propose descriptor distillation that formulates the descriptor generation into a learning problem under the teacher-student framework. To achieve real-time descriptor generation, we design a compact student network and learn it by transferring the knowledge from a pre-trained large teacher model. To reduce the descriptor dimensions from the teacher to the student, we propose a novel loss function that enables the knowledge transfer between two different dimensional descriptors. The experimental results demonstrate that our model is 30% lighter than the state-of-the-art model and produces better descriptors in patch matching. Moreover, we build a MR-SLAM system based on the proposed method and show that our descriptor distillation can achieve higher localization performance for MR-SLAM with lower bandwidth.
<div id='section'>Paperid: <span id='pid'>81, <a href='https://arxiv.org/pdf/2506.19769.pdf' target='_blank'>https://arxiv.org/pdf/2506.19769.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shulan Ruan, Rongwei Wang, Xuchen Shen, Huijie Liu, Baihui Xiao, Jun Shi, Kun Zhang, Zhenya Huang, Yu Liu, Enhong Chen, You He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.19769">A Survey of Multi-sensor Fusion Perception for Embodied AI: Background, Methods, Challenges and Prospects</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-sensor fusion perception (MSFP) is a key technology for embodied AI, which can serve a variety of downstream tasks (e.g., 3D object detection and semantic segmentation) and application scenarios (e.g., autonomous driving and swarm robotics). Recently, impressive achievements on AI-based MSFP methods have been reviewed in relevant surveys. However, we observe that the existing surveys have some limitations after a rigorous and detailed investigation. For one thing, most surveys are oriented to a single task or research field, such as 3D object detection or autonomous driving. Therefore, researchers in other related tasks often find it difficult to benefit directly. For another, most surveys only introduce MSFP from a single perspective of multi-modal fusion, while lacking consideration of the diversity of MSFP methods, such as multi-view fusion and time-series fusion. To this end, in this paper, we hope to organize MSFP research from a task-agnostic perspective, where methods are reported from various technical views. Specifically, we first introduce the background of MSFP. Next, we review multi-modal and multi-agent fusion methods. A step further, time-series fusion methods are analyzed. In the era of LLM, we also investigate multimodal LLM fusion methods. Finally, we discuss open challenges and future directions for MSFP. We hope this survey can help researchers understand the important progress in MSFP and provide possible insights for future research.
<div id='section'>Paperid: <span id='pid'>82, <a href='https://arxiv.org/pdf/2504.15418.pdf' target='_blank'>https://arxiv.org/pdf/2504.15418.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Victoria Marie Tuck, Hardik Parwana, Pei-Wei Chen, Georgios Fainekos, Bardh Hoxha, Hideki Okamoto, S. Shankar Sastry, Sanjit A. Seshia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.15418">MRTA-Sim: A Modular Simulator for Multi-Robot Allocation, Planning, and Control in Open-World Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces MRTA-Sim, a Python/ROS2/Gazebo simulator for testing approaches to Multi-Robot Task Allocation (MRTA) problems on simulated robots in complex, indoor environments. Grid-based approaches to MRTA problems can be too restrictive for use in complex, dynamic environments such in warehouses, department stores, hospitals, etc. However, approaches that operate in free-space often operate at a layer of abstraction above the control and planning layers of a robot and make an assumption on approximate travel time between points of interest in the system. These abstractions can neglect the impact of the tight space and multi-agent interactions on the quality of the solution. Therefore, MRTA solutions should be tested with the navigation stacks of the robots in mind, taking into account robot planning, conflict avoidance between robots, and human interaction and avoidance. This tool connects the allocation output of MRTA solvers to individual robot planning using the NAV2 stack and local, centralized multi-robot deconfliction using Control Barrier Function-Quadrtic Programs (CBF-QPs), creating a platform closer to real-world operation for more comprehensive testing of these approaches. The simulation architecture is modular so that users can swap out methods at different levels of the stack. We show the use of our system with a Satisfiability Modulo Theories (SMT)-based approach to dynamic MRTA on a fleet of indoor delivery robots.
<div id='section'>Paperid: <span id='pid'>83, <a href='https://arxiv.org/pdf/2403.11737.pdf' target='_blank'>https://arxiv.org/pdf/2403.11737.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Victoria Marie Tuck, Pei-Wei Chen, Georgios Fainekos, Bardh Hoxha, Hideki Okamoto, S. Shankar Sastry, Sanjit A. Seshia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.11737">SMT-Based Dynamic Multi-Robot Task Allocation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Robot Task Allocation (MRTA) is a problem that arises in many application domains including package delivery, warehouse robotics, and healthcare. In this work, we consider the problem of MRTA for a dynamic stream of tasks with task deadlines and capacitated agents (capacity for more than one simultaneous task). Previous work commonly focuses on the static case, uses specialized algorithms for restrictive task specifications, or lacks guarantees. We propose an approach to Dynamic MRTA for capacitated robots that is based on Satisfiability Modulo Theories (SMT) solving and addresses these concerns. We show our approach is both sound and complete, and that the SMT encoding is general, enabling extension to a broader class of task specifications. We show how to leverage the incremental solving capabilities of SMT solvers, keeping learned information when allocating new tasks arriving online, and to solve non-incrementally, which we provide runtime comparisons of. Additionally, we provide an algorithm to start with a smaller but potentially incomplete encoding that can iteratively be adjusted to the complete encoding. We evaluate our method on a parameterized set of benchmarks encoding multi-robot delivery created from a graph abstraction of a hospital-like environment. The effectiveness of our approach is demonstrated using a range of encodings, including quantifier-free theories of uninterpreted functions and linear or bitvector arithmetic across multiple solvers.
<div id='section'>Paperid: <span id='pid'>84, <a href='https://arxiv.org/pdf/2302.13029.pdf' target='_blank'>https://arxiv.org/pdf/2302.13029.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yukuan Jia, Ruiqing Mao, Yuxuan Sun, Sheng Zhou, Zhisheng Niu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.13029">MASS: Mobility-Aware Sensor Scheduling of Cooperative Perception for Connected Automated Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Timely and reliable environment perception is fundamental to safe and efficient automated driving. However, the perception of standalone intelligence inevitably suffers from occlusions. A new paradigm, Cooperative Perception (CP), comes to the rescue by sharing sensor data from another perspective, i.e., from a cooperative vehicle (CoV). Due to the limited communication bandwidth, it is essential to schedule the most beneficial CoV, considering both the viewpoints and communication quality. Existing methods rely on the exchange of meta-information, such as visibility maps, to predict the perception gains from nearby vehicles, which induces extra communication and processing overhead. In this paper, we propose a new approach, learning while scheduling, for distributed scheduling of CP. The solution enables CoVs to predict the perception gains using past observations, leveraging the temporal continuity of perception gains. Specifically, we design a mobility-aware sensor scheduling (MASS) algorithm based on the restless multi-armed bandit (RMAB) theory to maximize the expected average perception gain. An upper bound on the expected average learning regret is proved, which matches the lower bound of any online algorithm up to a logarithmic factor. Extensive simulations are carried out on realistic traffic traces. The results show that the proposed MASS algorithm achieves the best average perception gain and improves recall by up to 4.2 percentage points compared to other learning-based algorithms. Finally, a case study on a trace of LiDAR frames qualitatively demonstrates the superiority of adaptive exploration, the key element of the MASS algorithm.
<div id='section'>Paperid: <span id='pid'>85, <a href='https://arxiv.org/pdf/2202.06085.pdf' target='_blank'>https://arxiv.org/pdf/2202.06085.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yukuan Jia, Ruiqing Mao, Yuxuan Sun, Sheng Zhou, Zhisheng Niu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2202.06085">Online V2X Scheduling for Raw-Level Cooperative Perception</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cooperative perception of connected vehicles comes to the rescue when the field of view restricts stand-alone intelligence. While raw-level cooperative perception preserves most information to guarantee accuracy, it is demanding in communication bandwidth and computation power. Therefore, it is important to schedule the most beneficial vehicle to share its sensor in terms of supplementary view and stable network connection. In this paper, we present a model of raw-level cooperative perception and formulate the energy minimization problem of sensor sharing scheduling as a variant of the Multi-Armed Bandit (MAB) problem. Specifically, volatility of the neighboring vehicles, heterogeneity of V2X channels, and the time-varying traffic context are taken into consideration. Then we propose an online learning-based algorithm with logarithmic performance loss, achieving a decent trade-off between exploration and exploitation. Simulation results under different scenarios indicate that the proposed algorithm quickly learns to schedule the optimal cooperative vehicle and saves more energy as compared to baseline algorithms.
<div id='section'>Paperid: <span id='pid'>86, <a href='https://arxiv.org/pdf/2410.04320.pdf' target='_blank'>https://arxiv.org/pdf/2410.04320.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haonan An, Zhengru Fang, Yuang Zhang, Senkang Hu, Xianhao Chen, Guowen Xu, Yuguang Fang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.04320">Channel-Aware Throughput Maximization for Cooperative Data Fusion in CAV</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Connected and autonomous vehicles (CAVs) have garnered significant attention due to their extended perception range and enhanced sensing coverage. To address challenges such as blind spots and obstructions, CAVs employ vehicle-to-vehicle (V2V) communications to aggregate sensory data from surrounding vehicles. However, cooperative perception is often constrained by the limitations of achievable network throughput and channel quality. In this paper, we propose a channel-aware throughput maximization approach to facilitate CAV data fusion, leveraging a self-supervised autoencoder for adaptive data compression. We formulate the problem as a mixed integer programming (MIP) model, which we decompose into two sub-problems to derive optimal data rate and compression ratio solutions under given link conditions. An autoencoder is then trained to minimize bitrate with the determined compression ratio, and a fine-tuning strategy is employed to further reduce spectrum resource consumption. Experimental evaluation on the OpenCOOD platform demonstrates the effectiveness of our proposed algorithm, showing more than 20.19\% improvement in network throughput and a 9.38\% increase in average precision (AP@IoU) compared to state-of-the-art methods, with an optimal latency of 19.99 ms.
<div id='section'>Paperid: <span id='pid'>87, <a href='https://arxiv.org/pdf/2209.06415.pdf' target='_blank'>https://arxiv.org/pdf/2209.06415.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Senthil Hariharan Arul, Amrit Singh Bedi, Dinesh Manocha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.06415">DMCA: Dense Multi-agent Navigation using Attention and Communication</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In decentralized multi-robot navigation, ensuring safe and efficient movement with limited environmental awareness remains a challenge. While robots traditionally navigate based on local observations, this approach falters in complex environments. A possible solution is to enhance understanding of the world through inter-agent communication, but mere information broadcasting falls short in efficiency. In this work, we address this problem by simultaneously learning decentralized multi-robot collision avoidance and selective inter-agent communication. We use a multi-head self-attention mechanism that encodes observable information from neighboring robots into a concise and fixed-length observation vector, thereby handling varying numbers of neighbors. Our method focuses on improving navigation performance through selective communication. We cast the communication selection as a link prediction problem, where the network determines the necessity of establishing a communication link with a specific neighbor based on the observable state information. The communicated information enhances the neighbor's observation and aids in selecting an appropriate navigation plan. By training the network end-to-end, we concurrently learn the optimal weights for the observation encoder, communication selection, and navigation components. We showcase the benefits of our approach by achieving safe and efficient navigation among multiple robots, even in dense and challenging environments. Comparative evaluations against various learning-based and model-based baselines demonstrate our superior navigation performance, resulting in an impressive improvement of up to 24% in success rate within complex evaluation scenarios.
<div id='section'>Paperid: <span id='pid'>88, <a href='https://arxiv.org/pdf/2209.05738.pdf' target='_blank'>https://arxiv.org/pdf/2209.05738.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aakriti Agrawal, Amrit Singh Bedi, Dinesh Manocha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.05738">RTAW: An Attention Inspired Reinforcement Learning Method for Multi-Robot Task Allocation in Warehouse Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a novel reinforcement learning based algorithm for multi-robot task allocation problem in warehouse environments. We formulate it as a Markov Decision Process and solve via a novel deep multi-agent reinforcement learning method (called RTAW) with attention inspired policy architecture. Hence, our proposed policy network uses global embeddings that are independent of the number of robots/tasks. We utilize proximal policy optimization algorithm for training and use a carefully designed reward to obtain a converged policy. The converged policy ensures cooperation among different robots to minimize total travel delay (TTD) which ultimately improves the makespan for a sufficiently large task-list. In our extensive experiments, we compare the performance of our RTAW algorithm to state of the art methods such as myopic pickup distance minimization (greedy) and regret based baselines on different navigation schemes. We show an improvement of upto 14% (25-1000 seconds) in TTD on scenarios with hundreds or thousands of tasks for different challenging warehouse layouts and task generation schemes. We also demonstrate the scalability of our approach by showing performance with up to $1000$ robots in simulations.
<div id='section'>Paperid: <span id='pid'>89, <a href='https://arxiv.org/pdf/2312.07948.pdf' target='_blank'>https://arxiv.org/pdf/2312.07948.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ye Tao, Ehsan Javanmardi, Pengfei Lin, Jin Nakazato, Yuze Jiang, Manabu Tsukada, Hiroshi Esaki
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.07948">Zero-Knowledge Proof of Traffic: A Deterministic and Privacy-Preserving Cross Verification Mechanism for Cooperative Perception Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cooperative perception is crucial for connected automated vehicles in intelligent transportation systems (ITSs); however, ensuring the authenticity of perception data remains a challenge as the vehicles cannot verify events that they do not witness independently. Various studies have been conducted on establishing the authenticity of data, such as trust-based statistical methods and plausibility-based methods. However, these methods are limited as they require prior knowledge such as previous sender behaviors or predefined rules to evaluate the authenticity. To overcome this limitation, this study proposes a novel approach called zero-knowledge Proof of Traffic (zk-PoT), which involves generating cryptographic proofs to the traffic observations. Multiple independent proofs regarding the same vehicle can be deterministically cross-verified by any receivers without relying on ground truth, probabilistic, or plausibility evaluations. Additionally, no private information is compromised during the entire procedure. A full on-board unit software stack that reflects the behavior of zk-PoT is implemented within a specifically designed simulator called Flowsim. A comprehensive experimental analysis is then conducted using synthesized city-scale simulations, which demonstrates that zk-PoT's cross-verification ratio ranges between 80 % to 96 %, and 80 % of the verification is achieved in 2 s, with a protocol overhead of approximately 25 %. Furthermore, the analyses of various attacks indicate that most of the attacks could be prevented, and some, such as collusion attacks, can be mitigated. The proposed approach can be incorporated into existing works, including the European Telecommunications Standards Institute (ETSI) and the International Organization for Standardization (ISO) ITS standards, without disrupting the backward compatibility.
<div id='section'>Paperid: <span id='pid'>90, <a href='https://arxiv.org/pdf/2409.09042.pdf' target='_blank'>https://arxiv.org/pdf/2409.09042.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yucheng Sheng, Le Liang, Hao Ye, Shi Jin, Geoffrey Ye Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.09042">Semantic Communication for Cooperative Perception using HARQ</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cooperative perception, offering a wider field of view than standalone perception, is becoming increasingly crucial in autonomous driving. This perception is enabled through vehicle-to-vehicle (V2V) communication, allowing connected automated vehicles (CAVs) to exchange sensor data, such as light detection and ranging (LiDAR) point clouds, thereby enhancing the collective understanding of the environment. In this paper, we leverage an importance map to distill critical semantic information, introducing a cooperative perception semantic communication framework that employs intermediate fusion. To counter the challenges posed by time-varying multipath fading, our approach incorporates the use of orthogonal frequency-division multiplexing (OFDM) along with channel estimation and equalization strategies. Furthermore, recognizing the necessity for reliable transmission, especially in the low SNR scenarios, we introduce a novel semantic error detection method that is integrated with our semantic communication framework in the spirit of hybrid automatic repeated request (HARQ). Simulation results show that our model surpasses the traditional separate source-channel coding methods in perception performance, both with and without HARQ. Additionally, in terms of throughput, our proposed HARQ schemes demonstrate superior efficiency to the conventional coding approaches.
<div id='section'>Paperid: <span id='pid'>91, <a href='https://arxiv.org/pdf/2311.06498.pdf' target='_blank'>https://arxiv.org/pdf/2311.06498.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yucheng Sheng, Hao Ye, Le Liang, Shi Jin, Geoffrey Ye Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.06498">Semantic Communication for Cooperative Perception based on Importance Map</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cooperative perception, which has a broader perception field than single-vehicle perception, has played an increasingly important role in autonomous driving to conduct 3D object detection. Through vehicle-to-vehicle (V2V) communication technology, various connected automated vehicles (CAVs) can share their sensory information (LiDAR point clouds) for cooperative perception. We employ an importance map to extract significant semantic information and propose a novel cooperative perception semantic communication scheme with intermediate fusion. Meanwhile, our proposed architecture can be extended to the challenging time-varying multipath fading channel. To alleviate the distortion caused by the time-varying multipath fading, we adopt explicit orthogonal frequency-division multiplexing (OFDM) blocks combined with channel estimation and channel equalization. Simulation results demonstrate that our proposed model outperforms the traditional separate source-channel coding over various channel models. Moreover, a robustness study indicates that only part of semantic information is key to cooperative perception. Although our proposed model has only been trained over one specific channel, it has the ability to learn robust coded representations of semantic information that remain resilient to various channel models, demonstrating its generality and robustness.
<div id='section'>Paperid: <span id='pid'>92, <a href='https://arxiv.org/pdf/2502.19313.pdf' target='_blank'>https://arxiv.org/pdf/2502.19313.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhe Wang, Shaocong Xu, Xucai Zhuang, Tongda Xu, Yan Wang, Jingjing Liu, Yilun Chen, Ya-Qin Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.19313">CoopDETR: A Unified Cooperative Perception Framework for 3D Detection via Object Query</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cooperative perception enhances the individual perception capabilities of autonomous vehicles (AVs) by providing a comprehensive view of the environment. However, balancing perception performance and transmission costs remains a significant challenge. Current approaches that transmit region-level features across agents are limited in interpretability and demand substantial bandwidth, making them unsuitable for practical applications. In this work, we propose CoopDETR, a novel cooperative perception framework that introduces object-level feature cooperation via object query. Our framework consists of two key modules: single-agent query generation, which efficiently encodes raw sensor data into object queries, reducing transmission cost while preserving essential information for detection; and cross-agent query fusion, which includes Spatial Query Matching (SQM) and Object Query Aggregation (OQA) to enable effective interaction between queries. Our experiments on the OPV2V and V2XSet datasets demonstrate that CoopDETR achieves state-of-the-art performance and significantly reduces transmission costs to 1/782 of previous methods.
<div id='section'>Paperid: <span id='pid'>93, <a href='https://arxiv.org/pdf/2502.11227.pdf' target='_blank'>https://arxiv.org/pdf/2502.11227.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiazhao Liang, Hao Huang, Yu Hao, Geeta Chandra Raju Bethala, Congcong Wen, John-Ross Rizzo, Yi Fang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.11227">Integrating Retrospective Framework in Multi-Robot Collaboration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in Large Language Models (LLMs) have demonstrated substantial capabilities in enhancing communication and coordination in multi-robot systems. However, existing methods often struggle to achieve efficient collaboration and decision-making in dynamic and uncertain environments, which are common in real-world multi-robot scenarios. To address these challenges, we propose a novel retrospective actor-critic framework for multi-robot collaboration. This framework integrates two key components: (1) an actor that performs real-time decision-making based on observations and task directives, and (2) a critic that retrospectively evaluates the outcomes to provide feedback for continuous refinement, such that the proposed framework can adapt effectively to dynamic conditions. Extensive experiments conducted in simulated environments validate the effectiveness of our approach, demonstrating significant improvements in task performance and adaptability. This work offers a robust solution to persistent challenges in robotic collaboration.
<div id='section'>Paperid: <span id='pid'>94, <a href='https://arxiv.org/pdf/2501.17329.pdf' target='_blank'>https://arxiv.org/pdf/2501.17329.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ashish Bastola, Hao Wang, Abolfazl Razi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.17329">Anomaly Detection in Cooperative Vehicle Perception Systems under Imperfect Communication</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Anomaly detection is a critical requirement for ensuring safety in autonomous driving. In this work, we leverage Cooperative Perception to share information across nearby vehicles, enabling more accurate identification and consensus of anomalous behaviors in complex traffic scenarios. To account for the real-world challenge of imperfect communication, we propose a cooperative-perception-based anomaly detection framework (CPAD), which is a robust architecture that remains effective under communication interruptions, thereby facilitating reliable performance even in low-bandwidth settings. Since no multi-agent anomaly detection dataset exists for vehicle trajectories, we introduce 15,000 different scenarios with a 90,000 trajectories benchmark dataset generated through rule-based vehicle dynamics analysis. Empirical results demonstrate that our approach outperforms standard anomaly classification methods in F1-score, AUC and showcase strong robustness to agent connection interruptions.
<div id='section'>Paperid: <span id='pid'>95, <a href='https://arxiv.org/pdf/2410.02643.pdf' target='_blank'>https://arxiv.org/pdf/2410.02643.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nikolaos Stathoulopoulos, Vidya Sumathy, Christoforos Kanellakis, George Nikolakopoulos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.02643">Why Sample Space Matters: Keyframe Sampling Optimization for LiDAR-based Place Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in robotics are driving real-world autonomy for long-term and large-scale missions, where loop closures via place recognition are vital for mitigating pose estimation drift. However, achieving real-time performance remains challenging for resource-constrained mobile robots and multi-robot systems due to the computational burden of high-density sampling, which increases the complexity of comparing and verifying query samples against a growing map database. Conventional methods often retain redundant information or miss critical data by relying on fixed sampling intervals or operating in 3-D space instead of the descriptor feature space. To address these challenges, we introduce the concept of sample space and propose a novel keyframe sampling approach for LiDAR-based place recognition. Our method minimizes redundancy while preserving essential information in the hyper-dimensional descriptor space, supporting both learning-based and handcrafted descriptors. The proposed approach incorporates a sliding window optimization strategy to ensure efficient keyframe selection and real-time performance, enabling seamless integration into robotic pipelines. In sum, our approach demonstrates robust performance across diverse datasets, with the ability to adapt seamlessly from indoor to outdoor scenarios without parameter tuning, reducing loop closure detection times and memory requirements.
<div id='section'>Paperid: <span id='pid'>96, <a href='https://arxiv.org/pdf/2402.15272.pdf' target='_blank'>https://arxiv.org/pdf/2402.15272.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhe Wang, Siqi Fan, Xiaoliang Huo, Tongda Xu, Yan Wang, Jingjing Liu, Yilun Chen, Ya-Qin Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.15272">EMIFF: Enhanced Multi-scale Image Feature Fusion for Vehicle-Infrastructure Cooperative 3D Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In autonomous driving, cooperative perception makes use of multi-view cameras from both vehicles and infrastructure, providing a global vantage point with rich semantic context of road conditions beyond a single vehicle viewpoint. Currently, two major challenges persist in vehicle-infrastructure cooperative 3D (VIC3D) object detection: $1)$ inherent pose errors when fusing multi-view images, caused by time asynchrony across cameras; $2)$ information loss in transmission process resulted from limited communication bandwidth. To address these issues, we propose a novel camera-based 3D detection framework for VIC3D task, Enhanced Multi-scale Image Feature Fusion (EMIFF). To fully exploit holistic perspectives from both vehicles and infrastructure, we propose Multi-scale Cross Attention (MCA) and Camera-aware Channel Masking (CCM) modules to enhance infrastructure and vehicle features at scale, spatial, and channel levels to correct the pose error introduced by camera asynchrony. We also introduce a Feature Compression (FC) module with channel and spatial compression blocks for transmission efficiency. Experiments show that EMIFF achieves SOTA on DAIR-V2X-C datasets, significantly outperforming previous early-fusion and late-fusion methods with comparable transmission costs.
<div id='section'>Paperid: <span id='pid'>97, <a href='https://arxiv.org/pdf/2411.14009.pdf' target='_blank'>https://arxiv.org/pdf/2411.14009.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rebekah Rousi, Niko Makitalo, Hooman Samani, Kai-Kristian Kemell, Jose Siqueira de Cerqueira, Ville Vakkuri, Tommi Mikkonen, Pekka Abrahamsson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.14009">GPT versus Humans: Uncovering Ethical Concerns in Conversational Generative AI-empowered Multi-Robot Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The emergence of generative artificial intelligence (GAI) and large language models (LLMs) such ChatGPT has enabled the realization of long-harbored desires in software and robotic development. The technology however, has brought with it novel ethical challenges. These challenges are compounded by the application of LLMs in other machine learning systems, such as multi-robot systems. The objectives of the study were to examine novel ethical issues arising from the application of LLMs in multi-robot systems. Unfolding ethical issues in GPT agent behavior (deliberation of ethical concerns) was observed, and GPT output was compared with human experts. The article also advances a model for ethical development of multi-robot systems. A qualitative workshop-based method was employed in three workshops for the collection of ethical concerns: two human expert workshops (N=16 participants) and one GPT-agent-based workshop (N=7 agents; two teams of 6 agents plus one judge). Thematic analysis was used to analyze the qualitative data. The results reveal differences between the human-produced and GPT-based ethical concerns. Human experts placed greater emphasis on new themes related to deviance, data privacy, bias and unethical corporate conduct. GPT agents emphasized concerns present in existing AI ethics guidelines. The study contributes to a growing body of knowledge in context-specific AI ethics and GPT application. It demonstrates the gap between human expert thinking and LLM output, while emphasizing new ethical concerns emerging in novel technology.
<div id='section'>Paperid: <span id='pid'>98, <a href='https://arxiv.org/pdf/2401.09473.pdf' target='_blank'>https://arxiv.org/pdf/2401.09473.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rebekah Rousi, Hooman Samani, Niko MÃ¤kitalo, Ville Vakkuri, Simo Linkola, Kai-Kristian Kemell, Paulius Daubaris, Ilenia Fronza, Tommi Mikkonen, Pekka Abrahamsson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.09473">Business and ethical concerns in domestic Conversational Generative AI-empowered multi-robot systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Business and technology are intricately connected through logic and design. They are equally sensitive to societal changes and may be devastated by scandal. Cooperative multi-robot systems (MRSs) are on the rise, allowing robots of different types and brands to work together in diverse contexts. Generative artificial intelligence has been a dominant topic in recent artificial intelligence (AI) discussions due to its capacity to mimic humans through the use of natural language and the production of media, including deep fakes. In this article, we focus specifically on the conversational aspects of generative AI, and hence use the term Conversational Generative artificial intelligence (CGI). Like MRSs, CGIs have enormous potential for revolutionizing processes across sectors and transforming the way humans conduct business. From a business perspective, cooperative MRSs alone, with potential conflicts of interest, privacy practices, and safety concerns, require ethical examination. MRSs empowered by CGIs demand multi-dimensional and sophisticated methods to uncover imminent ethical pitfalls. This study focuses on ethics in CGI-empowered MRSs while reporting the stages of developing the MORUL model.
<div id='section'>Paperid: <span id='pid'>99, <a href='https://arxiv.org/pdf/2306.07229.pdf' target='_blank'>https://arxiv.org/pdf/2306.07229.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Daniel Hert, Tomas Baca, Pavel Petracek, Vit Kratky, Robert Penicka, Vojtech Spurny, Matej Petrlik, Matous Vrba, David Zaitlik, Pavel Stoudek, Viktor Walter, Petr Stepan, Jiri Horyna, Vaclav Pritzl, Martin Sramek, Afzal Ahmad, Giuseppe Silano, Daniel Bonilla Licea, Petr Stibinger, Tiago Nascimento, Martin Saska
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.07229">MRS Drone: A Modular Platform for Real-World Deployment of Aerial Multi-Robot Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a modular autonomous Unmanned Aerial Vehicle (UAV) platform called the Multi-robot Systems (MRS) Drone that can be used in a large range of indoor and outdoor applications. The MRS Drone features unique modularity with respect to changes in actuators, frames, and sensory configuration. As the name suggests, the platform is specially tailored for deployment within a MRS group. The MRS Drone contributes to the state-of-the-art of UAV platforms by allowing smooth real-world deployment of multiple aerial robots, as well as by outperforming other platforms with its modularity. For real-world multi-robot deployment in various applications, the platform is easy to both assemble and modify. Moreover, it is accompanied by a realistic simulator to enable safe pre-flight testing and a smooth transition to complex real-world experiments. In this manuscript, we present mechanical and electrical designs, software architecture, and technical specifications to build a fully autonomous multi UAV system. Finally, we demonstrate the full capabilities and the unique modularity of the MRS Drone in various real-world applications that required a diverse range of platform configurations.
<div id='section'>Paperid: <span id='pid'>100, <a href='https://arxiv.org/pdf/2302.04735.pdf' target='_blank'>https://arxiv.org/pdf/2302.04735.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Giuseppe Silano, Jan Bednar, Tiago Nascimento, Jesus Capitan, Martin Saska, Anibal Ollero
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.04735">A Multi-Layer Software Architecture for Aerial Cognitive Multi-Robot Systems in Power Line Inspection Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a multi-layer software architecture to perform cooperative missions with a fleet of quadrotors providing support in electrical power line inspection operations. The proposed software framework guarantees the compliance with safety requirements between drones and human workers while ensuring that the mission is carried out successfully. Besides, cognitive capabilities are integrated in the multi-vehicle system in order to reply to unforeseen events and external disturbances. The feasibility and effectiveness of the proposed architecture are demonstrated by means of realistic simulations.
<div id='section'>Paperid: <span id='pid'>101, <a href='https://arxiv.org/pdf/2509.15597.pdf' target='_blank'>https://arxiv.org/pdf/2509.15597.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi Dong, Zhongguo Li, Sarvapali D. Ramchurn, Xiaowei Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15597">Distributed Nash Equilibrium Seeking Algorithm in Aggregative Games for Heterogeneous Multi-Robot Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper develops a distributed Nash Equilibrium seeking algorithm for heterogeneous multi-robot systems. The algorithm utilises distributed optimisation and output control to achieve the Nash equilibrium by leveraging information shared among neighbouring robots. Specifically, we propose a distributed optimisation algorithm that calculates the Nash equilibrium as a tailored reference for each robot and designs output control laws for heterogeneous multi-robot systems to track it in an aggregative game. We prove that our algorithm is guaranteed to converge and result in efficient outcomes. The effectiveness of our approach is demonstrated through numerical simulations and empirical testing with physical robots.
<div id='section'>Paperid: <span id='pid'>102, <a href='https://arxiv.org/pdf/2509.15597.pdf' target='_blank'>https://arxiv.org/pdf/2509.15597.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi Dong, Zhongguo Li, Sarvapali D. Ramchurn, Xiaowei Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15597">Distributed Nash Equilibrium Seeking Algorithm in Aggregative Games for Heterogeneous Multi-Robot Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper develops a distributed Nash Equilibrium seeking algorithm for heterogeneous multi-robot systems. The algorithm utilises distributed optimisation and output control to achieve the Nash equilibrium by leveraging information shared among neighbouring robots. Specifically, we propose a distributed optimisation algorithm that calculates the Nash equilibrium as a tailored reference for each robot and designs output control laws for heterogeneous multi-robot systems to track it in an aggregative game. We prove that our algorithm is guaranteed to converge and result in efficient outcomes. The effectiveness of our approach is demonstrated through numerical simulations and empirical testing with physical robots.
<div id='section'>Paperid: <span id='pid'>103, <a href='https://arxiv.org/pdf/2302.02266.pdf' target='_blank'>https://arxiv.org/pdf/2302.02266.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anirudh Chari, Rui Chen, Changliu Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.02266">Space-Time Conflict Spheres for Constrained Multi-Agent Motion Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent motion planning (MAMP) is a critical challenge in applications such as connected autonomous vehicles and multi-robot systems. In this paper, we propose a space-time conflict resolution approach for MAMP. We formulate the problem using a novel, flexible sphere-based discretization for trajectories. Our approach leverages a depth-first conflict search strategy to provide the scalability of decoupled approaches while maintaining the computational guarantees of coupled approaches. We compose procedures for evading discretization error and adhering to kinematic constraints in generated solutions. Theoretically, we prove the continuous-time feasibility and formulation-space completeness of our algorithm. Experimentally, we demonstrate that our algorithm matches the performance of the current state of the art with respect to both runtime and solution quality, while expanding upon the abilities of current work through accommodation for both static and dynamic obstacles. We evaluate our algorithm in various unsignalized traffic intersection scenarios using CARLA, an open-source vehicle simulator. Results show significant success rate improvement in spatially constrained settings, involving both connected and non-connected vehicles. Furthermore, we maintain a reasonable suboptimality ratio that scales well among increasingly complex scenarios.
<div id='section'>Paperid: <span id='pid'>104, <a href='https://arxiv.org/pdf/2302.01728.pdf' target='_blank'>https://arxiv.org/pdf/2302.01728.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi Dong, Zhongguo Li, Xingyu Zhao, Zhengtao Ding, Xiaowei Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.01728">Decentralised and Cooperative Control of Multi-Robot Systems through Distributed Optimisation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-robot cooperative control has gained extensive research interest due to its wide applications in civil, security, and military domains. This paper proposes a cooperative control algorithm for multi-robot systems with general linear dynamics. The algorithm is based on distributed cooperative optimisation and output regulation, and it achieves global optimum by utilising only information shared among neighbouring robots. Technically, a high-level distributed optimisation algorithm for multi-robot systems is presented, which will serve as an optimal reference generator for each individual agent. Then, based on the distributed optimisation algorithm, an output regulation method is utilised to solve the optimal coordination problem for general linear dynamic systems. The convergence of the proposed algorithm is theoretically proved. Both numerical simulations and real-time physical robot experiments are conducted to validate the effectiveness of the proposed cooperative control algorithms.
<div id='section'>Paperid: <span id='pid'>105, <a href='https://arxiv.org/pdf/2507.13702.pdf' target='_blank'>https://arxiv.org/pdf/2507.13702.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junho Choi, Kihwan Ryoo, Jeewon Kim, Taeyun Kim, Eungchang Lee, Myeongwoo Jeong, Kevin Christiansen Marsim, Hyungtae Lim, Hyun Myung
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.13702">SaWa-ML: Structure-Aware Pose Correction and Weight Adaptation-Based Robust Multi-Robot Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-robot localization is a crucial task for implementing multi-robot systems. Numerous researchers have proposed optimization-based multi-robot localization methods that use camera, IMU, and UWB sensors. Nevertheless, characteristics of individual robot odometry estimates and distance measurements between robots used in the optimization are not sufficiently considered. In addition, previous researches were heavily influenced by the odometry accuracy that is estimated from individual robots. Consequently, long-term drift error caused by error accumulation is potentially inevitable. In this paper, we propose a novel visual-inertial-range-based multi-robot localization method, named SaWa-ML, which enables geometric structure-aware pose correction and weight adaptation-based robust multi-robot localization. Our contributions are twofold: (i) we leverage UWB sensor data, whose range error does not accumulate over time, to first estimate the relative positions between robots and then correct the positions of each robot, thus reducing long-term drift errors, (ii) we design adaptive weights for robot pose correction by considering the characteristics of the sensor data and visual-inertial odometry estimates. The proposed method has been validated in real-world experiments, showing a substantial performance increase compared with state-of-the-art algorithms.
<div id='section'>Paperid: <span id='pid'>106, <a href='https://arxiv.org/pdf/2412.08428.pdf' target='_blank'>https://arxiv.org/pdf/2412.08428.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Martin Schuck, Dinushka Orrin Dahanaggamaarachchi, Ben Sprenger, Vedant Vyas, Siqi Zhou, Angela P. Schoellig
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.08428">SwarmGPT: Combining Large Language Models with Safe Motion Planning for Drone Swarm Choreography</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Drone swarm performances -- synchronized, expressive aerial displays set to music -- have emerged as a captivating application of modern robotics. Yet designing smooth, safe choreographies remains a complex task requiring expert knowledge. We present SwarmGPT, a language-based choreographer that leverages the reasoning power of large language models (LLMs) to streamline drone performance design. The LLM is augmented by a safety filter that ensures deployability by making minimal corrections when safety or feasibility constraints are violated. By decoupling high-level choreographic design from low-level motion planning, our system enables non-experts to iteratively refine choreographies using natural language without worrying about collisions or actuator limits. We validate our approach through simulations with swarms up to 200 drones and real-world experiments with up to 20 drones performing choreographies to diverse types of songs, demonstrating scalable, synchronized, and safe performances. Beyond entertainment, this work offers a blueprint for integrating foundation models into safety-critical swarm robotics applications.
<div id='section'>Paperid: <span id='pid'>107, <a href='https://arxiv.org/pdf/2409.12274.pdf' target='_blank'>https://arxiv.org/pdf/2409.12274.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuwei Wu, Yuezhan Tao, Peihan Li, Guangyao Shi, Gaurav S. Sukhatme, Vijay Kumar, Lifeng Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.12274">Hierarchical LLMs In-the-loop Optimization for Real-time Multi-Robot Target Tracking under Unknown Hazards</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real-time multi-robot coordination in hazardous and adversarial environments requires fast, reliable adaptation to dynamic threats. While Large Language Models (LLMs) offer strong high-level reasoning capabilities, the lack of safety guarantees limits their direct use in critical decision-making. In this paper, we propose a hierarchical optimization framework that integrates LLMs into the decision loop for multi-robot target tracking in dynamic and hazardous environments. Rather than generating control actions directly, LLMs are used to generate task configuration and adjust parameters in a bi-level task allocation and planning problem. We formulate multi-robot coordination for tracking tasks as a bi-level optimization problem, with LLMs to reason about potential hazards in the environment and the status of the robot team and modify both the inner and outer levels of the optimization. This hierarchical approach enables real-time adjustments to the robots' behavior. Additionally, a human supervisor can offer broad guidance and assessments to address unexpected dangers, model mismatches, and performance issues arising from local minima. We validate our proposed framework in both simulation and real-world experiments with comprehensive evaluations, demonstrating its effectiveness and showcasing its capability for safe LLM integration for multi-robot systems.
<div id='section'>Paperid: <span id='pid'>108, <a href='https://arxiv.org/pdf/2409.12274.pdf' target='_blank'>https://arxiv.org/pdf/2409.12274.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuwei Wu, Yuezhan Tao, Peihan Li, Guangyao Shi, Gaurav S. Sukhatme, Vijay Kumar, Lifeng Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.12274">Hierarchical LLMs In-the-loop Optimization for Real-time Multi-Robot Target Tracking under Unknown Hazards</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real-time multi-robot coordination in hazardous and adversarial environments requires fast, reliable adaptation to dynamic threats. While Large Language Models (LLMs) offer strong high-level reasoning capabilities, the lack of safety guarantees limits their direct use in critical decision-making. In this paper, we propose a hierarchical optimization framework that integrates LLMs into the decision loop for multi-robot target tracking in dynamic and hazardous environments. Rather than generating control actions directly, LLMs are used to generate task configuration and adjust parameters in a bi-level task allocation and planning problem. We formulate multi-robot coordination for tracking tasks as a bi-level optimization problem, with LLMs to reason about potential hazards in the environment and the status of the robot team and modify both the inner and outer levels of the optimization. This hierarchical approach enables real-time adjustments to the robots' behavior. Additionally, a human supervisor can offer broad guidance and assessments to address unexpected dangers, model mismatches, and performance issues arising from local minima. We validate our proposed framework in both simulation and real-world experiments with comprehensive evaluations, demonstrating its effectiveness and showcasing its capability for safe LLM integration for multi-robot systems.
<div id='section'>Paperid: <span id='pid'>109, <a href='https://arxiv.org/pdf/2404.17147.pdf' target='_blank'>https://arxiv.org/pdf/2404.17147.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenrong Zhang, Jianan Liu, Xi Zhou, Tao Huang, Qing-Long Han, Jingxin Liu, Hongbin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.17147">On the Federated Learning Framework for Cooperative Perception</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cooperative perception is essential to enhance the efficiency and safety of future transportation systems, requiring extensive data sharing among vehicles on the road, which raises significant privacy concerns. Federated learning offers a promising solution by enabling data privacy-preserving collaborative enhancements in perception, decision-making, and planning among connected and autonomous vehicles (CAVs). However, federated learning is impeded by significant challenges arising from data heterogeneity across diverse clients, potentially diminishing model accuracy and prolonging convergence periods. This study introduces a specialized federated learning framework for CP, termed the federated dynamic weighted aggregation (FedDWA) algorithm, facilitated by dynamic adjusting loss (DALoss) function. This framework employs dynamic client weighting to direct model convergence and integrates a novel loss function that utilizes Kullback-Leibler divergence (KLD) to counteract the detrimental effects of non-independently and identically distributed (Non-IID) and unbalanced data. Utilizing the BEV transformer as the primary model, our rigorous testing on the OpenV2V dataset, augmented with FedBEVT data, demonstrates significant improvements in the average intersection over union (IoU). These results highlight the substantial potential of our federated learning framework to address data heterogeneity challenges in CP, thereby enhancing the accuracy of environmental perception models and facilitating more robust and efficient collaborative learning solutions in the transportation sector.
<div id='section'>Paperid: <span id='pid'>110, <a href='https://arxiv.org/pdf/2310.03525.pdf' target='_blank'>https://arxiv.org/pdf/2310.03525.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tao Huang, Jianan Liu, Xi Zhou, Dinh C. Nguyen, Mostafa Rahimi Azghadi, Yuxuan Xia, Qing-Long Han, Sumei Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.03525">Vehicle-to-Everything Cooperative Perception for Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Achieving fully autonomous driving with enhanced safety and efficiency relies on vehicle-to-everything cooperative perception, which enables vehicles to share perception data, thereby enhancing situational awareness and overcoming the limitations of the sensing ability of individual vehicles. Vehicle-to-everything cooperative perception plays a crucial role in extending the perception range, increasing detection accuracy, and supporting more robust decision-making and control in complex environments. This paper provides a comprehensive survey of recent developments in vehicle-to-everything cooperative perception, introducing mathematical models that characterize the perception process under different collaboration strategies. Key techniques for enabling reliable perception sharing, such as agent selection, data alignment, and feature fusion, are examined in detail. In addition, major challenges are discussed, including differences in agents and models, uncertainty in perception outputs, and the impact of communication constraints such as transmission delay and data loss. The paper concludes by outlining promising research directions, including privacy-preserving artificial intelligence methods, collaborative intelligence, and integrated sensing frameworks to support future advancements in vehicle-to-everything cooperative perception.
<div id='section'>Paperid: <span id='pid'>111, <a href='https://arxiv.org/pdf/2310.03525.pdf' target='_blank'>https://arxiv.org/pdf/2310.03525.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tao Huang, Jianan Liu, Xi Zhou, Dinh C. Nguyen, Mostafa Rahimi Azghadi, Yuxuan Xia, Qing-Long Han, Sumei Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.03525">Vehicle-to-Everything Cooperative Perception for Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Achieving fully autonomous driving with enhanced safety and efficiency relies on vehicle-to-everything cooperative perception, which enables vehicles to share perception data, thereby enhancing situational awareness and overcoming the limitations of the sensing ability of individual vehicles. Vehicle-to-everything cooperative perception plays a crucial role in extending the perception range, increasing detection accuracy, and supporting more robust decision-making and control in complex environments. This paper provides a comprehensive survey of recent developments in vehicle-to-everything cooperative perception, introducing mathematical models that characterize the perception process under different collaboration strategies. Key techniques for enabling reliable perception sharing, such as agent selection, data alignment, and feature fusion, are examined in detail. In addition, major challenges are discussed, including differences in agents and models, uncertainty in perception outputs, and the impact of communication constraints such as transmission delay and data loss. The paper concludes by outlining promising research directions, including privacy-preserving artificial intelligence methods, collaborative intelligence, and integrated sensing frameworks to support future advancements in vehicle-to-everything cooperative perception.
<div id='section'>Paperid: <span id='pid'>112, <a href='https://arxiv.org/pdf/2409.16009.pdf' target='_blank'>https://arxiv.org/pdf/2409.16009.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ike Obi, Ruiqi Wang, Wonse Jo, Byung-Cheol Min
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.16009">Modeling and Evaluating Trust Dynamics in Multi-Human Multi-Robot Task Allocation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Trust is essential in human-robot collaboration, particularly in multi-human, multi-robot (MH-MR) teams, where it plays a crucial role in maintaining team cohesion in complex operational environments. Despite its importance, trust is rarely incorporated into task allocation and reallocation algorithms for MH-MR collaboration. While prior research in single-human, single-robot interactions has shown that integrating trust significantly enhances both performance outcomes and user experience, its role in MH-MR task allocation remains underexplored. In this paper, we introduce the Expectation Confirmation Trust (ECT) Model, a novel framework for modeling trust dynamics in MH-MR teams. We evaluate the ECT model against five existing trust models and a no-trust baseline to assess its impact on task allocation outcomes across different team configurations (2H-2R, 5H-5R, and 10H-10R). Our results show that the ECT model improves task success rate, reduces mean completion time, and lowers task error rates. These findings highlight the complexities of trust-based task allocation in MH-MR teams. We discuss the implications of incorporating trust into task allocation algorithms and propose future research directions for adaptive trust mechanisms that balance efficiency and performance in dynamic, multi-agent environments.
<div id='section'>Paperid: <span id='pid'>113, <a href='https://arxiv.org/pdf/2309.15234.pdf' target='_blank'>https://arxiv.org/pdf/2309.15234.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weizheng Wang, Le Mao, Ruiqi Wang, Byung-Cheol Min
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.15234">Multi-Robot Cooperative Socially-Aware Navigation Using Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In public spaces shared with humans, ensuring multi-robot systems navigate without collisions while respecting social norms is challenging, particularly with limited communication. Although current robot social navigation techniques leverage advances in reinforcement learning and deep learning, they frequently overlook robot dynamics in simulations, leading to a simulation-to-reality gap. In this paper, we bridge this gap by presenting a new multi-robot social navigation environment crafted using Dec-POSMDP and multi-agent reinforcement learning. Furthermore, we introduce SAMARL: a novel benchmark for cooperative multi-robot social navigation. SAMARL employs a unique spatial-temporal transformer combined with multi-agent reinforcement learning. This approach effectively captures the complex interactions between robots and humans, thus promoting cooperative tendencies in multi-robot systems. Our extensive experiments reveal that SAMARL outperforms existing baseline and ablation models in our designed environment. Demo videos for this work can be found at: https://sites.google.com/view/samarl
<div id='section'>Paperid: <span id='pid'>114, <a href='https://arxiv.org/pdf/2308.01533.pdf' target='_blank'>https://arxiv.org/pdf/2308.01533.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Biru Zhang, Jiankun Wang, Max Q. -H. Meng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.01533">Multi-robot Path Planning with Rapidly-exploring Random Disjointed-Trees</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-robot path planning is a computational process involving finding paths for each robot from its start to the goal while ensuring collision-free operation. It is widely used in robots and autonomous driving. However, the computational time of multi-robot path planning algorithms is enormous, resulting in low efficiency in practical applications. To address this problem, this article proposes a novel multi-robot path planning algorithm (Multi-Agent Rapidly-exploring Random Disjointed-Trees*, MA-RRdT*) based on multi-tree random sampling. The proposed algorithm is based on a single-robot path planning algorithm (Rapidly-exploring Random disjointed-Trees*, RRdT*). The novel MA-RRdT* algorithm has the advantages of fast speed, high space exploration efficiency, and suitability for complex maps. Comparative experiments are completed to evaluate the effectiveness of MA-RRdT*. The final experimental results validate the superior performance of the MA-RRdT* algorithm in terms of time cost and space exploration efficiency.
<div id='section'>Paperid: <span id='pid'>115, <a href='https://arxiv.org/pdf/2303.10465.pdf' target='_blank'>https://arxiv.org/pdf/2303.10465.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wonse Jo, Ruiqi Wang, Baijian Yang, Dan Foti, Mo Rastgaar, Byung-Cheol Min
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.10465">Cognitive Load-based Affective Workload Allocation for Multi-human Multi-robot Teams</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The interaction and collaboration between humans and multiple robots represent a novel field of research known as human multi-robot systems. Adequately designed systems within this field allow teams composed of both humans and robots to work together effectively on tasks such as monitoring, exploration, and search and rescue operations. This paper presents a deep reinforcement learning-based affective workload allocation controller specifically for multi-human multi-robot teams. The proposed controller can dynamically reallocate workloads based on the performance of the operators during collaborative missions with multi-robot systems. The operators' performances are evaluated through the scores of a self-reported questionnaire (i.e., subjective measurement) and the results of a deep learning-based cognitive workload prediction algorithm that uses physiological and behavioral data (i.e., objective measurement). To evaluate the effectiveness of the proposed controller, we use a multi-human multi-robot CCTV monitoring task as an example and carry out comprehensive real-world experiments with 32 human subjects for both quantitative measurement and qualitative analysis. Our results demonstrate the performance and effectiveness of the proposed controller and highlight the importance of incorporating both subjective and objective measurements of the operators' cognitive workload as well as seeking consent for workload transitions, to enhance the performance of multi-human multi-robot teams.
<div id='section'>Paperid: <span id='pid'>116, <a href='https://arxiv.org/pdf/2503.12982.pdf' target='_blank'>https://arxiv.org/pdf/2503.12982.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunshuang Yuan, Yan Xia, Daniel Cremers, Monika Sester
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.12982">SparseAlign: A Fully Sparse Framework for Cooperative Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cooperative perception can increase the view field and decrease the occlusion of an ego vehicle, hence improving the perception performance and safety of autonomous driving. Despite the success of previous works on cooperative object detection, they mostly operate on dense Bird's Eye View (BEV) feature maps, which are computationally demanding and can hardly be extended to long-range detection problems. More efficient fully sparse frameworks are rarely explored. In this work, we design a fully sparse framework, SparseAlign, with three key features: an enhanced sparse 3D backbone, a query-based temporal context learning module, and a robust detection head specially tailored for sparse features. Extensive experimental results on both OPV2V and DairV2X datasets show that our framework, despite its sparsity, outperforms the state of the art with less communication bandwidth requirements. In addition, experiments on the OPV2Vt and DairV2Xt datasets for time-aligned cooperative object detection also show a significant performance gain compared to the baseline works.
<div id='section'>Paperid: <span id='pid'>117, <a href='https://arxiv.org/pdf/2401.10156.pdf' target='_blank'>https://arxiv.org/pdf/2401.10156.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaige Qu, Weihua Zhuang, Qiang Ye, Wen Wu, Xuemin Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.10156">Model-Assisted Learning for Adaptive Cooperative Perception of Connected Autonomous Vehicles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cooperative perception (CP) is a key technology to facilitate consistent and accurate situational awareness for connected and autonomous vehicles (CAVs). To tackle the network resource inefficiency issue in traditional broadcast-based CP, unicast-based CP has been proposed to associate CAV pairs for cooperative perception via vehicle-to-vehicle transmission. In this paper, we investigate unicast-based CP among CAV pairs. With the consideration of dynamic perception workloads and channel conditions due to vehicle mobility and dynamic radio resource availability, we propose an adaptive cooperative perception scheme for CAV pairs in a mixed-traffic autonomous driving scenario with both CAVs and human-driven vehicles. We aim to determine when to switch between cooperative perception and stand-alone perception for each CAV pair, and allocate communication and computing resources to cooperative CAV pairs for maximizing the computing efficiency gain under perception task delay requirements. A model-assisted multi-agent reinforcement learning (MARL) solution is developed, which integrates MARL for an adaptive CAV cooperation decision and an optimization model for communication and computing resource allocation. Simulation results demonstrate the effectiveness of the proposed scheme in achieving high computing efficiency gain, as compared with benchmark schemes.
<div id='section'>Paperid: <span id='pid'>118, <a href='https://arxiv.org/pdf/2312.06395.pdf' target='_blank'>https://arxiv.org/pdf/2312.06395.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Giovanna Amorim, MarÃ­a Santos, Shinkyu Park, Alessio Franci, Naomi Ehrich Leonard
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.06395">Threshold Decision-Making Dynamics Adaptive to Physical Constraints and Changing Environment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a threshold decision-making framework for controlling the physical dynamics of an agent switching between two spatial tasks. Our framework couples a nonlinear opinion dynamics model that represents the evolution of an agent's preference for a particular task with the physical dynamics of the agent. We prove the bifurcation that governs the behavior of the coupled dynamics. We show by means of the bifurcation behavior how the coupled dynamics are adaptive to the physical constraints of the agent. We also show how the bifurcation can be modulated to allow the agent to switch tasks based on thresholds adaptive to environmental conditions. We illustrate the benefits of the approach through a decentralized multi-robot task allocation application for trash collection.
<div id='section'>Paperid: <span id='pid'>119, <a href='https://arxiv.org/pdf/2509.21523.pdf' target='_blank'>https://arxiv.org/pdf/2509.21523.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaofan Yu, Yuwei Wu, Katherine Mao, Ye Tian, Vijay Kumar, Tajana Rosing
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21523">DroneFL: Federated Learning for Multi-UAV Visual Target Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-robot target tracking is a fundamental problem that requires coordinated monitoring of dynamic entities in applications such as precision agriculture, environmental monitoring, disaster response, and security surveillance. While Federated Learning (FL) has the potential to enhance learning across multiple robots without centralized data aggregation, its use in multi-Unmanned Aerial Vehicle (UAV) target tracking remains largely underexplored. Key challenges include limited onboard computational resources, significant data heterogeneity in FL due to varying targets and the fields of view, and the need for tight coupling between trajectory prediction and multi-robot planning. In this paper, we introduce DroneFL, the first federated learning framework specifically designed for efficient multi-UAV target tracking. We design a lightweight local model to predict target trajectories from sensor inputs, using a frozen YOLO backbone and a shallow transformer for efficient onboard training. The updated models are periodically aggregated in the cloud for global knowledge sharing. To alleviate the data heterogeneity that hinders FL convergence, DroneFL introduces a position-invariant model architecture with altitude-based adaptive instance normalization. Finally, we fuse predictions from multiple UAVs in the cloud and generate optimal trajectories that balance target prediction accuracy and overall tracking performance. Our results show that DroneFL reduces prediction error by 6%-83% and tracking distance by 0.4%-4.6% compared to a distributed non-FL framework. In terms of efficiency, DroneFL runs in real time on a Raspberry Pi 5 and has on average just 1.56 KBps data rate to the cloud.
<div id='section'>Paperid: <span id='pid'>120, <a href='https://arxiv.org/pdf/2509.21523.pdf' target='_blank'>https://arxiv.org/pdf/2509.21523.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaofan Yu, Yuwei Wu, Katherine Mao, Ye Tian, Vijay Kumar, Tajana Rosing
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21523">DroneFL: Federated Learning for Multi-UAV Visual Target Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-robot target tracking is a fundamental problem that requires coordinated monitoring of dynamic entities in applications such as precision agriculture, environmental monitoring, disaster response, and security surveillance. While Federated Learning (FL) has the potential to enhance learning across multiple robots without centralized data aggregation, its use in multi-Unmanned Aerial Vehicle (UAV) target tracking remains largely underexplored. Key challenges include limited onboard computational resources, significant data heterogeneity in FL due to varying targets and the fields of view, and the need for tight coupling between trajectory prediction and multi-robot planning. In this paper, we introduce DroneFL, the first federated learning framework specifically designed for efficient multi-UAV target tracking. We design a lightweight local model to predict target trajectories from sensor inputs, using a frozen YOLO backbone and a shallow transformer for efficient onboard training. The updated models are periodically aggregated in the cloud for global knowledge sharing. To alleviate the data heterogeneity that hinders FL convergence, DroneFL introduces a position-invariant model architecture with altitude-based adaptive instance normalization. Finally, we fuse predictions from multiple UAVs in the cloud and generate optimal trajectories that balance target prediction accuracy and overall tracking performance. Our results show that DroneFL reduces prediction error by 6%-83% and tracking distance by 0.4%-4.6% compared to a distributed non-FL framework. In terms of efficiency, DroneFL runs in real time on a Raspberry Pi 5 and has on average just 1.56 KBps data rate to the cloud.
<div id='section'>Paperid: <span id='pid'>121, <a href='https://arxiv.org/pdf/2509.17195.pdf' target='_blank'>https://arxiv.org/pdf/2509.17195.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Damian Owerko, Frederic Vatnsdal, Saurav Agarwal, Vijay Kumar, Alejandro Ribeiro
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17195">MAST: Multi-Agent Spatial Transformer for Learning to Collaborate</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This article presents a novel multi-agent spatial transformer (MAST) for learning communication policies in large-scale decentralized and collaborative multi-robot systems (DC-MRS). Challenges in collaboration in DC-MRS arise from: (i) partial observable states as robots make only localized perception, (ii) limited communication range with no central server, and (iii) independent execution of actions. The robots need to optimize a common task-specific objective, which, under the restricted setting, must be done using a communication policy that exhibits the desired collaborative behavior. The proposed MAST is a decentralized transformer architecture that learns communication policies to compute abstract information to be shared with other agents and processes the received information with the robot's own observations. The MAST extends the standard transformer with new positional encoding strategies and attention operations that employ windowing to limit the receptive field for MRS. These are designed for local computation, shift-equivariance, and permutation equivariance, making it a promising approach for DC-MRS. We demonstrate the efficacy of MAST on decentralized assignment and navigation (DAN) and decentralized coverage control. Efficiently trained using imitation learning in a centralized setting, the decentralized MAST policy is robust to communication delays, scales to large teams, and performs better than the baselines and other learning-based approaches.
<div id='section'>Paperid: <span id='pid'>122, <a href='https://arxiv.org/pdf/2509.17195.pdf' target='_blank'>https://arxiv.org/pdf/2509.17195.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Damian Owerko, Frederic Vatnsdal, Saurav Agarwal, Vijay Kumar, Alejandro Ribeiro
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17195">MAST: Multi-Agent Spatial Transformer for Learning to Collaborate</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This article presents a novel multi-agent spatial transformer (MAST) for learning communication policies in large-scale decentralized and collaborative multi-robot systems (DC-MRS). Challenges in collaboration in DC-MRS arise from: (i) partial observable states as robots make only localized perception, (ii) limited communication range with no central server, and (iii) independent execution of actions. The robots need to optimize a common task-specific objective, which, under the restricted setting, must be done using a communication policy that exhibits the desired collaborative behavior. The proposed MAST is a decentralized transformer architecture that learns communication policies to compute abstract information to be shared with other agents and processes the received information with the robot's own observations. The MAST extends the standard transformer with new positional encoding strategies and attention operations that employ windowing to limit the receptive field for MRS. These are designed for local computation, shift-equivariance, and permutation equivariance, making it a promising approach for DC-MRS. We demonstrate the efficacy of MAST on decentralized assignment and navigation (DAN) and decentralized coverage control. Efficiently trained using imitation learning in a centralized setting, the decentralized MAST policy is robust to communication delays, scales to large teams, and performs better than the baselines and other learning-based approaches.
<div id='section'>Paperid: <span id='pid'>123, <a href='https://arxiv.org/pdf/2508.09581.pdf' target='_blank'>https://arxiv.org/pdf/2508.09581.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junkai Jiang, Yihe Chen, Yibin Yang, Ruochen Li, Shaobing Xu, Jianqiang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09581">ESCoT: An Enhanced Step-based Coordinate Trajectory Planning Method for Multiple Car-like Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-vehicle trajectory planning (MVTP) is one of the key challenges in multi-robot systems (MRSs) and has broad applications across various fields. This paper presents ESCoT, an enhanced step-based coordinate trajectory planning method for multiple car-like robots. ESCoT incorporates two key strategies: collaborative planning for local robot groups and replanning for duplicate configurations. These strategies effectively enhance the performance of step-based MVTP methods. Through extensive experiments, we show that ESCoT 1) in sparse scenarios, significantly improves solution quality compared to baseline step-based method, achieving up to 70% improvement in typical conflict scenarios and 34% in randomly generated scenarios, while maintaining high solving efficiency; and 2) in dense scenarios, outperforms all baseline methods, maintains a success rate of over 50% even in the most challenging configurations. The results demonstrate that ESCoT effectively solves MVTP, further extending the capabilities of step-based methods. Finally, practical robot tests validate the algorithm's applicability in real-world scenarios.
<div id='section'>Paperid: <span id='pid'>124, <a href='https://arxiv.org/pdf/2507.16068.pdf' target='_blank'>https://arxiv.org/pdf/2507.16068.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhehui Huang, Guangyao Shi, Yuwei Wu, Vijay Kumar, Gaurav S. Sukhatme
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.16068">Compositional Coordination for Multi-Robot Teams with Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-robot coordination has traditionally relied on a mission-specific and expert-driven pipeline, where natural language mission descriptions are manually translated by domain experts into mathematical formulation, algorithm design, and executable code. This conventional process is labor-intensive, inaccessible to non-experts, and inflexible to changes in mission requirements. Here, we propose LAN2CB (Language to Collective Behavior), a novel framework that leverages large language models (LLMs) to streamline and generalize the multi-robot coordination pipeline. LAN2CB transforms natural language (NL) mission descriptions into executable Python code for multi-robot systems through two core modules: (1) Mission Analysis, which parses mission descriptions into behavior trees, and (2) Code Generation, which leverages the behavior tree and a structured knowledge base to generate robot control code. We further introduce a dataset of natural language mission descriptions to support development and benchmarking. Experiments in both simulation and real-world environments demonstrate that LAN2CB enables robust and flexible multi-robot coordination from natural language, significantly reducing manual engineering effort and supporting broad generalization across diverse mission types. Website: https://sites.google.com/view/lan-cb
<div id='section'>Paperid: <span id='pid'>125, <a href='https://arxiv.org/pdf/2503.15435.pdf' target='_blank'>https://arxiv.org/pdf/2503.15435.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Baolu Li, Zongzhe Xu, Jinlong Li, Xinyu Liu, Jianwu Fang, Xiaopeng Li, Hongkai Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.15435">V2X-DG: Domain Generalization for Vehicle-to-Everything Cooperative Perception</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>LiDAR-based Vehicle-to-Everything (V2X) cooperative perception has demonstrated its impact on the safety and effectiveness of autonomous driving. Since current cooperative perception algorithms are trained and tested on the same dataset, the generalization ability of cooperative perception systems remains underexplored. This paper is the first work to study the Domain Generalization problem of LiDAR-based V2X cooperative perception (V2X-DG) for 3D detection based on four widely-used open source datasets: OPV2V, V2XSet, V2V4Real and DAIR-V2X. Our research seeks to sustain high performance not only within the source domain but also across other unseen domains, achieved solely through training on source domain. To this end, we propose Cooperative Mixup Augmentation based Generalization (CMAG) to improve the model generalization capability by simulating the unseen cooperation, which is designed compactly for the domain gaps in cooperative perception. Furthermore, we propose a constraint for the regularization of the robust generalized feature representation learning: Cooperation Feature Consistency (CFC), which aligns the intermediately fused features of the generalized cooperation by CMAG and the early fused features of the original cooperation in source domain. Extensive experiments demonstrate that our approach achieves significant performance gains when generalizing to other unseen datasets while it also maintains strong performance on the source dataset.
<div id='section'>Paperid: <span id='pid'>126, <a href='https://arxiv.org/pdf/2502.01172.pdf' target='_blank'>https://arxiv.org/pdf/2502.01172.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tim Felix Lakemann, Daniel Bonilla Licea, Viktor Walter, TomÃ¡Å¡ BÃ¡Äa, Martin Saska
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.01172">Towards agile multi-robot systems in the real world: Fast onboard tracking of active blinking markers for relative localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A novel onboard tracking approach enabling vision-based relative localization and communication using Active blinking Marker Tracking (AMT) is introduced in this article. Active blinking markers on multi-robot team members improve the robustness of relative localization for aerial vehicles in tightly coupled multi-robot systems during real-world deployments, while also serving as a resilient communication system. Traditional tracking algorithms struggle with fast-moving blinking markers due to their intermittent appearance in camera frames and the complexity of associating multiple of these markers across consecutive frames. AMT addresses this by using weighted polynomial regression to predict the future appearance of active blinking markers while accounting for uncertainty in the prediction. In outdoor experiments, the AMT approach outperformed state-of-the-art methods in tracking density, accuracy, and complexity. The experimental validation of this novel tracking approach for relative localization and optical communication involved testing motion patterns motivated by our research on agile multi-robot deployment.
<div id='section'>Paperid: <span id='pid'>127, <a href='https://arxiv.org/pdf/2502.01172.pdf' target='_blank'>https://arxiv.org/pdf/2502.01172.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tim Felix Lakemann, Daniel Bonilla Licea, Viktor Walter, TomÃ¡Å¡ BÃ¡Äa, Martin Saska
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.01172">Towards agile multi-robot systems in the real world: Fast onboard tracking of active blinking markers for relative localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A novel onboard tracking approach enabling vision-based relative localization and communication using Active blinking Marker Tracking (AMT) is introduced in this article. Active blinking markers on multi-robot team members improve the robustness of relative localization for aerial vehicles in tightly coupled multi-robot systems during real-world deployments, while also serving as a resilient communication system. Traditional tracking algorithms struggle with fast-moving blinking markers due to their intermittent appearance in camera frames and the complexity of associating multiple of these markers across consecutive frames. AMT addresses this by using weighted polynomial regression to predict the future appearance of active blinking markers while accounting for uncertainty in the prediction. In outdoor experiments, the AMT approach outperformed state-of-the-art methods in tracking density, accuracy, and complexity. The experimental validation of this novel tracking approach for relative localization and optical communication involved testing motion patterns motivated by our research on agile multi-robot deployment.
<div id='section'>Paperid: <span id='pid'>128, <a href='https://arxiv.org/pdf/2501.10447.pdf' target='_blank'>https://arxiv.org/pdf/2501.10447.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoxiao Li, Zhirui Sun, Hongpeng Wang, Shuai Li, Jiankun Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.10447">A Predictive Cooperative Collision Avoidance for Multi-Robot Systems Using Control Barrier Function</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Control barrier function (CBF)-based methods provide the minimum modification necessary to formally guarantee safety in the context of quadratic programming, and strict safety guarantee for safety critical systems. However, most CBF-related derivatives myopically focus on present safety at each time step, a reasoning over a look-ahead horizon is exactly missing. In this paper, a predictive safety matrix is constructed. We then consolidate the safety condition based on the smallest eigenvalue of the proposed safety matrix. A predefined deconfliction strategy of motion paths is embedded into the trajectory tracking module to manage deadlock conflicts, which computes the deadlock escape velocity with the minimum attitude angle. Comparison results show that the introduction of the predictive term is robust for measurement uncertainty and is immune to oscillations. The proposed deadlock avoidance method avoids a large detour, without obvious stagnation.
<div id='section'>Paperid: <span id='pid'>129, <a href='https://arxiv.org/pdf/2501.03585.pdf' target='_blank'>https://arxiv.org/pdf/2501.03585.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoxiao Li, Zhirui Sun, Mansha Zheng, Hongpeng Wang, Shuai Li, Jiankun Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.03585">Collision Risk Quantification and Conflict Resolution in Trajectory Tracking for Acceleration-Actuated Multi-Robot Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>One of the pivotal challenges in a multi-robot system is how to give attention to accuracy and efficiency while ensuring safety. Prior arts cannot strictly guarantee collision-free for an arbitrarily large number of robots or the results are considerably conservative. Smoothness of the avoidance trajectory also needs to be further optimized. This paper proposes an accelerationactuated simultaneous obstacle avoidance and trajectory tracking method for arbitrarily large teams of robots, that provides a nonconservative collision avoidance strategy and gives approaches for deadlock avoidance. We propose two ways of deadlock resolution, one involves incorporating an auxiliary velocity vector into the error function of the trajectory tracking module, which is proven to have no influence on global convergence of the tracking error. Furthermore, unlike the traditional methods that they address conflicts after a deadlock occurs, our decision-making mechanism avoids the near-zero velocity, which is much more safer and efficient in crowed environments. Extensive comparison show that the proposed method is superior to the existing studies when deployed in a large-scale robot system, with minimal invasiveness.
<div id='section'>Paperid: <span id='pid'>130, <a href='https://arxiv.org/pdf/2411.08851.pdf' target='_blank'>https://arxiv.org/pdf/2411.08851.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Irving Solis, James Motes, Mike Qin, Marco Morales, Nancy M. Amato
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.08851">Experience-based Subproblem Planning for Multi-Robot Motion Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-robot systems enhance efficiency and productivity across various applications, from manufacturing to surveillance. While single-robot motion planning has improved by using databases of prior solutions, extending this approach to multi-robot motion planning (MRMP) presents challenges due to the increased complexity and diversity of tasks and configurations. Recent discrete methods have attempted to address this by focusing on relevant lower-dimensional subproblems, but they are inadequate for complex scenarios like those involving manipulator robots. To overcome this, we propose a novel approach that %leverages experience-based planning by constructs and utilizes databases of solutions for smaller sub-problems. By focusing on interactions between fewer robots, our method reduces the need for exhaustive database growth, allowing for efficient handling of more complex MRMP scenarios. We validate our approach with experiments involving both mobile and manipulator robots, demonstrating significant improvements over existing methods in scalability and planning efficiency. Our contributions include a rapidly constructed database for low-dimensional MRMP problems, a framework for applying these solutions to larger problems, and experimental validation with up to 32 mobile and 16 manipulator robots.
<div id='section'>Paperid: <span id='pid'>131, <a href='https://arxiv.org/pdf/2409.11230.pdf' target='_blank'>https://arxiv.org/pdf/2409.11230.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peihan Li, Yuwei Wu, Jiazhen Liu, Gaurav S. Sukhatme, Vijay Kumar, Lifeng Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.11230">Resilient Multi-Robot Target Tracking with Sensing and Communication Danger Zones</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-robot collaboration for target tracking in adversarial environments poses significant challenges, including system failures, dynamic priority shifts, and other unpredictable factors. These challenges become even more pronounced when the environment is unknown. In this paper, we propose a resilient coordination framework for multi-robot, multi-target tracking in environments with unknown sensing and communication danger zones. We consider scenarios where failures caused by these danger zones are probabilistic and temporary, allowing robots to escape from danger zones to minimize the risk of future failures. We formulate this problem as a nonlinear optimization with soft chance constraints, enabling real-time adjustments to robot behaviors based on varying types of dangers and failures. This approach dynamically balances target tracking performance and resilience, adapting to evolving sensing and communication conditions in real-time. To validate the effectiveness of the proposed method, we assess its performance across various tracking scenarios, benchmark it against methods without resilient adaptation and collaboration, and conduct several real-world experiments.
<div id='section'>Paperid: <span id='pid'>132, <a href='https://arxiv.org/pdf/2404.07880.pdf' target='_blank'>https://arxiv.org/pdf/2404.07880.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiazhen Liu, Peihan Li, Yuwei Wu, Gaurav S. Sukhatme, Vijay Kumar, Lifeng Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.07880">Multi-Robot Target Tracking with Sensing and Communication Danger Zones</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-robot target tracking finds extensive applications in different scenarios, such as environmental surveillance and wildfire management, which require the robustness of the practical deployment of multi-robot systems in uncertain and dangerous environments. Traditional approaches often focus on the performance of tracking accuracy with no modeling and assumption of the environments, neglecting potential environmental hazards which result in system failures in real-world deployments. To address this challenge, we investigate multi-robot target tracking in the adversarial environment considering sensing and communication attacks with uncertainty. We design specific strategies to avoid different danger zones and proposed a multi-agent tracking framework under the perilous environment. We approximate the probabilistic constraints and formulate practical optimization strategies to address computational challenges efficiently. We evaluate the performance of our proposed methods in simulations to demonstrate the ability of robots to adjust their risk-aware behaviors under different levels of environmental uncertainty and risk confidence. The proposed method is further validated via real-world robot experiments where a team of drones successfully track dynamic ground robots while being risk-aware of the sensing and/or communication danger zones.
<div id='section'>Paperid: <span id='pid'>133, <a href='https://arxiv.org/pdf/2307.07935.pdf' target='_blank'>https://arxiv.org/pdf/2307.07935.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinlong Li, Runsheng Xu, Xinyu Liu, Baolu Li, Qin Zou, Jiaqi Ma, Hongkai Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.07935">S2R-ViT for Multi-Agent Cooperative Perception: Bridging the Gap from Simulation to Reality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Due to the lack of enough real multi-agent data and time-consuming of labeling, existing multi-agent cooperative perception algorithms usually select the simulated sensor data for training and validating. However, the perception performance is degraded when these simulation-trained models are deployed to the real world, due to the significant domain gap between the simulated and real data. In this paper, we propose the first Simulation-to-Reality transfer learning framework for multi-agent cooperative perception using a novel Vision Transformer, named as S2R-ViT, which considers both the Deployment Gap and Feature Gap between simulated and real data. We investigate the effects of these two types of domain gaps and propose a novel uncertainty-aware vision transformer to effectively relief the Deployment Gap and an agent-based feature adaptation module with inter-agent and ego-agent discriminators to reduce the Feature Gap. Our intensive experiments on the public multi-agent cooperative perception datasets OPV2V and V2V4Real demonstrate that the proposed S2R-ViT can effectively bridge the gap from simulation to reality and outperform other methods significantly for point cloud-based 3D object detection.
<div id='section'>Paperid: <span id='pid'>134, <a href='https://arxiv.org/pdf/2303.07601.pdf' target='_blank'>https://arxiv.org/pdf/2303.07601.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Runsheng Xu, Xin Xia, Jinlong Li, Hanzhao Li, Shuo Zhang, Zhengzhong Tu, Zonglin Meng, Hao Xiang, Xiaoyu Dong, Rui Song, Hongkai Yu, Bolei Zhou, Jiaqi Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.07601">V2V4Real: A Real-world Large-scale Dataset for Vehicle-to-Vehicle Cooperative Perception</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modern perception systems of autonomous vehicles are known to be sensitive to occlusions and lack the capability of long perceiving range. It has been one of the key bottlenecks that prevents Level 5 autonomy. Recent research has demonstrated that the Vehicle-to-Vehicle (V2V) cooperative perception system has great potential to revolutionize the autonomous driving industry. However, the lack of a real-world dataset hinders the progress of this field. To facilitate the development of cooperative perception, we present V2V4Real, the first large-scale real-world multi-modal dataset for V2V perception. The data is collected by two vehicles equipped with multi-modal sensors driving together through diverse scenarios. Our V2V4Real dataset covers a driving area of 410 km, comprising 20K LiDAR frames, 40K RGB frames, 240K annotated 3D bounding boxes for 5 classes, and HDMaps that cover all the driving routes. V2V4Real introduces three perception tasks, including cooperative 3D object detection, cooperative 3D object tracking, and Sim2Real domain adaptation for cooperative perception. We provide comprehensive benchmarks of recent cooperative perception algorithms on three tasks. The V2V4Real dataset can be found at https://research.seas.ucla.edu/mobility-lab/v2v4real/.
<div id='section'>Paperid: <span id='pid'>135, <a href='https://arxiv.org/pdf/2212.08273.pdf' target='_blank'>https://arxiv.org/pdf/2212.08273.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinlong Li, Runsheng Xu, Xinyu Liu, Jin Ma, Zicheng Chi, Jiaqi Ma, Hongkai Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.08273">Learning for Vehicle-to-Vehicle Cooperative Perception under Lossy Communication</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning has been widely used in the perception (e.g., 3D object detection) of intelligent vehicle driving. Due to the beneficial Vehicle-to-Vehicle (V2V) communication, the deep learning based features from other agents can be shared to the ego vehicle so as to improve the perception of the ego vehicle. It is named as Cooperative Perception in the V2V research, whose algorithms have been dramatically advanced recently. However, all the existing cooperative perception algorithms assume the ideal V2V communication without considering the possible lossy shared features because of the Lossy Communication (LC) which is common in the complex real-world driving scenarios. In this paper, we first study the side effect (e.g., detection performance drop) by the lossy communication in the V2V Cooperative Perception, and then we propose a novel intermediate LC-aware feature fusion method to relieve the side effect of lossy communication by a LC-aware Repair Network (LCRN) and enhance the interaction between the ego vehicle and other vehicles by a specially designed V2V Attention Module (V2VAM) including intra-vehicle attention of ego vehicle and uncertainty-aware inter-vehicle attention. The extensive experiment on the public cooperative perception dataset OPV2V (based on digital-twin CARLA simulator) demonstrates that the proposed method is quite effective for the cooperative point cloud based 3D object detection under lossy V2V communication.
<div id='section'>Paperid: <span id='pid'>136, <a href='https://arxiv.org/pdf/2207.05643.pdf' target='_blank'>https://arxiv.org/pdf/2207.05643.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Koorosh Aslansefat, Panagiota Nikolaou, Martin Walker, Mohammed Naveed Akram, Ioannis Sorokos, Jan Reich, Panayiotis Kolios, Maria K. Michael, Theocharis Theocharides, Georgios Ellinas, Daniel Schneider, Yiannis Papadopoulos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2207.05643">SafeDrones: Real-Time Reliability Evaluation of UAVs using Executable Digital Dependable Identities</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The use of Unmanned Arial Vehicles (UAVs) offers many advantages across a variety of applications. However, safety assurance is a key barrier to widespread usage, especially given the unpredictable operational and environmental factors experienced by UAVs, which are hard to capture solely at design-time. This paper proposes a new reliability modeling approach called SafeDrones to help address this issue by enabling runtime reliability and risk assessment of UAVs. It is a prototype instantiation of the Executable Digital Dependable Identity (EDDI) concept, which aims to create a model-based solution for real-time, data-driven dependability assurance for multi-robot systems. By providing real-time reliability estimates, SafeDrones allows UAVs to update their missions accordingly in an adaptive manner.
<div id='section'>Paperid: <span id='pid'>137, <a href='https://arxiv.org/pdf/2407.09902.pdf' target='_blank'>https://arxiv.org/pdf/2407.09902.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ian D. Miller, Fernando Cladera, Trey Smith, Camillo Jose Taylor, Vijay Kumar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.09902">Air-Ground Collaboration with SPOMP: Semantic Panoramic Online Mapping and Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mapping and navigation have gone hand-in-hand since long before robots existed. Maps are a key form of communication, allowing someone who has never been somewhere to nonetheless navigate that area successfully. In the context of multi-robot systems, the maps and information that flow between robots are necessary for effective collaboration, whether those robots are operating concurrently, sequentially, or completely asynchronously. In this paper, we argue that maps must go beyond encoding purely geometric or visual information to enable increasingly complex autonomy, particularly between robots. We propose a framework for multi-robot autonomy, focusing in particular on air and ground robots operating in outdoor 2.5D environments. We show that semantic maps can enable the specification, planning, and execution of complex collaborative missions, including localization in GPS-denied settings. A distinguishing characteristic of this work is that we strongly emphasize field experiments and testing, and by doing so demonstrate that these ideas can work at scale in the real world. We also perform extensive simulation experiments to validate our ideas at even larger scales. We believe these experiments and the experimental results constitute a significant step forward toward advancing the state-of-the-art of large-scale, collaborative multi-robot systems operating with real communication, navigation, and perception constraints.
<div id='section'>Paperid: <span id='pid'>138, <a href='https://arxiv.org/pdf/2403.16034.pdf' target='_blank'>https://arxiv.org/pdf/2403.16034.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Xiang, Zhaoliang Zheng, Xin Xia, Runsheng Xu, Letian Gao, Zewei Zhou, Xu Han, Xinkai Ji, Mingxi Li, Zonglin Meng, Li Jin, Mingyue Lei, Zhaoyang Ma, Zihang He, Haoxuan Ma, Yunshuang Yuan, Yingqian Zhao, Jiaqi Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.16034">V2X-Real: a Large-Scale Dataset for Vehicle-to-Everything Cooperative Perception</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in Vehicle-to-Everything (V2X) technologies have enabled autonomous vehicles to share sensing information to see through occlusions, greatly boosting the perception capability. However, there are no real-world datasets to facilitate the real V2X cooperative perception research -- existing datasets either only support Vehicle-to-Infrastructure cooperation or Vehicle-to-Vehicle cooperation. In this paper, we present V2X-Real, a large-scale dataset that includes a mixture of multiple vehicles and smart infrastructure to facilitate the V2X cooperative perception development with multi-modality sensing data. Our V2X-Real is collected using two connected automated vehicles and two smart infrastructure, which are all equipped with multi-modal sensors including LiDAR sensors and multi-view cameras. The whole dataset contains 33K LiDAR frames and 171K camera data with over 1.2M annotated bounding boxes of 10 categories in very challenging urban scenarios. According to the collaboration mode and ego perspective, we derive four types of datasets for Vehicle-Centric, Infrastructure-Centric, Vehicle-to-Vehicle, and Infrastructure-to-Infrastructure cooperative perception. Comprehensive multi-class multi-agent benchmarks of SOTA cooperative perception methods are provided. The V2X-Real dataset and codebase are available at https://mobility-lab.seas.ucla.edu/v2x-real.
<div id='section'>Paperid: <span id='pid'>139, <a href='https://arxiv.org/pdf/2310.07247.pdf' target='_blank'>https://arxiv.org/pdf/2310.07247.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wentao Jiang, Hao Xiang, Xinyu Cai, Runsheng Xu, Jiaqi Ma, Yikang Li, Gim Hee Lee, Si Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.07247">Optimizing the Placement of Roadside LiDARs for Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent cooperative perception is an increasingly popular topic in the field of autonomous driving, where roadside LiDARs play an essential role. However, how to optimize the placement of roadside LiDARs is a crucial but often overlooked problem. This paper proposes an approach to optimize the placement of roadside LiDARs by selecting optimized positions within the scene for better perception performance. To efficiently obtain the best combination of locations, a greedy algorithm based on perceptual gain is proposed, which selects the location that can maximize the perceptual gain sequentially. We define perceptual gain as the increased perceptual capability when a new LiDAR is placed. To obtain the perception capability, we propose a perception predictor that learns to evaluate LiDAR placement using only a single point cloud frame. A dataset named Roadside-Opt is created using the CARLA simulator to facilitate research on the roadside LiDAR placement problem.
<div id='section'>Paperid: <span id='pid'>140, <a href='https://arxiv.org/pdf/2309.04257.pdf' target='_blank'>https://arxiv.org/pdf/2309.04257.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andrea Testa, Guido Carnevale, Giuseppe Notarstefano
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.04257">A Tutorial on Distributed Optimization for Cooperative Robotics: from Setups and Algorithms to Toolboxes and Research Directions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Several interesting problems in multi-robot systems can be cast in the framework of distributed optimization. Examples include multi-robot task allocation, vehicle routing, target protection, and surveillance. While the theoretical analysis of distributed optimization algorithms has received significant attention, its application to cooperative robotics has not been investigated in detail. In this paper, we show how notable scenarios in cooperative robotics can be addressed by suitable distributed optimization setups. Specifically, after a brief introduction on the widely investigated consensus optimization (most suited for data analytics) and on the partition-based setup (matching the graph structure in the optimization), we focus on two distributed settings modeling several scenarios in cooperative robotics, i.e., the so-called constraint-coupled and aggregative optimization frameworks. For each one, we consider use-case applications, and we discuss tailored distributed algorithms with their convergence properties. Then, we revise state-of-the-art toolboxes allowing for the implementation of distributed schemes on real networks of robots without central coordinators. For each use case, we discuss its implementation in these toolboxes and provide simulations and real experiments on networks of heterogeneous robots.
<div id='section'>Paperid: <span id='pid'>141, <a href='https://arxiv.org/pdf/2304.10628.pdf' target='_blank'>https://arxiv.org/pdf/2304.10628.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Xiang, Runsheng Xu, Jiaqi Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.10628">HM-ViT: Hetero-modal Vehicle-to-Vehicle Cooperative perception with vision transformer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vehicle-to-Vehicle technologies have enabled autonomous vehicles to share information to see through occlusions, greatly enhancing perception performance. Nevertheless, existing works all focused on homogeneous traffic where vehicles are equipped with the same type of sensors, which significantly hampers the scale of collaboration and benefit of cross-modality interactions. In this paper, we investigate the multi-agent hetero-modal cooperative perception problem where agents may have distinct sensor modalities. We present HM-ViT, the first unified multi-agent hetero-modal cooperative perception framework that can collaboratively predict 3D objects for highly dynamic vehicle-to-vehicle (V2V) collaborations with varying numbers and types of agents. To effectively fuse features from multi-view images and LiDAR point clouds, we design a novel heterogeneous 3D graph transformer to jointly reason inter-agent and intra-agent interactions. The extensive experiments on the V2V perception dataset OPV2V demonstrate that the HM-ViT outperforms SOTA cooperative perception methods for V2V hetero-modal cooperative perception. We will release codes to facilitate future research.
<div id='section'>Paperid: <span id='pid'>142, <a href='https://arxiv.org/pdf/2301.11361.pdf' target='_blank'>https://arxiv.org/pdf/2301.11361.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ola Shorinwa, Trevor Halsted, Javier Yu, Mac Schwager
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.11361">Distributed Optimization Methods for Multi-Robot Systems: Part II -- A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Although the field of distributed optimization is well-developed, relevant literature focused on the application of distributed optimization to multi-robot problems is limited. This survey constitutes the second part of a two-part series on distributed optimization applied to multi-robot problems. In this paper, we survey three main classes of distributed optimization algorithms -- distributed first-order methods, distributed sequential convex programming methods, and alternating direction method of multipliers (ADMM) methods -- focusing on fully-distributed methods that do not require coordination or computation by a central computer. We describe the fundamental structure of each category and note important variations around this structure, designed to address its associated drawbacks. Further, we provide practical implications of noteworthy assumptions made by distributed optimization algorithms, noting the classes of robotics problems suitable for these algorithms. Moreover, we identify important open research challenges in distributed optimization, specifically for robotics problems.
<div id='section'>Paperid: <span id='pid'>143, <a href='https://arxiv.org/pdf/2301.11313.pdf' target='_blank'>https://arxiv.org/pdf/2301.11313.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ola Shorinwa, Trevor Halsted, Javier Yu, Mac Schwager
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.11313">Distributed Optimization Methods for Multi-Robot Systems: Part I -- A Tutorial</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Distributed optimization provides a framework for deriving distributed algorithms for a variety of multi-robot problems. This tutorial constitutes the first part of a two-part series on distributed optimization applied to multi-robot problems, which seeks to advance the application of distributed optimization in robotics. In this tutorial, we demonstrate that many canonical multi-robot problems can be cast within the distributed optimization framework, such as multi-robot simultaneous localization and planning (SLAM), multi-robot target tracking, and multi-robot task assignment problems. We identify three broad categories of distributed optimization algorithms: distributed first-order methods, distributed sequential convex programming, and the alternating direction method of multipliers (ADMM). We describe the basic structure of each category and provide representative algorithms within each category. We then work through a simulation case study of multiple drones collaboratively tracking a ground vehicle. We compare solutions to this problem using a number of different distributed optimization algorithms. In addition, we implement a distributed optimization algorithm in hardware on a network of Rasberry Pis communicating with XBee modules to illustrate robustness to the challenges of real-world communication networks.
<div id='section'>Paperid: <span id='pid'>144, <a href='https://arxiv.org/pdf/2505.11663.pdf' target='_blank'>https://arxiv.org/pdf/2505.11663.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaleb Ben Naveed, Devansh R. Agrawal, Rahul Kumar, Dimitra Panagou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.11663">Adaptive Ergodic Search with Energy-Aware Scheduling for Persistent Multi-Robot Missions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous robots are increasingly deployed for long-term information-gathering tasks, which pose two key challenges: planning informative trajectories in environments that evolve across space and time, and ensuring persistent operation under energy constraints. This paper presents a unified framework, mEclares, that addresses both challenges through adaptive ergodic search and energy-aware scheduling in multi-robot systems. Our contributions are two-fold: (1) we model real-world variability using stochastic spatiotemporal environments, where the underlying information evolves unpredictably due to process uncertainty. To guide exploration, we construct a target information spatial distribution (TISD) based on clarity, a metric that captures the decay of information in the absence of observations and highlights regions of high uncertainty; and (2) we introduce Robustmesch (Rmesch), an online scheduling method that enables persistent operation by coordinating rechargeable robots sharing a single mobile charging station. Unlike prior work, our approach avoids reliance on preplanned schedules, static or dedicated charging stations, and simplified robot dynamics. Instead, the scheduler supports general nonlinear models, accounts for uncertainty in the estimated position of the charging station, and handles central node failures. The proposed framework is validated through real-world hardware experiments, and feasibility guarantees are provided under specific assumptions.
<div id='section'>Paperid: <span id='pid'>145, <a href='https://arxiv.org/pdf/2504.03120.pdf' target='_blank'>https://arxiv.org/pdf/2504.03120.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haejoon Lee, Dimitra Panagou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.03120">Distributed Resilience-Aware Control in Multi-Robot Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Ensuring resilient consensus in multi-robot systems with misbehaving agents remains a challenge, as many existing network resilience properties are inherently combinatorial and globally defined. While previous works have proposed control laws to enhance or preserve resilience in multi-robot networks, they often assume a fixed topology with known resilience properties, or require global state knowledge. These assumptions may be impractical in physically-constrained environments, where safety and resilience requirements are conflicting, or when misbehaving agents share inaccurate state information. In this work, we propose a distributed control law that enables each robot to guarantee resilient consensus and safety during its navigation without fixed topologies using only locally available information. To this end, we establish a sufficient condition for resilient consensus in time-varying networks based on the degree of non-misbehaving or normal agents. Using this condition, we design a Control Barrier Function (CBF)-based controller that guarantees resilient consensus and collision avoidance without requiring estimates of global state and/or control actions of all other robots. Finally, we validate our method through simulations.
<div id='section'>Paperid: <span id='pid'>146, <a href='https://arxiv.org/pdf/2504.03120.pdf' target='_blank'>https://arxiv.org/pdf/2504.03120.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haejoon Lee, Dimitra Panagou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.03120">Distributed Resilience-Aware Control in Multi-Robot Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Ensuring resilient consensus in multi-robot systems with misbehaving agents remains a challenge, as many existing network resilience properties are inherently combinatorial and globally defined. While previous works have proposed control laws to enhance or preserve resilience in multi-robot networks, they often assume a fixed topology with known resilience properties, or require global state knowledge. These assumptions may be impractical in physically-constrained environments, where safety and resilience requirements are conflicting, or when misbehaving agents share inaccurate state information. In this work, we propose a distributed control law that enables each robot to guarantee resilient consensus and safety during its navigation without fixed topologies using only locally available information. To this end, we establish a sufficient condition for resilient consensus in time-varying networks based on the degree of non-misbehaving or normal agents. Using this condition, we design a Control Barrier Function (CBF)-based controller that guarantees resilient consensus and collision avoidance without requiring estimates of global state and/or control actions of all other robots. Finally, we validate our method through simulations.
<div id='section'>Paperid: <span id='pid'>147, <a href='https://arxiv.org/pdf/2410.12163.pdf' target='_blank'>https://arxiv.org/pdf/2410.12163.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kui Wang, Kazuma Nonomura, Zongdian Li, Tao Yu, Kei Sakaguchi, Omar Hashash, Walid Saad, Changyang She, Yonghui Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.12163">Augmented Intelligence in Smart Intersections: Local Digital Twins-Assisted Hybrid Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vehicle-road collaboration is a promising approach for enhancing the safety and efficiency of autonomous driving by extending the intelligence of onboard systems to smart roadside infrastructures. The introduction of digital twins (DTs), particularly local DTs (LDTs) at the edge, in smart mobility presents a new embodiment of augmented intelligence, which could enhance information exchange and extract human driving expertise to improve onboard intelligence. This paper presents a novel LDT-assisted hybrid autonomous driving system for improving safety and efficiency in traffic intersections. By leveraging roadside units (RSUs) equipped with sensory and computing capabilities, the proposed system continuously monitors traffic, extracts human driving knowledge, and generates intersection-specific local driving agents through an offline reinforcement learning (RL) framework. When connected and automated vehicles (CAVs) pass through RSU-equipped intersections, RSUs can provide local agents to support safe and efficient driving in local areas. Meanwhile, they provide real-time cooperative perception (CP) to broaden onboard sensory horizons. The proposed LDT-assisted hybrid system is implemented with state-of-the-art products, e.g., CAVs and RSUs, and technologies, e.g., millimeter-wave (mmWave) communications. Hardware-in-the-loop (HiL) simulations and proof-of-concept (PoC) tests validate system performance from two standpoints: (i) The peak latency for CP and local agent downloading are 8.51 ms and 146 ms, respectively, aligning with 3GPP requirements for vehicle-to-everything (V2X) and model transfer use cases. Moreover, (ii) local driving agents can improve safety measures by 10% and reduce travel time by 15% compared with conventional onboard systems. The implemented prototype also demonstrates reliable real-time performance, fulfilling the targets of the proposed system design.
<div id='section'>Paperid: <span id='pid'>148, <a href='https://arxiv.org/pdf/2305.08532.pdf' target='_blank'>https://arxiv.org/pdf/2305.08532.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Paola Torrico MorÃ³n, Sahar Salimpour, Lei Fu, Xianjia Yu, Jorge PeÃ±a Queralta, Tomi Westerlund
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.08532">Benchmarking UWB-Based Infrastructure-Free Positioning and Multi-Robot Relative Localization: Dataset and Characterization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Ultra-wideband (UWB) positioning has emerged as a low-cost and dependable localization solution for multiple use cases, from mobile robots to asset tracking within the Industrial IoT. The technology is mature and the scientific literature contains multiple datasets and methods for localization based on fixed UWB nodes. At the same time, research in UWB-based relative localization and infrastructure-free localization is gaining traction, further domains. tools and datasets in this domain are scarce. Therefore, we introduce in this paper a novel dataset for benchmarking infrastructure-free relative localization targeting the domain of multi-robot systems. Compared to previous datasets, we analyze the performance of different relative localization approaches for a much wider variety of scenarios with varying numbers of fixed and mobile nodes. A motion capture system provides ground truth data, are multi-modal and include inertial or odometry measurements for benchmarking sensor fusion methods. Additionally, the dataset contains measurements of ranging accuracy based on the relative orientation of antennas and a comprehensive set of measurements for ranging between a single pair of nodes. Our experimental analysis shows that high accuracy can be localization, but the variability of the ranging error is significant across different settings and setups.
<div id='section'>Paperid: <span id='pid'>149, <a href='https://arxiv.org/pdf/2303.17207.pdf' target='_blank'>https://arxiv.org/pdf/2303.17207.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sahar Salimpour, Paola Torrico MorÃ³n, Xianjia Yu, Tomi Westerlund, Jorge PeÃ±a Queralta
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.17207">Exploiting Redundancy for UWB Anomaly Detection in Infrastructure-Free Multi-Robot Relative Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Ultra-wideband (UWB) localization methods have emerged as a cost-effective and accurate solution for GNSS-denied environments. There is a significant amount of previous research in terms of resilience of UWB ranging, with non-line-of-sight and multipath detection methods. However, little attention has been paid to resilience against disturbances in relative localization systems involving multiple nodes. This paper presents an approach to detecting range anomalies in UWB ranging measurements from the perspective of multi-robot cooperative localization. We introduce an approach to exploiting redundancy for relative localization in multi-robot systems, where the position of each node is calculated using different subsets of available data. This enables us to effectively identify nodes that present ranging anomalies and eliminate their effect within the cooperative localization scheme. We analyze anomalies created by timing errors in the ranging process, e.g., owing to malfunctioning hardware. However, our method is generic and can be extended to other types of ranging anomalies. Our approach results in a more resilient cooperative localization framework with a negligible impact in terms of the computational workload.
<div id='section'>Paperid: <span id='pid'>150, <a href='https://arxiv.org/pdf/2506.15868.pdf' target='_blank'>https://arxiv.org/pdf/2506.15868.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingyue Lei, Zewei Zhou, Hongchen Li, Jia Hu, Jiaqi Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.15868">CooperRisk: A Driving Risk Quantification Pipeline with Multi-Agent Cooperative Perception and Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Risk quantification is a critical component of safe autonomous driving, however, constrained by the limited perception range and occlusion of single-vehicle systems in complex and dense scenarios. Vehicle-to-everything (V2X) paradigm has been a promising solution to sharing complementary perception information, nevertheless, how to ensure the risk interpretability while understanding multi-agent interaction with V2X remains an open question. In this paper, we introduce the first V2X-enabled risk quantification pipeline, CooperRisk, to fuse perception information from multiple agents and quantify the scenario driving risk in future multiple timestamps. The risk is represented as a scenario risk map to ensure interpretability based on risk severity and exposure, and the multi-agent interaction is captured by the learning-based cooperative prediction model. We carefully design a risk-oriented transformer-based prediction model with multi-modality and multi-agent considerations. It aims to ensure scene-consistent future behaviors of multiple agents and avoid conflicting predictions that could lead to overly conservative risk quantification and cause the ego vehicle to become overly hesitant to drive. Then, the temporal risk maps could serve to guide a model predictive control planner. We evaluate the CooperRisk pipeline in a real-world V2X dataset V2XPnP, and the experiments demonstrate its superior performance in risk quantification, showing a 44.35% decrease in conflict rate between the ego vehicle and background traffic participants.
<div id='section'>Paperid: <span id='pid'>151, <a href='https://arxiv.org/pdf/2503.10034.pdf' target='_blank'>https://arxiv.org/pdf/2503.10034.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Xiang, Zhaoliang Zheng, Xin Xia, Seth Z. Zhao, Letian Gao, Zewei Zhou, Tianhui Cai, Yun Zhang, Jiaqi Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.10034">V2X-ReaLO: An Open Online Framework and Dataset for Cooperative Perception in Reality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cooperative perception enabled by Vehicle-to-Everything (V2X) communication holds significant promise for enhancing the perception capabilities of autonomous vehicles, allowing them to overcome occlusions and extend their field of view. However, existing research predominantly relies on simulated environments or static datasets, leaving the feasibility and effectiveness of V2X cooperative perception especially for intermediate fusion in real-world scenarios largely unexplored. In this work, we introduce V2X-ReaLO, an open online cooperative perception framework deployed on real vehicles and smart infrastructure that integrates early, late, and intermediate fusion methods within a unified pipeline and provides the first practical demonstration of online intermediate fusion's feasibility and performance under genuine real-world conditions. Additionally, we present an open benchmark dataset specifically designed to assess the performance of online cooperative perception systems. This new dataset extends V2X-Real dataset to dynamic, synchronized ROS bags and provides 25,028 test frames with 6,850 annotated key frames in challenging urban scenarios. By enabling real-time assessments of perception accuracy and communication lantency under dynamic conditions, V2X-ReaLO sets a new benchmark for advancing and optimizing cooperative perception systems in real-world applications. The codes and datasets will be released to further advance the field.
<div id='section'>Paperid: <span id='pid'>152, <a href='https://arxiv.org/pdf/2412.01812.pdf' target='_blank'>https://arxiv.org/pdf/2412.01812.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zewei Zhou, Hao Xiang, Zhaoliang Zheng, Seth Z. Zhao, Mingyue Lei, Yun Zhang, Tianhui Cai, Xinyi Liu, Johnson Liu, Maheswari Bajji, Xin Xia, Zhiyu Huang, Bolei Zhou, Jiaqi Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.01812">V2XPnP: Vehicle-to-Everything Spatio-Temporal Fusion for Multi-Agent Perception and Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vehicle-to-everything (V2X) technologies offer a promising paradigm to mitigate the limitations of constrained observability in single-vehicle systems. Prior work primarily focuses on single-frame cooperative perception, which fuses agents' information across different spatial locations but ignores temporal cues and temporal tasks (e.g., temporal perception and prediction). In this paper, we focus on the spatio-temporal fusion in V2X scenarios and design one-step and multi-step communication strategies (when to transmit) as well as examine their integration with three fusion strategies - early, late, and intermediate (what to transmit), providing comprehensive benchmarks with 11 fusion models (how to fuse). Furthermore, we propose V2XPnP, a novel intermediate fusion framework within one-step communication for end-to-end perception and prediction. Our framework employs a unified Transformer-based architecture to effectively model complex spatio-temporal relationships across multiple agents, frames, and high-definition maps. Moreover, we introduce the V2XPnP Sequential Dataset that supports all V2X collaboration modes and addresses the limitations of existing real-world datasets, which are restricted to single-frame or single-mode cooperation. Extensive experiments demonstrate that our framework outperforms state-of-the-art methods in both perception and prediction tasks.
<div id='section'>Paperid: <span id='pid'>153, <a href='https://arxiv.org/pdf/2411.03672.pdf' target='_blank'>https://arxiv.org/pdf/2411.03672.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yansong Qu, Zixuan Xu, Zilin Huang, Zihao Sheng, Tiantian Chen, Sikai Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.03672">MetaSSC: Enhancing 3D Semantic Scene Completion for Autonomous Driving through Meta-Learning and Long-sequence Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Semantic scene completion (SSC) is essential for achieving comprehensive perception in autonomous driving systems. However, existing SSC methods often overlook the high deployment costs in real-world applications. Traditional architectures, such as 3D Convolutional Neural Networks (3D CNNs) and self-attention mechanisms, face challenges in efficiently capturing long-range dependencies within 3D voxel grids, limiting their effectiveness. To address these issues, we introduce MetaSSC, a novel meta-learning-based framework for SSC that leverages deformable convolution, large-kernel attention, and the Mamba (D-LKA-M) model. Our approach begins with a voxel-based semantic segmentation (SS) pretraining task, aimed at exploring the semantics and geometry of incomplete regions while acquiring transferable meta-knowledge. Using simulated cooperative perception datasets, we supervise the perception training of a single vehicle using aggregated sensor data from multiple nearby connected autonomous vehicles (CAVs), generating richer and more comprehensive labels. This meta-knowledge is then adapted to the target domain through a dual-phase training strategy that does not add extra model parameters, enabling efficient deployment. To further enhance the model's capability in capturing long-sequence relationships within 3D voxel grids, we integrate Mamba blocks with deformable convolution and large-kernel attention into the backbone network. Extensive experiments demonstrate that MetaSSC achieves state-of-the-art performance, significantly outperforming competing models while also reducing deployment costs.
<div id='section'>Paperid: <span id='pid'>154, <a href='https://arxiv.org/pdf/2408.10531.pdf' target='_blank'>https://arxiv.org/pdf/2408.10531.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaru Zhong, Haibao Yu, Tianyi Zhu, Jiahui Xu, Wenxian Yang, Zaiqing Nie, Chao Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.10531">Leveraging Temporal Contexts to Enhance Vehicle-Infrastructure Cooperative Perception</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Infrastructure sensors installed at elevated positions offer a broader perception range and encounter fewer occlusions. Integrating both infrastructure and ego-vehicle data through V2X communication, known as vehicle-infrastructure cooperation, has shown considerable advantages in enhancing perception capabilities and addressing corner cases encountered in single-vehicle autonomous driving. However, cooperative perception still faces numerous challenges, including limited communication bandwidth and practical communication interruptions. In this paper, we propose CTCE, a novel framework for cooperative 3D object detection. This framework transmits queries with temporal contexts enhancement, effectively balancing transmission efficiency and performance to accommodate real-world communication conditions. Additionally, we propose a temporal-guided fusion module to further improve performance. The roadside temporal enhancement and vehicle-side spatial-temporal fusion together constitute a multi-level temporal contexts integration mechanism, fully leveraging temporal information to enhance performance. Furthermore, a motion-aware reconstruction module is introduced to recover lost roadside queries due to communication interruptions. Experimental results on V2X-Seq and V2X-Sim datasets demonstrate that CTCE outperforms the baseline QUEST, achieving improvements of 3.8% and 1.3% in mAP, respectively. Experiments under communication interruption conditions validate CTCE's robustness to communication interruptions.
<div id='section'>Paperid: <span id='pid'>155, <a href='https://arxiv.org/pdf/2404.03834.pdf' target='_blank'>https://arxiv.org/pdf/2404.03834.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Md Ishat-E-Rabban, Guangyao Shi, Griffin Bonner, Pratap Tokekar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.03834">Fast k-connectivity Restoration in Multi-Robot Systems for Robust Communication Maintenance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Maintaining a robust communication network plays an important role in the success of a multi-robot team jointly performing an optimization task. A key characteristic of a robust cooperative multi-robot system is the ability to repair the communication topology in the case of robot failure. In this paper, we focus on the Fast k-connectivity Restoration (FCR) problem, which aims to repair a network to make it k-connected with minimum robot movement. We develop a Quadratically Constrained Program (QCP) formulation of the FCR problem, which provides a way to optimally solve the problem, but cannot handle large instances due to high computational overhead. We therefore present a scalable algorithm, called EA-SCR, for the FCR problem using graph theoretic concepts. By conducting empirical studies, we demonstrate that the EA-SCR algorithm performs within 10 percent of the optimal while being orders of magnitude faster. We also show that EA-SCR outperforms existing solutions by 30 percent in terms of the FCR distance metric.
<div id='section'>Paperid: <span id='pid'>156, <a href='https://arxiv.org/pdf/2309.13526.pdf' target='_blank'>https://arxiv.org/pdf/2309.13526.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiang Liu, Yongjie Xue, Yuru Zhang, Dawei Chen, Kyungtae Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.13526">AdaMap: High-Scalable Real-Time Cooperative Perception at the Edge</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cooperative perception is the key approach to augment the perception of connected and automated vehicles (CAVs) toward safe autonomous driving. However, it is challenging to achieve real-time perception sharing for hundreds of CAVs in large-scale deployment scenarios. In this paper, we propose AdaMap, a new high-scalable real-time cooperative perception system, which achieves assured percentile end-to-end latency under time-varying network dynamics. To achieve AdaMap, we design a tightly coupled data plane and control plane. In the data plane, we design a new hybrid localization module to dynamically switch between object detection and tracking, and a novel point cloud representation module to adaptively compress and reconstruct the point cloud of detected objects. In the control plane, we design a new graph-based object selection method to un-select excessive multi-viewed point clouds of objects, and a novel approximated gradient descent algorithm to optimize the representation of point clouds. We implement AdaMap on an emulation platform, including realistic vehicle and server computation and a simulated 5G network, under a 150-CAV trace collected from the CARLA simulator. The evaluation results show that, AdaMap reduces up to 49x average transmission data size at the cost of 0.37 reconstruction loss, as compared to state-of-the-art solutions, which verifies its high scalability, adaptability, and computation efficiency.
<div id='section'>Paperid: <span id='pid'>157, <a href='https://arxiv.org/pdf/2308.10966.pdf' target='_blank'>https://arxiv.org/pdf/2308.10966.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rohan Chandra, Vrushabh Zinage, Efstathios Bakolas, Peter Stone, Joydeep Biswas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.10966">Deadlock-free, Safe, and Decentralized Multi-Robot Navigation in Social Mini-Games via Discrete-Time Control Barrier Functions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present an approach to ensure safe and deadlock-free navigation for decentralized multi-robot systems operating in constrained environments, including doorways and intersections. Although many solutions have been proposed that ensure safety and resolve deadlocks, optimally preventing deadlocks in a minimally invasive and decentralized fashion remains an open problem. We first formalize the objective as a non-cooperative, non-communicative, partially observable multi-robot navigation problem in constrained spaces with multiple conflicting agents, which we term as social mini-games. Formally, we solve a discrete-time optimal receding horizon control problem leveraging control barrier functions for safe long-horizon planning. Our approach to ensuring liveness rests on the insight that \textit{there exists barrier certificates that allow each robot to preemptively perturb their state in a minimally-invasive fashion onto liveness sets i.e. states where robots are deadlock-free}. We evaluate our approach in simulation as well on physical robots using F$1/10$ robots, a Clearpath Jackal, as well as a Boston Dynamics Spot in a doorway, hallway, and corridor intersection scenario. Compared to both fully decentralized and centralized approaches with and without deadlock resolution capabilities, we demonstrate that our approach results in safer, more efficient, and smoother navigation, based on a comprehensive set of metrics including success rate, collision rate, stop time, change in velocity, path deviation, time-to-goal, and flow rate.
<div id='section'>Paperid: <span id='pid'>158, <a href='https://arxiv.org/pdf/2308.01804.pdf' target='_blank'>https://arxiv.org/pdf/2308.01804.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Siqi Fan, Haibao Yu, Wenxian Yang, Jirui Yuan, Zaiqing Nie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.01804">QUEST: Query Stream for Practical Cooperative Perception</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cooperative perception can effectively enhance individual perception performance by providing additional viewpoint and expanding the sensing field. Existing cooperation paradigms are either interpretable (result cooperation) or flexible (feature cooperation). In this paper, we propose the concept of query cooperation to enable interpretable instance-level flexible feature interaction. To specifically explain the concept, we propose a cooperative perception framework, termed QUEST, which let query stream flow among agents. The cross-agent queries are interacted via fusion for co-aware instances and complementation for individual unaware instances. Taking camera-based vehicle-infrastructure perception as a typical practical application scene, the experimental results on the real-world dataset, DAIR-V2X-Seq, demonstrate the effectiveness of QUEST and further reveal the advantage of the query cooperation paradigm on transmission flexibility and robustness to packet dropout. We hope our work can further facilitate the cross-agent representation interaction for better cooperative perception in practice.
<div id='section'>Paperid: <span id='pid'>159, <a href='https://arxiv.org/pdf/2303.01940.pdf' target='_blank'>https://arxiv.org/pdf/2303.01940.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Stefano Bonato, Stefano Carlo Lambertenghi, Elia Cereda, Alessandro Giusti, Daniele Palossi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.01940">Ultra-low Power Deep Learning-based Monocular Relative Localization Onboard Nano-quadrotors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Precise relative localization is a crucial functional block for swarm robotics. This work presents a novel autonomous end-to-end system that addresses the monocular relative localization, through deep neural networks (DNNs), of two peer nano-drones, i.e., sub-40g of weight and sub-100mW processing power. To cope with the ultra-constrained nano-drone platform, we propose a vertically-integrated framework, from the dataset collection to the final in-field deployment, including dataset augmentation, quantization, and system optimizations. Experimental results show that our DNN can precisely localize a 10cm-size target nano-drone by employing only low-resolution monochrome images, up to ~2m distance. On a disjoint testing dataset our model yields a mean R2 score of 0.42 and a root mean square error of 18cm, which results in a mean in-field prediction error of 15cm and in a closed-loop control error of 17cm, over a ~60s-flight test. Ultimately, the proposed system improves the State-of-the-Art by showing long-endurance tracking performance (up to 2min continuous tracking), generalization capabilities being deployed in a never-seen-before environment, and requiring a minimal power consumption of 95mW for an onboard real-time inference-rate of 48Hz.
<div id='section'>Paperid: <span id='pid'>160, <a href='https://arxiv.org/pdf/2210.08731.pdf' target='_blank'>https://arxiv.org/pdf/2210.08731.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lin Ma, Longrui Chen, Yan Zhang, Mengdi Chu, Wenjie Jiang, Jiahao Shen, Chuxuan Li, Yifeng Shi, Nairui Luo, Jirui Yuan, Guyue Zhou, Jiangtao Gong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.08731">Evaluation of Pedestrian Safety in a High-Fidelity Simulation Environment Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pedestrians' safety is a crucial factor in assessing autonomous driving scenarios. However, pedestrian safety evaluation is rarely considered by existing autonomous driving simulation platforms. This paper proposes a pedestrian safety evaluation method for autonomous driving, in which not only the collision events but also the conflict events together with the characteristics of pedestrians are fully considered. Moreover, to apply the pedestrian safety evaluation system, we construct a high-fidelity simulation framework embedded with pedestrian safety-critical characteristics. We demonstrate our simulation framework and pedestrian safety evaluation with a comparative experiment with two kinds of autonomous driving perception algorithms -- single-vehicle perception and vehicle-to-infrastructure (V2I) cooperative perception. The results show that our framework can evaluate different autonomous driving algorithms with detailed and quantitative pedestrian safety indexes. To this end, the proposed simulation method and framework can be used to access different autonomous driving algorithms and evaluate pedestrians' safety performance in future autonomous driving simulations, which can inspire more pedestrian-friendly autonomous driving algorithms.
<div id='section'>Paperid: <span id='pid'>161, <a href='https://arxiv.org/pdf/2011.00685.pdf' target='_blank'>https://arxiv.org/pdf/2011.00685.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Md Ishat-E-Rabban, Guangyao Shi, Pratap Tokekar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2011.00685">Fast Biconnectivity Restoration in Multi-Robot Systems for Robust Communication Maintenance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Maintaining a robust communication network plays an important role in the success of a multi-robot team jointly performing an optimization task. A key characteristic of a robust multi-robot system is the ability to repair the communication topology itself in the case of robot failure. In this paper, we focus on the Fast Biconnectivity Restoration (FBR) problem, which aims to repair a connected network to make it biconnected as fast as possible, where a biconnected network is a communication topology that cannot be disconnected by removing one node. We develop a Quadratically Constrained Program (QCP) formulation of the FBR problem, which provides a way to optimally solve the problem. We also propose an approximation algorithm for the FBR problem based on graph theory. By conducting empirical studies, we demonstrate that our proposed approximation algorithm performs close to the optimal while significantly outperforming the existing solutions.
<div id='section'>Paperid: <span id='pid'>162, <a href='https://arxiv.org/pdf/2509.15052.pdf' target='_blank'>https://arxiv.org/pdf/2509.15052.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Walker Gosrich, Saurav Agarwal, Kashish Garg, Siddharth Mayya, Matthew Malencia, Mark Yim, Vijay Kumar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15052">Online Multi-Robot Coordination and Cooperation with Task Precedence Relationships</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a new formulation for the multi-robot task allocation problem that incorporates (a) complex precedence relationships between tasks, (b) efficient intra-task coordination, and (c) cooperation through the formation of robot coalitions. A task graph specifies the tasks and their relationships, and a set of reward functions models the effects of coalition size and preceding task performance. Maximizing task rewards is NP-hard; hence, we propose network flow-based algorithms to approximate solutions efficiently. A novel online algorithm performs iterative re-allocation, providing robustness to task failures and model inaccuracies to achieve higher performance than offline approaches. We comprehensively evaluate the algorithms in a testbed with random missions and reward functions and compare them to a mixed-integer solver and a greedy heuristic. Additionally, we validate the overall approach in an advanced simulator, modeling reward functions based on realistic physical phenomena and executing the tasks with realistic robot dynamics. Results establish efficacy in modeling complex missions and efficiency in generating high-fidelity task plans while leveraging task relationships.
<div id='section'>Paperid: <span id='pid'>163, <a href='https://arxiv.org/pdf/2509.15052.pdf' target='_blank'>https://arxiv.org/pdf/2509.15052.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Walker Gosrich, Saurav Agarwal, Kashish Garg, Siddharth Mayya, Matthew Malencia, Mark Yim, Vijay Kumar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15052">Online Multi-Robot Coordination and Cooperation with Task Precedence Relationships</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a new formulation for the multi-robot task allocation problem that incorporates (a) complex precedence relationships between tasks, (b) efficient intra-task coordination, and (c) cooperation through the formation of robot coalitions. A task graph specifies the tasks and their relationships, and a set of reward functions models the effects of coalition size and preceding task performance. Maximizing task rewards is NP-hard; hence, we propose network flow-based algorithms to approximate solutions efficiently. A novel online algorithm performs iterative re-allocation, providing robustness to task failures and model inaccuracies to achieve higher performance than offline approaches. We comprehensively evaluate the algorithms in a testbed with random missions and reward functions and compare them to a mixed-integer solver and a greedy heuristic. Additionally, we validate the overall approach in an advanced simulator, modeling reward functions based on realistic physical phenomena and executing the tasks with realistic robot dynamics. Results establish efficacy in modeling complex missions and efficiency in generating high-fidelity task plans while leveraging task relationships.
<div id='section'>Paperid: <span id='pid'>164, <a href='https://arxiv.org/pdf/2509.13095.pdf' target='_blank'>https://arxiv.org/pdf/2509.13095.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zijie Zhao, Honglei Guo, Shengqian Chen, Kaixuan Xu, Bo Jiang, Yuanheng Zhu, Dongbin Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.13095">Empowering Multi-Robot Cooperation via Sequential World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Model-based reinforcement learning (MBRL) has shown significant potential in robotics due to its high sample efficiency and planning capability. However, extending MBRL to multi-robot cooperation remains challenging due to the complexity of joint dynamics and the reliance on synchronous communication. SeqWM employs independent, autoregressive agent-wise world models to represent joint dynamics, where each agent generates its future trajectory and plans its actions based on the predictions of its predecessors. This design lowers modeling complexity, alleviates the reliance on communication synchronization, and enables the emergence of advanced cooperative behaviors through explicit intention sharing. Experiments in challenging simulated environments (Bi-DexHands and Multi-Quad) demonstrate that SeqWM outperforms existing state-of-the-art model-based and model-free baselines in both overall performance and sample efficiency, while exhibiting advanced cooperative behaviors such as predictive adaptation, temporal alignment, and role division. Furthermore, SeqWM has been success fully deployed on physical quadruped robots, demonstrating its effectiveness in real-world multi-robot systems. Demos and code are available at: https://sites.google.com/view/seqwm-marl
<div id='section'>Paperid: <span id='pid'>165, <a href='https://arxiv.org/pdf/2509.13095.pdf' target='_blank'>https://arxiv.org/pdf/2509.13095.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zijie Zhao, Honglei Guo, Shengqian Chen, Kaixuan Xu, Bo Jiang, Yuanheng Zhu, Dongbin Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.13095">Empowering Multi-Robot Cooperation via Sequential World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Model-based reinforcement learning (MBRL) has shown significant potential in robotics due to its high sample efficiency and planning capability. However, extending MBRL to multi-robot cooperation remains challenging due to the complexity of joint dynamics and the reliance on synchronous communication. SeqWM employs independent, autoregressive agent-wise world models to represent joint dynamics, where each agent generates its future trajectory and plans its actions based on the predictions of its predecessors. This design lowers modeling complexity, alleviates the reliance on communication synchronization, and enables the emergence of advanced cooperative behaviors through explicit intention sharing. Experiments in challenging simulated environments (Bi-DexHands and Multi-Quad) demonstrate that SeqWM outperforms existing state-of-the-art model-based and model-free baselines in both overall performance and sample efficiency, while exhibiting advanced cooperative behaviors such as predictive adaptation, temporal alignment, and role division. Furthermore, SeqWM has been success fully deployed on physical quadruped robots, demonstrating its effectiveness in real-world multi-robot systems. Demos and code are available at: https://sites.google.com/view/seqwm-marl
<div id='section'>Paperid: <span id='pid'>166, <a href='https://arxiv.org/pdf/2508.20898.pdf' target='_blank'>https://arxiv.org/pdf/2508.20898.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaxi Huang, Yan Huang, Yixian Zhao, Wenchao Meng, Jinming Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.20898">CoCoL: A Communication Efficient Decentralized Collaborative Method for Multi-Robot Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Collaborative learning enhances the performance and adaptability of multi-robot systems in complex tasks but faces significant challenges due to high communication overhead and data heterogeneity inherent in multi-robot tasks. To this end, we propose CoCoL, a Communication efficient decentralized Collaborative Learning method tailored for multi-robot systems with heterogeneous local datasets. Leveraging a mirror descent framework, CoCoL achieves remarkable communication efficiency with approximate Newton-type updates by capturing the similarity between objective functions of robots, and reduces computational costs through inexact sub-problem solutions. Furthermore, the integration of a gradient tracking scheme ensures its robustness against data heterogeneity. Experimental results on three representative multi robot collaborative learning tasks show the superiority of the proposed CoCoL in significantly reducing both the number of communication rounds and total bandwidth consumption while maintaining state-of-the-art accuracy. These benefits are particularly evident in challenging scenarios involving non-IID (non-independent and identically distributed) data distribution, streaming data, and time-varying network topologies.
<div id='section'>Paperid: <span id='pid'>167, <a href='https://arxiv.org/pdf/2508.02529.pdf' target='_blank'>https://arxiv.org/pdf/2508.02529.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peihan Li, Jiazhen Liu, Yuwei Wu, Lifeng Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02529">Failure-Aware Multi-Robot Coordination for Resilient and Adaptive Target Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-robot coordination is crucial for autonomous systems, yet real-world deployments often encounter various failures. These include both temporary and permanent disruptions in sensing and communication, which can significantly degrade system robustness and performance if not explicitly modeled. Despite its practical importance, failure-aware coordination remains underexplored in the literature. To bridge the gap between idealized conditions and the complexities of real-world environments, we propose a unified failure-aware coordination framework designed to enable resilient and adaptive multi-robot target tracking under both temporary and permanent failure conditions. Our approach systematically distinguishes between two classes of failures: (1) probabilistic and temporary disruptions, where robots recover from intermittent sensing or communication losses by dynamically adapting paths and avoiding inferred danger zones, and (2) permanent failures, where robots lose sensing or communication capabilities irreversibly, requiring sustained, decentralized behavioral adaptation. To handle these scenarios, the robot team is partitioned into subgroups. Robots that remain connected form a communication group and collaboratively plan using partially centralized nonlinear optimization. Robots experiencing permanent disconnection or failure continue to operate independently through decentralized or individual optimization, allowing them to contribute to the task within their local context. We extensively evaluate our method across a range of benchmark variations and conduct a comprehensive assessment under diverse real-world failure scenarios. Results show that our framework consistently achieves robust performance in realistic environments with unknown danger zones, offering a practical and generalizable solution for the multi-robot systems community.
<div id='section'>Paperid: <span id='pid'>168, <a href='https://arxiv.org/pdf/2507.13677.pdf' target='_blank'>https://arxiv.org/pdf/2507.13677.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chuheng Wei, Ziye Qin, Walter Zimmer, Guoyuan Wu, Matthew J. Barth
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.13677">HeCoFuse: Cross-Modal Complementary V2X Cooperative Perception with Heterogeneous Sensors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real-world Vehicle-to-Everything (V2X) cooperative perception systems often operate under heterogeneous sensor configurations due to cost constraints and deployment variability across vehicles and infrastructure. This heterogeneity poses significant challenges for feature fusion and perception reliability. To address these issues, we propose HeCoFuse, a unified framework designed for cooperative perception across mixed sensor setups where nodes may carry Cameras (C), LiDARs (L), or both. By introducing a hierarchical fusion mechanism that adaptively weights features through a combination of channel-wise and spatial attention, HeCoFuse can tackle critical challenges such as cross-modality feature misalignment and imbalanced representation quality. In addition, an adaptive spatial resolution adjustment module is employed to balance computational cost and fusion effectiveness. To enhance robustness across different configurations, we further implement a cooperative learning strategy that dynamically adjusts fusion type based on available modalities. Experiments on the real-world TUMTraf-V2X dataset demonstrate that HeCoFuse achieves 43.22% 3D mAP under the full sensor configuration (LC+LC), outperforming the CoopDet3D baseline by 1.17%, and reaches an even higher 43.38% 3D mAP in the L+LC scenario, while maintaining 3D mAP in the range of 21.74% to 43.38% across nine heterogeneous sensor configurations. These results, validated by our first-place finish in the CVPR 2025 DriveX challenge, establish HeCoFuse as the current state-of-the-art on TUM-Traf V2X dataset while demonstrating robust performance across diverse sensor deployments.
<div id='section'>Paperid: <span id='pid'>169, <a href='https://arxiv.org/pdf/2505.06513.pdf' target='_blank'>https://arxiv.org/pdf/2505.06513.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peihan Li, Lifeng Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.06513">LLM-Flock: Decentralized Multi-Robot Flocking via Large Language Models and Influence-Based Consensus</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) have advanced rapidly in recent years, demonstrating strong capabilities in problem comprehension and reasoning. Inspired by these developments, researchers have begun exploring the use of LLMs as decentralized decision-makers for multi-robot formation control. However, prior studies reveal that directly applying LLMs to such tasks often leads to unstable and inconsistent behaviors, where robots may collapse to the centroid of their positions or diverge entirely due to hallucinated reasoning, logical inconsistencies, and limited coordination awareness. To overcome these limitations, we propose a novel framework that integrates LLMs with an influence-based plan consensus protocol. In this framework, each robot independently generates a local plan toward the desired formation using its own LLM. The robots then iteratively refine their plans through a decentralized consensus protocol that accounts for their influence on neighboring robots. This process drives the system toward a coherent and stable flocking formation in a fully decentralized manner. We evaluate our approach through comprehensive simulations involving both state-of-the-art closed-source LLMs (e.g., o3-mini, Claude 3.5) and open-source models (e.g., Llama3.1-405b, Qwen-Max, DeepSeek-R1). The results show notable improvements in stability, convergence, and adaptability over previous LLM-based methods. We further validate our framework on a physical team of Crazyflie drones, demonstrating its practical viability and effectiveness in real-world multi-robot systems.
<div id='section'>Paperid: <span id='pid'>170, <a href='https://arxiv.org/pdf/2504.15425.pdf' target='_blank'>https://arxiv.org/pdf/2504.15425.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Songyuan Zhang, Oswin So, Mitchell Black, Zachary Serlin, Chuchu Fan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.15425">Solving Multi-Agent Safe Optimal Control with Distributed Epigraph Form MARL</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tasks for multi-robot systems often require the robots to collaborate and complete a team goal while maintaining safety. This problem is usually formalized as a constrained Markov decision process (CMDP), which targets minimizing a global cost and bringing the mean of constraint violation below a user-defined threshold. Inspired by real-world robotic applications, we define safety as zero constraint violation. While many safe multi-agent reinforcement learning (MARL) algorithms have been proposed to solve CMDPs, these algorithms suffer from unstable training in this setting. To tackle this, we use the epigraph form for constrained optimization to improve training stability and prove that the centralized epigraph form problem can be solved in a distributed fashion by each agent. This results in a novel centralized training distributed execution MARL algorithm named Def-MARL. Simulation experiments on 8 different tasks across 2 different simulators show that Def-MARL achieves the best overall performance, satisfies safety constraints, and maintains stable training. Real-world hardware experiments on Crazyflie quadcopters demonstrate the ability of Def-MARL to safely coordinate agents to complete complex collaborative tasks compared to other methods.
<div id='section'>Paperid: <span id='pid'>171, <a href='https://arxiv.org/pdf/2502.03814.pdf' target='_blank'>https://arxiv.org/pdf/2502.03814.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peihan Li, Zijian An, Shams Abrar, Lifeng Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.03814">Large Language Models for Multi-Robot Systems: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid advancement of Large Language Models (LLMs) has opened new possibilities in Multi-Robot Systems (MRS), enabling enhanced communication, task planning, and human-robot interaction. Unlike traditional single-robot and multi-agent systems, MRS poses unique challenges, including coordination, scalability, and real-world adaptability. This survey provides the first comprehensive exploration of LLM integration into MRS. It systematically categorizes their applications across high-level task allocation, mid-level motion planning, low-level action generation, and human intervention. We highlight key applications in diverse domains, such as household robotics, construction, formation control, target tracking, and robot games, showcasing the versatility and transformative potential of LLMs in MRS. Furthermore, we examine the challenges that limit adapting LLMs in MRS, including mathematical reasoning limitations, hallucination, latency issues, and the need for robust benchmarking systems. Finally, we outline opportunities for future research, emphasizing advancements in fine-tuning, reasoning techniques, and task-specific models. This survey aims to guide researchers in the intelligence and real-world deployment of MRS powered by LLMs. Based on the fast-evolving nature of research in the field, we keep updating the papers in the open-source GitHub repository.
<div id='section'>Paperid: <span id='pid'>172, <a href='https://arxiv.org/pdf/2502.03814.pdf' target='_blank'>https://arxiv.org/pdf/2502.03814.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peihan Li, Zijian An, Shams Abrar, Lifeng Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.03814">Large Language Models for Multi-Robot Systems: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid advancement of Large Language Models (LLMs) has opened new possibilities in Multi-Robot Systems (MRS), enabling enhanced communication, task planning, and human-robot interaction. Unlike traditional single-robot and multi-agent systems, MRS poses unique challenges, including coordination, scalability, and real-world adaptability. This survey provides the first comprehensive exploration of LLM integration into MRS. It systematically categorizes their applications across high-level task allocation, mid-level motion planning, low-level action generation, and human intervention. We highlight key applications in diverse domains, such as household robotics, construction, formation control, target tracking, and robot games, showcasing the versatility and transformative potential of LLMs in MRS. Furthermore, we examine the challenges that limit adapting LLMs in MRS, including mathematical reasoning limitations, hallucination, latency issues, and the need for robust benchmarking systems. Finally, we outline opportunities for future research, emphasizing advancements in fine-tuning, reasoning techniques, and task-specific models. This survey aims to guide researchers in the intelligence and real-world deployment of MRS powered by LLMs. Based on the fast-evolving nature of research in the field, we keep updating the papers in the open-source GitHub repository.
<div id='section'>Paperid: <span id='pid'>173, <a href='https://arxiv.org/pdf/2404.04752.pdf' target='_blank'>https://arxiv.org/pdf/2404.04752.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peihan Li, Vishnu Menon, Bhavanaraj Gudiguntla, Daniel Ting, Lifeng Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.04752">Challenges Faced by Large Language Models in Solving Multi-Agent Flocking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Flocking is a behavior where multiple agents in a system attempt to stay close to each other while avoiding collision and maintaining a desired formation. This is observed in the natural world and has applications in robotics, including natural disaster search and rescue, wild animal tracking, and perimeter surveillance and patrol. Recently, large language models (LLMs) have displayed an impressive ability to solve various collaboration tasks as individual decision-makers. Solving multi-agent flocking with LLMs would demonstrate their usefulness in situations requiring spatial and decentralized decision-making. Yet, when LLM-powered agents are tasked with implementing multi-agent flocking, they fall short of the desired behavior. After extensive testing, we find that agents with LLMs as individual decision-makers typically opt to converge on the average of their initial positions or diverge from each other. After breaking the problem down, we discover that LLMs cannot understand maintaining a shape or keeping a distance in a meaningful way. Solving multi-agent flocking with LLMs would enhance their ability to understand collaborative spatial reasoning and lay a foundation for addressing more complex multi-agent tasks. This paper discusses the challenges LLMs face in multi-agent flocking and suggests areas for future improvement and research.
<div id='section'>Paperid: <span id='pid'>174, <a href='https://arxiv.org/pdf/2311.13714.pdf' target='_blank'>https://arxiv.org/pdf/2311.13714.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kunal Garg, Songyuan Zhang, Oswin So, Charles Dawson, Chuchu Fan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.13714">Learning Safe Control for Multi-Robot Systems: Methods, Verification, and Open Challenges</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this survey, we review the recent advances in control design methods for robotic multi-agent systems (MAS), focussing on learning-based methods with safety considerations. We start by reviewing various notions of safety and liveness properties, and modeling frameworks used for problem formulation of MAS. Then we provide a comprehensive review of learning-based methods for safe control design for multi-robot systems. We start with various types of shielding-based methods, such as safety certificates, predictive filters, and reachability tools. Then, we review the current state of control barrier certificate learning in both a centralized and distributed manner, followed by a comprehensive review of multi-agent reinforcement learning with a particular focus on safety. Next, we discuss the state-of-the-art verification tools for the correctness of learning-based methods. Based on the capabilities and the limitations of the state of the art methods in learning and verification for MAS, we identify various broad themes for open challenges: how to design methods that can achieve good performance along with safety guarantees; how to decompose single-agent based centralized methods for MAS; how to account for communication-related practical issues; and how to assess transfer of theoretical guarantees to practice.
<div id='section'>Paperid: <span id='pid'>175, <a href='https://arxiv.org/pdf/2309.17437.pdf' target='_blank'>https://arxiv.org/pdf/2309.17437.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Siji Chen, Yanshen Sun, Peihan Li, Lifeng Zhou, Chang-Tien Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.17437">Learning Decentralized Flocking Controllers with Spatio-Temporal Graph Neural Network</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently a line of researches has delved the use of graph neural networks (GNNs) for decentralized control in swarm robotics. However, it has been observed that relying solely on the states of immediate neighbors is insufficient to imitate a centralized control policy. To address this limitation, prior studies proposed incorporating $L$-hop delayed states into the computation. While this approach shows promise, it can lead to a lack of consensus among distant flock members and the formation of small clusters, consequently resulting in the failure of cohesive flocking behaviors. Instead, our approach leverages spatiotemporal GNN, named STGNN that encompasses both spatial and temporal expansions. The spatial expansion collects delayed states from distant neighbors, while the temporal expansion incorporates previous states from immediate neighbors. The broader and more comprehensive information gathered from both expansions results in more effective and accurate predictions. We develop an expert algorithm for controlling a swarm of robots and employ imitation learning to train our decentralized STGNN model based on the expert algorithm. We simulate the proposed STGNN approach in various settings, demonstrating its decentralized capacity to emulate the global expert algorithm. Further, we implemented our approach to achieve cohesive flocking, leader following and obstacle avoidance by a group of Crazyflie drones. The performance of STGNN underscores its potential as an effective and reliable approach for achieving cohesive flocking, leader following and obstacle avoidance tasks.
<div id='section'>Paperid: <span id='pid'>176, <a href='https://arxiv.org/pdf/2309.10311.pdf' target='_blank'>https://arxiv.org/pdf/2309.10311.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianyi Ding, Ronghao Zheng, Senlin Zhang, Meiqin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.10311">Resource-Efficient Cooperative Online Scalar Field Mapping via Distributed Sparse Gaussian Process Regression</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cooperative online scalar field mapping is an important task for multi-robot systems. Gaussian process regression is widely used to construct a map that represents spatial information with confidence intervals. However, it is difficult to handle cooperative online mapping tasks because of its high computation and communication costs. This letter proposes a resource-efficient cooperative online field mapping method via distributed sparse Gaussian process regression. A novel distributed online Gaussian process evaluation method is developed such that robots can cooperatively evaluate and find observations of sufficient global utility to reduce computation. The bounded errors of distributed aggregation results are guaranteed theoretically, and the performances of the proposed algorithms are validated by real online light field mapping experiments.
<div id='section'>Paperid: <span id='pid'>177, <a href='https://arxiv.org/pdf/2303.03583.pdf' target='_blank'>https://arxiv.org/pdf/2303.03583.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Siqi Fan, Zhe Wang, Xiaoliang Huo, Yan Wang, Jingjing Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.03583">Calibration-free BEV Representation for Infrastructure Perception</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Effective BEV object detection on infrastructure can greatly improve traffic scenes understanding and vehicle-toinfrastructure (V2I) cooperative perception. However, cameras installed on infrastructure have various postures, and previous BEV detection methods rely on accurate calibration, which is difficult for practical applications due to inevitable natural factors (e.g., wind and snow). In this paper, we propose a Calibration-free BEV Representation (CBR) network, which achieves 3D detection based on BEV representation without calibration parameters and additional depth supervision. Specifically, we utilize two multi-layer perceptrons for decoupling the features from perspective view to front view and birdeye view under boxes-induced foreground supervision. Then, a cross-view feature fusion module matches features from orthogonal views according to similarity and conducts BEV feature enhancement with front view features. Experimental results on DAIR-V2X demonstrate that CBR achieves acceptable performance without any camera parameters and is naturally not affected by calibration noises. We hope CBR can serve as a baseline for future research addressing practical challenges of infrastructure perception.
<div id='section'>Paperid: <span id='pid'>178, <a href='https://arxiv.org/pdf/2302.03128.pdf' target='_blank'>https://arxiv.org/pdf/2302.03128.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhengwei Bai, Guoyuan Wu, Matthew J. Barth, Yongkang Liu, Emrah Akin Sisbot, Kentaro Oguchi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.03128">Cooperverse: A Mobile-Edge-Cloud Framework for Universal Cooperative Perception with Mixed Connectivity and Automation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cooperative perception (CP) is attracting increasing attention and is regarded as the core foundation to support cooperative driving automation, a potential key solution to addressing the safety, mobility, and sustainability issues of contemporary transportation systems. However, current research on CP is still at the beginning stages where a systematic problem formulation of CP is still missing, acting as the essential guideline of the system design of a CP system under real-world situations. In this paper, we formulate a universal CP system into an optimization problem and a mobile-edge-cloud framework called Cooperverse. This system addresses CP in a mixed connectivity and automation environment. A Dynamic Feature Sharing (DFS) methodology is introduced to support this CP system under certain constraints and a Random Priority Filtering (RPF) method is proposed to conduct DFS with high performance. Experiments have been conducted based on a high-fidelity CP platform, and the results show that the Cooperverse framework is effective for dynamic node engagement and the proposed DFS methodology can improve system CP performance by 14.5% and the RPF method can reduce the communication cost for mobile nodes by 90% with only 1.7% drop for average precision.
<div id='section'>Paperid: <span id='pid'>179, <a href='https://arxiv.org/pdf/2212.07060.pdf' target='_blank'>https://arxiv.org/pdf/2212.07060.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhengwei Bai, Guoyuan Wu, Matthew J. Barth, Yongkang Liu, Emrah Akin Sisbot, Kentaro Oguchi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.07060">VINet: Lightweight, Scalable, and Heterogeneous Cooperative Perception for 3D Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Utilizing the latest advances in Artificial Intelligence (AI), the computer vision community is now witnessing an unprecedented evolution in all kinds of perception tasks, particularly in object detection. Based on multiple spatially separated perception nodes, Cooperative Perception (CP) has emerged to significantly advance the perception of automated driving. However, current cooperative object detection methods mainly focus on ego-vehicle efficiency without considering the practical issues of system-wide costs. In this paper, we introduce VINet, a unified deep learning-based CP network for scalable, lightweight, and heterogeneous cooperative 3D object detection. VINet is the first CP method designed from the standpoint of large-scale system-level implementation and can be divided into three main phases: 1) Global Pre-Processing and Lightweight Feature Extraction which prepare the data into global style and extract features for cooperation in a lightweight manner; 2) Two-Stream Fusion which fuses the features from scalable and heterogeneous perception nodes; and 3) Central Feature Backbone and 3D Detection Head which further process the fused features and generate cooperative detection results. An open-source data experimental platform is designed and developed for CP dataset acquisition and model evaluation. The experimental analysis shows that VINet can reduce 84% system-level computational cost and 94% system-level communication cost while improving the 3D detection accuracy.
<div id='section'>Paperid: <span id='pid'>180, <a href='https://arxiv.org/pdf/2203.06319.pdf' target='_blank'>https://arxiv.org/pdf/2203.06319.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhengwei Bai, Guoyuan Wu, Matthew J. Barth, Yongkang Liu, Emrah Akin Sisbot, Kentaro Oguchi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2203.06319">PillarGrid: Deep Learning-based Cooperative Perception for 3D Object Detection from Onboard-Roadside LiDAR</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D object detection plays a fundamental role in enabling autonomous driving, which is regarded as the significant key to unlocking the bottleneck of contemporary transportation systems from the perspectives of safety, mobility, and sustainability. Most of the state-of-the-art (SOTA) object detection methods from point clouds are developed based on a single onboard LiDAR, whose performance will be inevitably limited by the range and occlusion, especially in dense traffic scenarios. In this paper, we propose \textit{PillarGrid}, a novel cooperative perception method fusing information from multiple 3D LiDARs (both on-board and roadside), to enhance the situation awareness for connected and automated vehicles (CAVs). PillarGrid consists of four main phases: 1) cooperative preprocessing of point clouds, 2) pillar-wise voxelization and feature extraction, 3) grid-wise deep fusion of features from multiple sensors, and 4) convolutional neural network (CNN)-based augmented 3D object detection. A novel cooperative perception platform is developed for model training and testing. Extensive experimentation shows that PillarGrid outperforms the SOTA single-LiDAR-based 3D object detection methods with respect to both accuracy and range by a large margin.
<div id='section'>Paperid: <span id='pid'>181, <a href='https://arxiv.org/pdf/2509.04383.pdf' target='_blank'>https://arxiv.org/pdf/2509.04383.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Serafino Cicerone, Alessia Di Fonso, Gabriele Di Stefano, Alfredo Navarra
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.04383">On the impact of unlimited computational power in OBLOT: consequences for synchronous robots on graphs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The OBLOT model has been extensively studied in theoretical swarm robotics. It assumes weak capabilities for the involved mobile robots, such as they are anonymous, disoriented, no memory of past events (oblivious), and silent. Their only means of (implicit) communication is transferred to their positioning, i.e., stigmergic information. These limited capabilities make the design of distributed algorithms a challenging task. Over the last two decades, numerous research papers have addressed the question of which tasks can be accomplished within this model. Nevertheless, as it usually happens in distributed computing, also in OBLOT the computational power available to the robots is neglected as the main cost measures for the designed algorithms refer to the number of movements or the number of rounds required. In this paper, we prove that for synchronous robots moving on finite graphs, the unlimited computational power (other than finite time) has a significant impact. In fact, by exploiting it, we provide a definitive resolution algorithm that applies to a wide class of problems while guaranteeing the minimum number of moves and rounds.
<div id='section'>Paperid: <span id='pid'>182, <a href='https://arxiv.org/pdf/2509.04383.pdf' target='_blank'>https://arxiv.org/pdf/2509.04383.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Serafino Cicerone, Alessia Di Fonso, Gabriele Di Stefano, Alfredo Navarra
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.04383">On the impact of unlimited computational power in OBLOT: consequences for synchronous robots on graphs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The OBLOT model has been extensively studied in theoretical swarm robotics. It assumes weak capabilities for the involved mobile robots, such as they are anonymous, disoriented, no memory of past events (oblivious), and silent. Their only means of (implicit) communication is transferred to their positioning, i.e., stigmergic information. These limited capabilities make the design of distributed algorithms a challenging task. Over the last two decades, numerous research papers have addressed the question of which tasks can be accomplished within this model. Nevertheless, as it usually happens in distributed computing, also in OBLOT the computational power available to the robots is neglected as the main cost measures for the designed algorithms refer to the number of movements or the number of rounds required. In this paper, we prove that for synchronous robots moving on finite graphs, the unlimited computational power (other than finite time) has a significant impact. In fact, by exploiting it, we provide a definitive resolution algorithm that applies to a wide class of problems while guaranteeing the minimum number of moves and rounds.
<div id='section'>Paperid: <span id='pid'>183, <a href='https://arxiv.org/pdf/2503.00606.pdf' target='_blank'>https://arxiv.org/pdf/2503.00606.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jihao Huang, Jun Zeng, Xuemin Chi, Koushil Sreenath, Zhitao Liu, Hongye Su
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.00606">Dynamic Collision Avoidance Using Velocity Obstacle-Based Control Barrier Functions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Designing safety-critical controllers for acceleration-controlled unicycle robots is challenging, as control inputs may not appear in the constraints of control Lyapunov functions(CLFs) and control barrier functions (CBFs), leading to invalid controllers. Existing methods often rely on state-feedback-based CLFs and high-order CBFs (HOCBFs), which are computationally expensive to construct and fail to maintain effectiveness in dynamic environments with fast-moving, nearby obstacles. To address these challenges, we propose constructing velocity obstacle-based CBFs (VOCBFs) in the velocity space to enhance dynamic collision avoidance capabilities, instead of relying on distance-based CBFs that require the introduction of HOCBFs. Additionally, by extending VOCBFs using variants of VO, we enable reactive collision avoidance between robots. We formulate a safety-critical controller for acceleration-controlled unicycle robots as a mixed-integer quadratic programming (MIQP), integrating state-feedback-based CLFs for navigation and VOCBFs for collision avoidance. To enhance the efficiency of solving the MIQP, we split the MIQP into multiple sub-optimization problems and employ a decision network to reduce computational costs. Numerical simulations demonstrate that our approach effectively guides the robot to its target while avoiding collisions. Compared to HOCBFs, VOCBFs exhibit significantly improved dynamic obstacle avoidance performance, especially when obstacles are fast-moving and close to the robot. Furthermore, we extend our method to distributed multi-robot systems.
<div id='section'>Paperid: <span id='pid'>184, <a href='https://arxiv.org/pdf/2412.10087.pdf' target='_blank'>https://arxiv.org/pdf/2412.10087.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuekai Qiu, Pengming Zhu, Yiming Hu, Zhiwen Zeng, Huimin Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.10087">Consensus-Based Dynamic Task Allocation for Multi-Robot System Considering Payloads Consumption</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a consensus-based payload algorithm (CBPA) to deal with the condition of robots' capability decrease for multi-robot task allocation. During the execution of complex tasks, robots' capabilities could decrease with the consumption of payloads, which causes a problem that the robot coalition would not meet the tasks' requirements in real time. The proposed CBPA is an enhanced version of the consensus-based bundle algorithm (CBBA) and comprises two primary core phases: the payload bundle construction and consensus phases. In the payload bundle construction phase, CBPA introduces a payload assignment matrix to track the payloads carried by the robots and the demands of multi-robot tasks in real time. Then, robots share their respective payload assignment matrix in the consensus phase. These two phases are iterated to dynamically adjust the number of robots performing multi-robot tasks and the number of tasks each robot performs and obtain conflict-free results to ensure that the robot coalition meets the demand and completes all tasks as quickly as possible. Physical experiment shows that CBPA is appropriate in complex and dynamic scenarios where robots need to collaborate and task requirements are tightly coupled to the robots' payloads. Numerical experiments show that CBPA has higher total task gains than CBBA.
<div id='section'>Paperid: <span id='pid'>185, <a href='https://arxiv.org/pdf/2412.09117.pdf' target='_blank'>https://arxiv.org/pdf/2412.09117.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wanli Ni, Ruyu Luo, Xinran Zhang, Peng Wang, Wen Wang, Hui Tian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.09117">Reconfigurable Intelligent Surface for Internet of Robotic Things</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid development of artificial intelligence, robotics, and Internet of Things, multi-robot systems are progressively acquiring human-like environmental perception and understanding capabilities, empowering them to complete complex tasks through autonomous decision-making and interaction. However, the Internet of Robotic Things (IoRT) faces significant challenges in terms of spectrum resources, sensing accuracy, communication latency, and energy supply. To address these issues, a reconfigurable intelligent surface (RIS)-aided IoRT network is proposed to enhance the overall performance of robotic communication, sensing, computation, and energy harvesting. In the case studies, by jointly optimizing parameters such as transceiver beamforming, robot trajectories, and RIS coefficients, solutions based on multi-agent deep reinforcement learning and multi-objective optimization are proposed to solve problems such as beamforming design, path planning, target sensing, and data aggregation. Numerical results are provided to demonstrate the effectiveness of proposed solutions in improve communication quality, sensing accuracy, computation error, and energy efficiency of RIS-aided IoRT networks.
<div id='section'>Paperid: <span id='pid'>186, <a href='https://arxiv.org/pdf/2411.16911.pdf' target='_blank'>https://arxiv.org/pdf/2411.16911.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuhao Qi, Zengjie Zhang, Zhiyong Sun, Sofie Haesaert
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.16911">Avoiding Deadlocks Is Not Enough: Analysis and Resolution of Blocked Airplanes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper is devoted to the analysis and resolution of a pathological phenomenon in airplane encounters called blocking mode. As autonomy in airplane systems increases, a pathological phenomenon can be observed in two-aircraft encounter scenarios, where airplanes stick together and fly in parallel for an extended period. This parallel flight results in a temporary blocking that significantly delays progress. In contrast to widely studied deadlocks in multi-robot systems, such transient blocking is often overlooked in existing literature. Since such prolonged parallel flying places high-speed airplanes at elevated risks of near-miss collisions, encounter conflicts must be resolved as quickly as possible in the context of aviation. We develop a mathematical model for a two-airplane encounter system that replicates this blocking phenomenon. Using this model, we analyze the conditions under which blocking occurs, quantify the duration of the blocking period, and demonstrate that the blocking condition is significantly less restrictive than that of deadlock. Based on these analytical insights, we propose an intention-aware strategy with an adaptive priority mechanism that enables efficient resolution of ongoing blocking phenomena while also incidentally eliminating deadlocks. Notably, the developed strategy does not rely on central coordination and communications that can be unreliable in harsh situations. The analytical findings and the proposed resolution strategy are validated through extensive simulations.
<div id='section'>Paperid: <span id='pid'>187, <a href='https://arxiv.org/pdf/2409.19829.pdf' target='_blank'>https://arxiv.org/pdf/2409.19829.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shreyas Muthusamy, Damian Owerko, Charilaos I. Kanatsoulis, Saurav Agarwal, Alejandro Ribeiro
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.19829">Generalizability of Graph Neural Networks for Decentralized Unlabeled Motion Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unlabeled motion planning involves assigning a set of robots to target locations while ensuring collision avoidance, aiming to minimize the total distance traveled. The problem forms an essential building block for multi-robot systems in applications such as exploration, surveillance, and transportation. We address this problem in a decentralized setting where each robot knows only the positions of its $k$-nearest robots and $k$-nearest targets. This scenario combines elements of combinatorial assignment and continuous-space motion planning, posing significant scalability challenges for traditional centralized approaches. To overcome these challenges, we propose a decentralized policy learned via a Graph Neural Network (GNN). The GNN enables robots to determine (1) what information to communicate to neighbors and (2) how to integrate received information with local observations for decision-making. We train the GNN using imitation learning with the centralized Hungarian algorithm as the expert policy, and further fine-tune it using reinforcement learning to avoid collisions and enhance performance. Extensive empirical evaluations demonstrate the scalability and effectiveness of our approach. The GNN policy trained on 100 robots generalizes to scenarios with up to 500 robots, outperforming state-of-the-art solutions by 8.6\% on average and significantly surpassing greedy decentralized methods. This work lays the foundation for solving multi-robot coordination problems in settings where scalability is important.
<div id='section'>Paperid: <span id='pid'>188, <a href='https://arxiv.org/pdf/2401.00212.pdf' target='_blank'>https://arxiv.org/pdf/2401.00212.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Eduardo Sebastian, Thai Duong, Nikolay Atanasov, Eduardo Montijano, Carlos Sagues
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.00212">Physics-Informed Multi-Agent Reinforcement Learning for Distributed Multi-Robot Problems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The networked nature of multi-robot systems presents challenges in the context of multi-agent reinforcement learning. Centralized control policies do not scale with increasing numbers of robots, whereas independent control policies do not exploit the information provided by other robots, exhibiting poor performance in cooperative-competitive tasks. In this work we propose a physics-informed reinforcement learning approach able to learn distributed multi-robot control policies that are both scalable and make use of all the available information to each robot. Our approach has three key characteristics. First, it imposes a port-Hamiltonian structure on the policy representation, respecting energy conservation properties of physical robot systems and the networked nature of robot team interactions. Second, it uses self-attention to ensure a sparse policy representation able to handle time-varying information at each robot from the interaction graph. Third, we present a soft actor-critic reinforcement learning algorithm parameterized by our self-attention port-Hamiltonian control policy, which accounts for the correlation among robots during training while overcoming the need of value function factorization. Extensive simulations in different multi-robot scenarios demonstrate the success of the proposed approach, surpassing previous multi-robot reinforcement learning solutions in scalability, while achieving similar or superior performance (with averaged cumulative reward up to x2 greater than the state-of-the-art with robot teams x6 larger than the number of robots at training time). We also validate our approach on multiple real robots in the Georgia Tech Robotarium under imperfect communication, demonstrating zero-shot sim-to-real transfer and scalability across number of robots.
<div id='section'>Paperid: <span id='pid'>189, <a href='https://arxiv.org/pdf/2309.11190.pdf' target='_blank'>https://arxiv.org/pdf/2309.11190.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Avisek Sharma, Satakshi Ghosh, Pritam Goswami, Buddhadeb Sau
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.11190">Space and Move-optimal Arbitrary Pattern Formation on Infinite Rectangular Grid by Oblivious Robot Swarm</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Arbitrary Pattern Formation (APF) is a fundamental coordination problem in swarm robotics. It requires a set of autonomous robots (mobile computing units) to form an arbitrary pattern (given as input) starting from any initial pattern. This problem has been extensively investigated in continuous and discrete scenarios, with this study focusing on the discrete variant. A set of robots is placed on the nodes of an infinite rectangular grid graph embedded in the euclidean plane. The movements of each robot is restricted to one of the four neighboring grid nodes from its current position. The robots are autonomous, anonymous, identical, and homogeneous, and operate Look-Compute-Move cycles. In this work, we adopt the classical $\mathcal{OBLOT}$ robot model, meaning the robots have no persistent memory or explicit communication methods, yet they possess full and unobstructed visibility. This work proposes an algorithm that solves the APF problem in a fully asynchronous scheduler assuming the initial configuration is asymmetric. The considered performance measures of the algorithm are space and number of moves required for the robots. The algorithm is asymptotically move-optimal. Here, we provide a definition of space complexity that takes the visibility issue into consideration. We observe an obvious lower bound $\mathcal{D}$ of the space complexity and show that the proposed algorithm has the space complexity $\mathcal{D}+4$. On comparing with previous related works, we show that this is the first proposed algorithm considering $\mathcal{OBLOT}$ robot model that is asymptotically move-optimal and has the least space complexity which is almost optimal.
<div id='section'>Paperid: <span id='pid'>190, <a href='https://arxiv.org/pdf/2308.16482.pdf' target='_blank'>https://arxiv.org/pdf/2308.16482.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Salma Salimi, Farhad Keramat, Tomi Westerlund, Jorge PeÃ±a Queralta
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.16482">A Customizable Conflict Resolution and Attribute-Based Access Control Framework for Multi-Robot Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As multi-robot systems continue to advance and become integral to various applications, managing conflicts and ensuring secure access control are critical challenges that need to be addressed. Access control is essential in multi-robot systems to ensure secure and authorized interactions among robots, protect sensitive data, and prevent unauthorized access to resources. This paper presents a novel framework for customizable conflict resolution and attribute-based access control in multi-robot systems for ROS 2 leveraging the Hyperledger Fabric blockchain. We introduce an attribute-based access control (ABAC) Fabric-ROS 2 bridge to enable secure communication and control between users and robots. By defining conflict resolution policies based on task priorities, robot capabilities, and user-defined constraints, our framework offers a flexible way to resolve conflicts. Additionally, it incorporates attribute-based access control, granting access rights based on user and robot attributes. ABAC offers a modular approach to control access compared to existing access control approaches in ROS 2, such as SROS2. Through this framework, multi-robot systems can be managed efficiently, securely, and adaptably, ensuring controlled access to resources and managing conflicts. Our experimental evaluation shows that our framework marginally improves latency and throughput over exiting Fabric and ROS 2 integration solutions. At higher network load, it is the only solution to operate reliably without a diverging transaction commitment latency. We also demonstrate how conflicts arising from simultaneous control or a robot by two users are resolved in real-time and motion distortion is effectively eliminated.
<div id='section'>Paperid: <span id='pid'>191, <a href='https://arxiv.org/pdf/2306.17157.pdf' target='_blank'>https://arxiv.org/pdf/2306.17157.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaiyuan Chen, Ryan Hoque, Karthik Dharmarajan, Edith LLontop, Simeon Adebola, Jeffrey Ichnowski, John Kubiatowicz, Ken Goldberg
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.17157">FogROS2-SGC: A ROS2 Cloud Robotics Platform for Secure Global Connectivity</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Robot Operating System (ROS2) is the most widely used software platform for building robotics applications. FogROS2 extends ROS2 to allow robots to access cloud computing on demand. However, ROS2 and FogROS2 assume that all robots are locally connected and that each robot has full access and control of the other robots. With applications like distributed multi-robot systems, remote robot control, and mobile robots, robotics increasingly involves the global Internet and complex trust management. Existing approaches for connecting disjoint ROS2 networks lack key features such as security, compatibility, efficiency, and ease of use. We introduce FogROS2-SGC, an extension of FogROS2 that can effectively connect robot systems across different physical locations, networks, and Data Distribution Services (DDS). With globally unique and location-independent identifiers, FogROS2-SGC securely and efficiently routes data between robotics components around the globe. FogROS2-SGC is agnostic to the ROS2 distribution and configuration, is compatible with non-ROS2 software, and seamlessly extends existing ROS2 applications without any code modification. Experiments suggest FogROS2-SGC is 19x faster than rosbridge (a ROS2 package with comparable features, but lacking security). We also apply FogROS2-SGC to 4 robots and compute nodes that are 3600km apart. Videos and code are available on the project website https://sites.google.com/view/fogros2-sgc.
<div id='section'>Paperid: <span id='pid'>192, <a href='https://arxiv.org/pdf/2304.07954.pdf' target='_blank'>https://arxiv.org/pdf/2304.07954.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jihao Huang, Jun Zeng, Xuemin Chi, Koushil Sreenath, Zhitao Liu, Hongye Su
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.07954">Velocity Obstacle for Polytopic Collision Avoidance for Distributed Multi-robot Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Obstacle avoidance for multi-robot navigation with polytopic shapes is challenging. Existing works simplify the system dynamics or consider it as a convex or non-convex optimization problem with positive distance constraints between robots, which limits real-time performance and scalability. Additionally, generating collision-free behavior for polytopic-shaped robots is harder due to implicit and non-differentiable distance functions between polytopes. In this paper, we extend the concept of velocity obstacle (VO) principle for polytopic-shaped robots and propose a novel approach to construct the VO in the function of vertex coordinates and other robot's states. Compared with existing work about obstacle avoidance between polytopic-shaped robots, our approach is much more computationally efficient as the proposed approach for construction of VO between polytopes is optimization-free. Based on VO representation for polytopic shapes, we later propose a navigation approach for distributed multi-robot systems. We validate our proposed VO representation and navigation approach in multiple challenging scenarios including large-scale randomized tests, and our approach outperforms the state of art in many evaluation metrics, including completion rate, deadlock rate, and the average travel distance.
<div id='section'>Paperid: <span id='pid'>193, <a href='https://arxiv.org/pdf/2304.00780.pdf' target='_blank'>https://arxiv.org/pdf/2304.00780.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Iacopo Catalano, Jorge PeÃ±a Queralta, Tomi Westerlund
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.00780">Evaluating the Performance of Multi-Scan Integration for UAV LiDAR-based Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Drones have become essential tools in a wide range of industries, including agriculture, surveying, and transportation. However, tracking unmanned aerial vehicles (UAVs) in challenging environments, such cluttered or GNSS-denied environments, remains a critical issue. Additionally, UAVs are being deployed as part of multi-robot systems, where tracking their position can be essential for relative state estimation. In this paper, we evaluate the performance of a multi-scan integration method for tracking UAVs in GNSS-denied environments using a solid-state LiDAR and a Kalman Filter (KF). We evaluate the algorithm's ability to track a UAV in a large open area at various distances and speeds. Our quantitative analysis shows that while "tracking by detection" using a constant velocity model is the only method that consistently tracks the target, integrating multiple scan frequencies using a KF achieves lower position errors and represents a viable option for tracking UAVs in similar scenarios.
<div id='section'>Paperid: <span id='pid'>194, <a href='https://arxiv.org/pdf/2304.00275.pdf' target='_blank'>https://arxiv.org/pdf/2304.00275.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuhao Qi, Zengjie Zhang, Sofie Haesaert, Zhiyong Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.00275">Automated Formation Control Synthesis from Temporal Logic Specifications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In many practical scenarios, multi-robot systems are envisioned to support humans in executing complicated tasks within structured environments, such as search-and-rescue tasks. We propose a framework for a multi-robot swarm to fulfill complex tasks represented by temporal logic specifications. Given temporal logic specifications on the swarm formation and navigation, we develop a controller with runtime safety and convergence guarantees that drive the swarm to formally satisfy the specification. In addition, the synthesized controller will autonomously switch formations as necessary and react to uncontrollable events from the environment. The efficacy of the proposed framework is validated with a simulation study on the navigation of multiple quadrotor robots.
<div id='section'>Paperid: <span id='pid'>195, <a href='https://arxiv.org/pdf/2212.02876.pdf' target='_blank'>https://arxiv.org/pdf/2212.02876.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Avisek Sharma, Satakshi Ghosh, Pritam Goswami, Buddhadeb Sau
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.02876">Space and Move-optimal Arbitrary Pattern Formation on a Rectangular Grid by Robot Swarms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Arbitrary pattern formation (\textsc{Apf}) is a well-studied problem in swarm robotics. To the best of our knowledge, the problem has been considered in two different settings: one in a euclidean plane and another in an infinite grid. This work deals with the problem in an infinite rectangular grid setting. The previous works in literature dealing with the \textsc{Apf} problem in an infinite grid had a fundamental issue. These deterministic algorithms use a lot of space in the grid to solve the problem, mainly to maintain the asymmetry of the configuration or to avoid a collision. These solution techniques cannot be useful if there is a space constraint in the application field. In this work, we consider luminous robots (with one light that can take three colors) to avoid symmetry, but we carefully designed a deterministic algorithm that solves the \textsc{Apf} problem using the minimal required space in the grid. The robots are autonomous, identical, and anonymous, and they operate in Look-Compute-Move cycles under a fully asynchronous scheduler. The \textsc{Apf} algorithm proposed in \cite{BOSE2020} by Bose et al. can be modified using luminous robots so that it uses minimal space, but that algorithm is not move-optimal. The algorithm proposed in this paper not only uses minimal space but is also asymptotically move-optimal. The algorithm proposed in this work is designed for an infinite rectangular grid, but it can be easily modified to work on a finite grid as well.
<div id='section'>Paperid: <span id='pid'>196, <a href='https://arxiv.org/pdf/2204.04176.pdf' target='_blank'>https://arxiv.org/pdf/2204.04176.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Austin K. Chen, Bryce L. Ferguson, Daigo Shishika, Michael Dorothy, Jason R. Marden, George J. Pappas, Vijay Kumar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2204.04176">Path Defense in Dynamic Defender-Attacker Blotto Games (dDAB) with Limited Information</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We consider a path guarding problem in dynamic Defender-Attacker Blotto games (dDAB), where a team of robots must defend a path in a graph against adversarial agents. Multi-robot systems are particularly well suited to this application, as recent work has shown the effectiveness of these systems in related areas such as perimeter defense and surveillance. When designing a defender policy that guarantees the defense of a path, information about the adversary and the environment can be helpful and may reduce the number of resources required by the defender to achieve a sufficient level of security. In this work, we characterize the necessary and sufficient number of assets needed to guarantee the defense of a shortest path between two nodes in dDAB games when the defender can only detect assets within $k$-hops of a shortest path. By characterizing the relationship between sensing horizon and required resources, we show that increasing the sensing capability of the defender greatly reduces the number of defender assets needed to defend the path.
<div id='section'>Paperid: <span id='pid'>197, <a href='https://arxiv.org/pdf/2507.11464.pdf' target='_blank'>https://arxiv.org/pdf/2507.11464.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ajay Shankar, Keisuke Okumura, Amanda Prorok
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.11464">LF: Online Multi-Robot Path Planning Meets Optimal Trajectory Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a multi-robot control paradigm to solve point-to-point navigation tasks for a team of holonomic robots with access to the full environment information. The framework invokes two processes asynchronously at high frequency: (i) a centralized, discrete, and full-horizon planner for computing collision- and deadlock-free paths rapidly, leveraging recent advances in multi-agent pathfinding (MAPF), and (ii) dynamics-aware, robot-wise optimal trajectory controllers that ensure all robots independently follow their assigned paths reliably. This hierarchical shift in planning representation from (i) discrete and coupled to (ii) continuous and decoupled domains enables the framework to maintain long-term scalable motion synthesis. As an instantiation of this idea, we present LF, which combines a fast state-of-the-art MAPF solver (LaCAM), and a robust feedback control stack (Freyja) for executing agile robot maneuvers. LF provides a robust and versatile mechanism for lifelong multi-robot navigation even under asynchronous and partial goal updates, and adapts to dynamic workspaces simply by quick replanning. We present various multirotor and ground robot demonstrations, including the deployment of 15 real multirotors with random, consecutive target updates while a person walks through the operational workspace.
<div id='section'>Paperid: <span id='pid'>198, <a href='https://arxiv.org/pdf/2504.08240.pdf' target='_blank'>https://arxiv.org/pdf/2504.08240.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhaoliang Zheng, Yun Zhang, Zongling Meng, Johnson Liu, Xin Xia, Jiaqi Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.08240">InSPE: Rapid Evaluation of Heterogeneous Multi-Modal Infrastructure Sensor Placement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Infrastructure sensing is vital for traffic monitoring at safety hotspots (e.g., intersections) and serves as the backbone of cooperative perception in autonomous driving. While vehicle sensing has been extensively studied, infrastructure sensing has received little attention, especially given the unique challenges of diverse intersection geometries, complex occlusions, varying traffic conditions, and ambient environments like lighting and weather. To address these issues and ensure cost-effective sensor placement, we propose Heterogeneous Multi-Modal Infrastructure Sensor Placement Evaluation (InSPE), a perception surrogate metric set that rapidly assesses perception effectiveness across diverse infrastructure and environmental scenarios with combinations of multi-modal sensors. InSPE systematically evaluates perception capabilities by integrating three carefully designed metrics, i.e., sensor coverage, perception occlusion, and information gain. To support large-scale evaluation, we develop a data generation tool within the CARLA simulator and also introduce Infra-Set, a dataset covering diverse intersection types and environmental conditions. Benchmarking experiments with state-of-the-art perception algorithms demonstrate that InSPE enables efficient and scalable sensor placement analysis, providing a robust solution for optimizing intelligent intersection infrastructure.
<div id='section'>Paperid: <span id='pid'>199, <a href='https://arxiv.org/pdf/2503.19391.pdf' target='_blank'>https://arxiv.org/pdf/2503.19391.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiying Song, Lei Yang, Fuxi Wen, Jun Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.19391">TraF-Align: Trajectory-aware Feature Alignment for Asynchronous Multi-agent Perception</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cooperative perception presents significant potential for enhancing the sensing capabilities of individual vehicles, however, inter-agent latency remains a critical challenge. Latencies cause misalignments in both spatial and semantic features, complicating the fusion of real-time observations from the ego vehicle with delayed data from others. To address these issues, we propose TraF-Align, a novel framework that learns the flow path of features by predicting the feature-level trajectory of objects from past observations up to the ego vehicle's current time. By generating temporally ordered sampling points along these paths, TraF-Align directs attention from the current-time query to relevant historical features along each trajectory, supporting the reconstruction of current-time features and promoting semantic interaction across multiple frames. This approach corrects spatial misalignment and ensures semantic consistency across agents, effectively compensating for motion and achieving coherent feature fusion. Experiments on two real-world datasets, V2V4Real and DAIR-V2X-Seq, show that TraF-Align sets a new benchmark for asynchronous cooperative perception.
<div id='section'>Paperid: <span id='pid'>200, <a href='https://arxiv.org/pdf/2503.11048.pdf' target='_blank'>https://arxiv.org/pdf/2503.11048.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lingpeng Chen, Siva Kailas, Srujan Deolasee, Wenhao Luo, Katia Sycara, Woojun Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.11048">Distributed Multi-robot Source Seeking in Unknown Environments with Unknown Number of Sources</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a novel distributed source seeking framework, DIAS, designed for multi-robot systems in scenarios where the number of sources is unknown and potentially exceeds the number of robots. Traditional robotic source seeking methods typically focused on directing each robot to a specific strong source and may fall short in comprehensively identifying all potential sources. DIAS addresses this gap by introducing a hybrid controller that identifies the presence of sources and then alternates between exploration for data gathering and exploitation for guiding robots to identified sources. It further enhances search efficiency by dividing the environment into Voronoi cells and approximating source density functions based on Gaussian process regression. Additionally, DIAS can be integrated with existing source seeking algorithms. We compare DIAS with existing algorithms, including DoSS and GMES in simulated gas leakage scenarios where the number of sources outnumbers or is equal to the number of robots. The numerical results show that DIAS outperforms the baseline methods in both the efficiency of source identification by the robots and the accuracy of the estimated environmental density function.
<div id='section'>Paperid: <span id='pid'>201, <a href='https://arxiv.org/pdf/2503.02256.pdf' target='_blank'>https://arxiv.org/pdf/2503.02256.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kenta Tsukahara, Kanji Tanaka, Daiki Iwata, Jonathan Tay Yu Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.02256">Continual Multi-Robot Learning from Black-Box Visual Place Recognition Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the context of visual place recognition (VPR), continual learning (CL) techniques offer significant potential for avoiding catastrophic forgetting when learning new places. However, existing CL methods often focus on knowledge transfer from a known model to a new one, overlooking the existence of unknown black-box models. We explore a novel multi-robot CL approach that enables knowledge transfer from black-box VPR models (teachers), such as those of local robots encountered by traveler robots (students) in unknown environments. Specifically, we introduce Membership Inference Attack, or MIA, the only major privacy attack applicable to black-box models, and leverage it to reconstruct pseudo training sets, which serve as the key knowledge to be exchanged between robots, from black-box VPR models. Furthermore, we aim to overcome the inherently low sampling efficiency of MIA by leveraging insights on place class prediction distribution and un-learned class detection imported from the VPR literature as a prior distribution. We also analyze both the individual effects of these methods and their combined impact. Experimental results demonstrate that our black-box MIA (BB-MIA) approach is remarkably powerful despite its simplicity, significantly enhancing the VPR capability of lower-performing robots through brief communication with other robots. This study contributes to optimizing knowledge sharing between robots in VPR and enhancing autonomy in open-world environments with multi-robot systems that are fault-tolerant and scalable.
<div id='section'>Paperid: <span id='pid'>202, <a href='https://arxiv.org/pdf/2412.06142.pdf' target='_blank'>https://arxiv.org/pdf/2412.06142.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zonglin Meng, Yun Zhang, Zhaoliang Zheng, Zhihao Zhao, Jiaqi Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.06142">AgentAlign: Misalignment-Adapted Multi-Agent Perception for Resilient Inter-Agent Sensor Correlations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cooperative perception has attracted wide attention given its capability to leverage shared information across connected automated vehicles (CAVs) and smart infrastructures to address sensing occlusion and range limitation issues. However, existing research overlooks the fragile multi-sensor correlations in multi-agent settings, as the heterogeneous agent sensor measurements are highly susceptible to environmental factors, leading to weakened inter-agent sensor interactions. The varying operational conditions and other real-world factors inevitably introduce multifactorial noise and consequentially lead to multi-sensor misalignment, making the deployment of multi-agent multi-modality perception particularly challenging in the real world. In this paper, we propose AgentAlign, a real-world heterogeneous agent cross-modality feature alignment framework, to effectively address these multi-modality misalignment issues. Our method introduces a cross-modality feature alignment space (CFAS) and heterogeneous agent feature alignment (HAFA) mechanism to harmonize multi-modality features across various agents dynamically. Additionally, we present a novel V2XSet-noise dataset that simulates realistic sensor imperfections under diverse environmental conditions, facilitating a systematic evaluation of our approach's robustness. Extensive experiments on the V2X-Real and V2XSet-Noise benchmarks demonstrate that our framework achieves state-of-the-art performance, underscoring its potential for real-world applications in cooperative autonomous driving. The controllable V2XSet-Noise dataset and generation pipeline will be released in the future.
<div id='section'>Paperid: <span id='pid'>203, <a href='https://arxiv.org/pdf/2411.14927.pdf' target='_blank'>https://arxiv.org/pdf/2411.14927.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenwei Yang, Jilei Mao, Wenxian Yang, Yibo Ai, Yu Kong, Haibao Yu, Weidong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.14927">LiDAR-based End-to-end Temporal Perception for Vehicle-Infrastructure Cooperation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Temporal perception, defined as the capability to detect and track objects across temporal sequences, serves as a fundamental component in autonomous driving systems. While single-vehicle perception systems encounter limitations, stemming from incomplete perception due to object occlusion and inherent blind spots, cooperative perception systems present their own challenges in terms of sensor calibration precision and positioning accuracy. To address these issues, we introduce LET-VIC, a LiDAR-based End-to-End Tracking framework for Vehicle-Infrastructure Cooperation (VIC). First, we employ Temporal Self-Attention and VIC Cross-Attention modules to effectively integrate temporal and spatial information from both vehicle and infrastructure perspectives. Then, we develop a novel Calibration Error Compensation (CEC) module to mitigate sensor misalignment issues and facilitate accurate feature alignment. Experiments on the V2X-Seq-SPD dataset demonstrate that LET-VIC significantly outperforms baseline models. Compared to LET-V, LET-VIC achieves +15.0% improvement in mAP and a +17.3% improvement in AMOTA. Furthermore, LET-VIC surpasses representative Tracking by Detection models, including V2VNet, FFNet, and PointPillars, with at least a +13.7% improvement in mAP and a +13.1% improvement in AMOTA without considering communication delays, showcasing its robust detection and tracking performance. The experiments demonstrate that the integration of multi-view perspectives, temporal sequences, or CEC in end-to-end training significantly improves both detection and tracking performance. All code will be open-sourced.
<div id='section'>Paperid: <span id='pid'>204, <a href='https://arxiv.org/pdf/2410.22662.pdf' target='_blank'>https://arxiv.org/pdf/2410.22662.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junting Chen, Checheng Yu, Xunzhe Zhou, Tianqi Xu, Yao Mu, Mengkang Hu, Wenqi Shao, Yikai Wang, Guohao Li, Lin Shao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.22662">EMOS: Embodiment-aware Heterogeneous Multi-robot Operating System with LLM Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Heterogeneous multi-robot systems (HMRS) have emerged as a powerful approach for tackling complex tasks that single robots cannot manage alone. Current large-language-model-based multi-agent systems (LLM-based MAS) have shown success in areas like software development and operating systems, but applying these systems to robot control presents unique challenges. In particular, the capabilities of each agent in a multi-robot system are inherently tied to the physical composition of the robots, rather than predefined roles. To address this issue, we introduce a novel multi-agent framework designed to enable effective collaboration among heterogeneous robots with varying embodiments and capabilities, along with a new benchmark named Habitat-MAS. One of our key designs is $\textit{Robot Resume}$: Instead of adopting human-designed role play, we propose a self-prompted approach, where agents comprehend robot URDF files and call robot kinematics tools to generate descriptions of their physics capabilities to guide their behavior in task planning and action execution. The Habitat-MAS benchmark is designed to assess how a multi-agent framework handles tasks that require embodiment-aware reasoning, which includes 1) manipulation, 2) perception, 3) navigation, and 4) comprehensive multi-floor object rearrangement. The experimental results indicate that the robot's resume and the hierarchical design of our multi-agent system are essential for the effective operation of the heterogeneous multi-robot system within this intricate problem context.
<div id='section'>Paperid: <span id='pid'>205, <a href='https://arxiv.org/pdf/2410.21040.pdf' target='_blank'>https://arxiv.org/pdf/2410.21040.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kazuma Obata, Tatsuya Aoki, Takato Horii, Tadahiro Taniguchi, Takayuki Nagai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.21040">LiP-LLM: Integrating Linear Programming and dependency graph with Large Language Models for multi-robot task planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study proposes LiP-LLM: integrating linear programming and dependency graph with large language models (LLMs) for multi-robot task planning. In order for multiple robots to perform tasks more efficiently, it is necessary to manage the precedence dependencies between tasks. Although multi-robot decentralized and centralized task planners using LLMs have been proposed, none of these studies focus on precedence dependencies from the perspective of task efficiency or leverage traditional optimization methods. It addresses key challenges in managing dependencies between skills and optimizing task allocation. LiP-LLM consists of three steps: skill list generation and dependency graph generation by LLMs, and task allocation using linear programming. The LLMs are utilized to generate a comprehensive list of skills and to construct a dependency graph that maps the relationships and sequential constraints among these skills. To ensure the feasibility and efficiency of skill execution, the skill list is generated by calculated likelihood, and linear programming is used to optimally allocate tasks to each robot. Experimental evaluations in simulated environments demonstrate that this method outperforms existing task planners, achieving higher success rates and efficiency in executing complex, multi-robot tasks. The results indicate the potential of combining LLMs with optimization techniques to enhance the capabilities of multi-robot systems in executing coordinated tasks accurately and efficiently. In an environment with two robots, a maximum success rate difference of 0.82 is observed in the language instruction group with a change in the object name.
<div id='section'>Paperid: <span id='pid'>206, <a href='https://arxiv.org/pdf/2409.11561.pdf' target='_blank'>https://arxiv.org/pdf/2409.11561.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weizheng Wang, Aniket Bera, Byung-Cheol Min
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.11561">Hypergraph-based Coordinated Task Allocation and Socially-aware Navigation for Multi-Robot Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A team of multiple robots seamlessly and safely working in human-filled public environments requires adaptive task allocation and socially-aware navigation that account for dynamic human behavior. Current approaches struggle with highly dynamic pedestrian movement and the need for flexible task allocation. We propose Hyper-SAMARL, a hypergraph-based system for multi-robot task allocation and socially-aware navigation, leveraging multi-agent reinforcement learning (MARL). Hyper-SAMARL models the environmental dynamics between robots, humans, and points of interest (POIs) using a hypergraph, enabling adaptive task assignment and socially-compliant navigation through a hypergraph diffusion mechanism. Our framework, trained with MARL, effectively captures interactions between robots and humans, adapting tasks based on real-time changes in human activity. Experimental results demonstrate that Hyper-SAMARL outperforms baseline models in terms of social navigation, task completion efficiency, and adaptability in various simulated scenarios.
<div id='section'>Paperid: <span id='pid'>207, <a href='https://arxiv.org/pdf/2402.16227.pdf' target='_blank'>https://arxiv.org/pdf/2402.16227.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Arshiya Taj Abdul, Augustinos D. Saravanos, Evangelos A. Theodorou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.16227">Scaling Robust Optimization for Multi-Agent Robotic Systems: A Distributed Perspective</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a novel distributed robust optimization scheme for steering distributions of multi-agent systems under stochastic and deterministic uncertainty. Robust optimization is a subfield of optimization which aims to discover an optimal solution that remains robustly feasible for all possible realizations of the problem parameters within a given uncertainty set. Such approaches would naturally constitute an ideal candidate for multi-robot control, where in addition to stochastic noise, there might be exogenous deterministic disturbances. Nevertheless, as these methods are usually associated with significantly high computational demands, their application to multi-agent robotics has remained limited. The scope of this work is to propose a scalable robust optimization framework that effectively addresses both types of uncertainties, while retaining computational efficiency and scalability. In this direction, we provide tractable approximations for robust constraints that are relevant in multi-robot settings. Subsequently, we demonstrate how computations can be distributed through an Alternating Direction Method of Multipliers (ADMM) approach towards achieving scalability and communication efficiency. All improvements are also theoretically justified by establishing and comparing the resulting computational complexities. Simulation results highlight the performance of the proposed algorithm in effectively handling both stochastic and deterministic uncertainty in multi-robot systems. The scalability of the method is also emphasized by showcasing tasks with up to hundreds of agents. The results of this work indicate the promise of blending robust optimization, distribution steering and distributed optimization towards achieving scalable, safe and robust multi-robot control.
<div id='section'>Paperid: <span id='pid'>208, <a href='https://arxiv.org/pdf/2402.11768.pdf' target='_blank'>https://arxiv.org/pdf/2402.11768.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Teng Guo, Jingjin Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.11768">Targeted Parallelization of Conflict-Based Search for Multi-Robot Path Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Robot Path Planning (MRPP) on graphs, equivalently known as Multi-Agent Path Finding (MAPF), is a well-established NP-hard problem with critically important applications. As serial computation in (near)-optimally solving MRPP approaches the computation efficiency limit, parallelization offers a promising route to push the limit further, especially in handling hard or large MRPP instances. In this study, we initiated a \emph{targeted} parallelization effort to boost the performance of conflict-based search for MRPP. Specifically, when instances are relatively small but robots are densely packed with strong interactions, we apply a decentralized parallel algorithm that concurrently explores multiple branches that leads to markedly enhanced solution discovery. On the other hand, when instances are large with sparse robot-robot interactions, we prioritize node expansion and conflict resolution. Our innovative multi-threaded approach to parallelizing bounded-suboptimal conflict search-based algorithms demonstrates significant improvements over baseline serial methods in success rate or runtime. Our contribution further pushes the understanding of MRPP and charts a promising path for elevating solution quality and computational efficiency through parallel algorithmic strategies.
<div id='section'>Paperid: <span id='pid'>209, <a href='https://arxiv.org/pdf/2402.11766.pdf' target='_blank'>https://arxiv.org/pdf/2402.11766.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Teng Guo, Jingjin Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.11766">Well-Connected Set and Its Application to Multi-Robot Path Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Parking lots and autonomous warehouses for accommodating many vehicles/robots adopt designs in which the underlying graphs are \emph{well-connected} to simplify planning and reduce congestion. In this study, we formulate and delve into the \emph{largest well-connected set} (LWCS) problem and explore its applications in layout design for multi-robot path planning. Roughly speaking, a well-connected set over a connected graph is a set of vertices such that there is a path on the graph connecting any pair of vertices in the set without passing through any additional vertices of the set. Identifying an LWCS has many potential high-utility applications, e.g., for determining parking garage layout and capacity, as prioritized planning can be shown to be complete when start/goal configurations belong to an LWCS. In this work, we establish that computing an LWCS is NP-complete. We further develop optimal and near-optimal LWCS algorithms, with the near-optimal algorithm targeting large maps. A complete prioritized planning method is given for planning paths for multiple robots residing on an LWCS.
<div id='section'>Paperid: <span id='pid'>210, <a href='https://arxiv.org/pdf/2310.17753.pdf' target='_blank'>https://arxiv.org/pdf/2310.17753.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Teng Guo, Jingjin Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.17753">Bin Assignment and Decentralized Path Planning for Multi-Robot Parcel Sorting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>At modern warehouses, mobile robots transport packages and drop them into collection bins/chutes based on shipping destinations grouped by, e.g., the ZIP code. System throughput, measured as the number of packages sorted per unit of time, determines the efficiency of the warehouse. This research develops a scalable, high-throughput multi-robot parcel sorting solution, decomposing the task into two related processes, bin assignment and offline/online multi-robot path planning, and optimizing both. Bin assignment matches collection bins with package types to minimize traveling costs. Subsequently, robots are assigned to pick up and drop packages into assigned bins. Multiple highly effective bin assignment algorithms are proposed that can work with an arbitrary planning algorithm. We propose a decentralized path planning routine using only local information to route the robots over a carefully constructed directed road network for multi-robot path planning. Our decentralized planner, provably probabilistically deadlock-free, consistently delivers near-optimal results on par with some top-performing centralized planners while significantly reducing computation times by orders of magnitude. Extensive simulations show that our overall framework delivers promising performances.
<div id='section'>Paperid: <span id='pid'>211, <a href='https://arxiv.org/pdf/2306.14409.pdf' target='_blank'>https://arxiv.org/pdf/2306.14409.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Teng Guo, Jingjin Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.14409">Efficient Heuristics for Multi-Robot Path Planning in Crowded Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Optimal Multi-Robot Path Planning (MRPP) has garnered significant attention due to its many applications in domains including warehouse automation, transportation, and swarm robotics. Current MRPP solvers can be divided into reduction-based, search-based, and rule-based categories, each with their strengths and limitations. Regardless of the methodology, however, the issue of handling dense MRPP instances remains a significant challenge, where existing approaches generally demonstrate a dichotomy regarding solution optimality and efficiency. This study seeks to bridge the gap in optimal MRPP resolution for dense, highly-entangled scenarios, with potential applications to high-density storage systems and traffic congestion control. Toward that goal, we analyze the behaviors of SOTA MRPP algorithms in dense settings and develop two hybrid algorithms leveraging the strengths of existing SOTA algorithms: DCBS (database-accelerated enhanced conflict-based search) and SCBS (sparsified enhanced conflict-based search). Experimental validations demonstrate that DCBS and SCBS deliver a significant reduction in computational time compared to existing bounded-suboptimal methods and improve solution quality compared to existing rule-based methods, achieving a desirable balance between computational efficiency and solution optimality. As a result, DCBS and SCBS are particularly suitable for quickly computing good-quality solutions for multi-robot routing in dense settings
<div id='section'>Paperid: <span id='pid'>212, <a href='https://arxiv.org/pdf/2304.04362.pdf' target='_blank'>https://arxiv.org/pdf/2304.04362.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yulun Tian, Yun Chang, Long Quang, Arthur Schang, Carlos Nieto-Granda, Jonathan P. How, Luca Carlone
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.04362">Resilient and Distributed Multi-Robot Visual SLAM: Datasets, Experiments, and Lessons Learned</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper revisits Kimera-Multi, a distributed multi-robot Simultaneous Localization and Mapping (SLAM) system, towards the goal of deployment in the real world. In particular, this paper has three main contributions. First, we describe improvements to Kimera-Multi to make it resilient to large-scale real-world deployments, with particular emphasis on handling intermittent and unreliable communication. Second, we collect and release challenging multi-robot benchmarking datasets obtained during live experiments conducted on the MIT campus, with accurate reference trajectories and maps for evaluation. The datasets include up to 8 robots traversing long distances (up to 8 km) and feature many challenging elements such as severe visual ambiguities (e.g., in underground tunnels and hallways), mixed indoor and outdoor trajectories with different lighting conditions, and dynamic entities (e.g., pedestrians and cars). Lastly, we evaluate the resilience of Kimera-Multi under different communication scenarios, and provide a quantitative comparison with a centralized baseline system. Based on the results from both live experiments and subsequent analysis, we discuss the strengths and weaknesses of Kimera-Multi, and suggest future directions for both algorithm and system design. We release the source code of Kimera-Multi and all datasets to facilitate further research towards the reliable real-world deployment of multi-robot SLAM systems.
<div id='section'>Paperid: <span id='pid'>213, <a href='https://arxiv.org/pdf/2303.01242.pdf' target='_blank'>https://arxiv.org/pdf/2303.01242.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianyue Wu, Fei Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.01242">Distributed Optimization in Sensor Network for Scalable Multi-Robot Relative State Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Distance measurements demonstrate distinctive scalability when used for relative state estimation in large-scale multi-robot systems. Despite the attractiveness of distance measurements, multi-robot relative state estimation based on distance measurements raises a tricky optimization problem, especially in the context of large-scale systems. Motivated by this, we aim to develop specialized computational techniques that enable robust and efficient estimation when deploying distance measurements at scale. We first reveal the commonality between the estimation problem and the one that finds realization of a sensor network, from which we draw crucial lesson to inspire the proposed methods. However, solving the latter problem in large-scale (still) requires distributed optimization schemes with scalability natures, efficient computational procedures, and fast convergence rates. Towards this goal, we propose a complementary pair of distributed computational techniques with the classical block coordinate descent (BCD) algorithm as a unified backbone. In the first method, we treat Burer-Monteiro factorization as a rank-restricted heuristic for rank-constrained semidefinite programming (SDP), where a specialized BCD-type algorithm that analytically solve each block update subproblem is employed. Although this method enables robust and (extremely) fast recovery of estimates from initial guesses, it inevitably fails as the initialization becomes disorganized. We therefore propose the second method, derived from a convex formulation named anchored edge-based semidefinite programming} (ESDP), to complement it, at the expense of a certain loss of efficiency. This formulation is structurally decomposable so that BCD can be naturally employed, where each subproblem is convex and (again) solved exactly...
<div id='section'>Paperid: <span id='pid'>214, <a href='https://arxiv.org/pdf/2302.01305.pdf' target='_blank'>https://arxiv.org/pdf/2302.01305.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Teng Guo, Jingjin Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.01305">Toward Efficient Physical and Algorithmic Design of Automated Garages</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Parking in large metropolitan areas is often a time-consuming task with further implications toward traffic patterns that affect urban landscaping. Reducing the premium space needed for parking has led to the development of automated mechanical parking systems. Compared to regular garages having one or two rows of vehicles in each island, automated garages can have multiple rows of vehicles stacked together to support higher parking demands. Although this multi-row layout reduces parking space, it makes the parking and retrieval more complicated. In this work, we propose an automated garage design that supports near 100% parking density. Modeling the problem of parking and retrieving multiple vehicles as a special class of multi-robot path planning problem, we propose associated algorithms for handling all common operations of the automated garage, including (1) optimal algorithm and near-optimal methods that find feasible and efficient solutions for simultaneous parking/retrieval and (2) a novel shuffling mechanism to rearrange vehicles to facilitate scheduled retrieval at rush hours. We conduct thorough simulation studies showing the proposed methods are promising for large and high-density real-world parking applications.
<div id='section'>Paperid: <span id='pid'>215, <a href='https://arxiv.org/pdf/2112.00165.pdf' target='_blank'>https://arxiv.org/pdf/2112.00165.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Enrica Rossi, Marco Tognon, Luca Ballotta, Ruggero Carli, Juan CortÃ©s, Antonio Franchi, Luca Schenato
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2112.00165">Coordinated Multi-Robot Trajectory Tracking Control over Sampled Communication</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose an inverse-kinematics controller for a class of multi-robot systems in the scenario of sampled communication. The goal is to make a group of robots perform trajectory tracking in a coordinated way when the sampling time of communications is much larger than the sampling time of low-level controllers, disrupting theoretical convergence guarantees of standard control design in continuous time. Given a desired trajectory in configuration space which is precomputed offline, the proposed controller receives configuration measurements, possibly via wireless, to re-compute velocity references for the robots, which are tracked by a low-level controller. We propose joint design of a sampled proportional feedback plus a novel continuous-time feedforward that linearizes the dynamics around the reference trajectory: this method is amenable to distributed communication implementation where only one broadcast transmission is needed per sample. Also, we provide closed-form expressions for instability and stability regions and convergence rate in terms of proportional gain $k$ and sampling period $T$. We test the proposed control strategy via numerical simulations in the scenario of cooperative aerial manipulation of a cable-suspended load using a realistic simulator (Fly-Crane). Finally, we compare our proposed controller with centralized approaches that adapt the feedback gain online through smart heuristics, and show that it achieves comparable performance.
<div id='section'>Paperid: <span id='pid'>216, <a href='https://arxiv.org/pdf/2103.13446.pdf' target='_blank'>https://arxiv.org/pdf/2103.13446.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ryan Kortvelesy, Amanda Prorok
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2103.13446">ModGNN: Expert Policy Approximation in Multi-Agent Systems with a Modular Graph Neural Network Architecture</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent work in the multi-agent domain has shown the promise of Graph Neural Networks (GNNs) to learn complex coordination strategies. However, most current approaches use minor variants of a Graph Convolutional Network (GCN), which applies a convolution to the communication graph formed by the multi-agent system. In this paper, we investigate whether the performance and generalization of GCNs can be improved upon. We introduce ModGNN, a decentralized framework which serves as a generalization of GCNs, providing more flexibility. To test our hypothesis, we evaluate an implementation of ModGNN against several baselines in the multi-agent flocking problem. We perform an ablation analysis to show that the most important component of our framework is one that does not exist in a GCN. By varying the number of agents, we also demonstrate that an application-agnostic implementation of ModGNN possesses an improved ability to generalize to new environments.
<div id='section'>Paperid: <span id='pid'>217, <a href='https://arxiv.org/pdf/2509.19463.pdf' target='_blank'>https://arxiv.org/pdf/2509.19463.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Doncey Albin, Daniel McGann, Miles Mena, Annika Thomas, Harel Biggie, Xuefei Sun, Steve McGuire, Jonathan P. How, Christoffer Heckman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19463">CU-Multi: A Dataset for Multi-Robot Collaborative Perception</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A central challenge for multi-robot systems is fusing independently gathered perception data into a unified representation. Despite progress in Collaborative SLAM (C-SLAM), benchmarking remains hindered by the scarcity of dedicated multi-robot datasets. Many evaluations instead partition single-robot trajectories, a practice that may only partially reflect true multi-robot operations and, more critically, lacks standardization, leading to results that are difficult to interpret or compare across studies. While several multi-robot datasets have recently been introduced, they mostly contain short trajectories with limited inter-robot overlap and sparse intra-robot loop closures. To overcome these limitations, we introduce CU-Multi, a dataset collected over multiple days at two large outdoor sites on the University of Colorado Boulder campus. CU-Multi comprises four synchronized runs with aligned start times and controlled trajectory overlap, replicating the distinct perspectives of a robot team. It includes RGB-D sensing, RTK GPS, semantic LiDAR, and refined ground-truth odometry. By combining overlap variation with dense semantic annotations, CU-Multi provides a strong foundation for reproducible evaluation in multi-robot collaborative perception tasks.
<div id='section'>Paperid: <span id='pid'>218, <a href='https://arxiv.org/pdf/2509.19463.pdf' target='_blank'>https://arxiv.org/pdf/2509.19463.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Doncey Albin, Daniel McGann, Miles Mena, Annika Thomas, Harel Biggie, Xuefei Sun, Steve McGuire, Jonathan P. How, Christoffer Heckman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19463">CU-Multi: A Dataset for Multi-Robot Collaborative Perception</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A central challenge for multi-robot systems is fusing independently gathered perception data into a unified representation. Despite progress in Collaborative SLAM (C-SLAM), benchmarking remains hindered by the scarcity of dedicated multi-robot datasets. Many evaluations instead partition single-robot trajectories, a practice that may only partially reflect true multi-robot operations and, more critically, lacks standardization, leading to results that are difficult to interpret or compare across studies. While several multi-robot datasets have recently been introduced, they mostly contain short trajectories with limited inter-robot overlap and sparse intra-robot loop closures. To overcome these limitations, we introduce CU-Multi, a dataset collected over multiple days at two large outdoor sites on the University of Colorado Boulder campus. CU-Multi comprises four synchronized runs with aligned start times and controlled trajectory overlap, replicating the distinct perspectives of a robot team. It includes RGB-D sensing, RTK GPS, semantic LiDAR, and refined ground-truth odometry. By combining overlap variation with dense semantic annotations, CU-Multi provides a strong foundation for reproducible evaluation in multi-robot collaborative perception tasks.
<div id='section'>Paperid: <span id='pid'>219, <a href='https://arxiv.org/pdf/2509.08257.pdf' target='_blank'>https://arxiv.org/pdf/2509.08257.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yongkai Tian, Yirong Qi, Xin Yu, Wenjun Wu, Jie Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.08257">Symmetry-Guided Multi-Agent Inverse Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In robotic systems, the performance of reinforcement learning depends on the rationality of predefined reward functions. However, manually designed reward functions often lead to policy failures due to inaccuracies. Inverse Reinforcement Learning (IRL) addresses this problem by inferring implicit reward functions from expert demonstrations. Nevertheless, existing methods rely heavily on large amounts of expert demonstrations to accurately recover the reward function. The high cost of collecting expert demonstrations in robotic applications, particularly in multi-robot systems, severely hinders the practical deployment of IRL. Consequently, improving sample efficiency has emerged as a critical challenge in multi-agent inverse reinforcement learning (MIRL). Inspired by the symmetry inherent in multi-agent systems, this work theoretically demonstrates that leveraging symmetry enables the recovery of more accurate reward functions. Building upon this insight, we propose a universal framework that integrates symmetry into existing multi-agent adversarial IRL algorithms, thereby significantly enhancing sample efficiency. Experimental results from multiple challenging tasks have demonstrated the effectiveness of this framework. Further validation in physical multi-robot systems has shown the practicality of our method.
<div id='section'>Paperid: <span id='pid'>220, <a href='https://arxiv.org/pdf/2509.08257.pdf' target='_blank'>https://arxiv.org/pdf/2509.08257.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yongkai Tian, Yirong Qi, Xin Yu, Wenjun Wu, Jie Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.08257">Symmetry-Guided Multi-Agent Inverse Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In robotic systems, the performance of reinforcement learning depends on the rationality of predefined reward functions. However, manually designed reward functions often lead to policy failures due to inaccuracies. Inverse Reinforcement Learning (IRL) addresses this problem by inferring implicit reward functions from expert demonstrations. Nevertheless, existing methods rely heavily on large amounts of expert demonstrations to accurately recover the reward function. The high cost of collecting expert demonstrations in robotic applications, particularly in multi-robot systems, severely hinders the practical deployment of IRL. Consequently, improving sample efficiency has emerged as a critical challenge in multi-agent inverse reinforcement learning (MIRL). Inspired by the symmetry inherent in multi-agent systems, this work theoretically demonstrates that leveraging symmetry enables the recovery of more accurate reward functions. Building upon this insight, we propose a universal framework that integrates symmetry into existing multi-agent adversarial IRL algorithms, thereby significantly enhancing sample efficiency. Experimental results from multiple challenging tasks have demonstrated the effectiveness of this framework. Further validation in physical multi-robot systems has shown the practicality of our method.
<div id='section'>Paperid: <span id='pid'>221, <a href='https://arxiv.org/pdf/2508.03526.pdf' target='_blank'>https://arxiv.org/pdf/2508.03526.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kun Song, Shentao Ma, Gaoming Chen, Ninglong Jin, Guangbao Zhao, Mingyu Ding, Zhenhua Xiong, Jia Pan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.03526">CollaBot: Vision-Language Guided Simultaneous Collaborative Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A central research topic in robotics is how to use this system to interact with the physical world. Traditional manipulation tasks primarily focus on small objects. However, in factory or home environments, there is often a need for the movement of large objects, such as moving tables. These tasks typically require multi-robot systems to work collaboratively. Previous research lacks a framework that can scale to arbitrary sizes of robots and generalize to various kinds of tasks. In this work, we propose CollaBot, a generalist framework for simultaneous collaborative manipulation. First, we use SEEM for scene segmentation and point cloud extraction of the target object. Then, we propose a collaborative grasping framework, which decomposes the task into local grasp pose generation and global collaboration. Finally, we design a 2-stage planning module that can generate collision-free trajectories to achieve this task. Experiments show a success rate of 52% across different numbers of robots, objects, and tasks, indicating the effectiveness of the proposed framework.
<div id='section'>Paperid: <span id='pid'>222, <a href='https://arxiv.org/pdf/2507.15781.pdf' target='_blank'>https://arxiv.org/pdf/2507.15781.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gian Carlo Maffettone, Alain Boldini, Mario di Bernardo, Maurizio Porfiri
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.15781">Density control of multi-agent swarms via bio-inspired leader-follower plasticity</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The design of control systems for the spatial self-organization of mobile agents is an open challenge across several engineering domains, including swarm robotics and synthetic biology. Here, we propose a bio-inspired leader-follower solution, which is aware of energy constraints of mobile agents and is apt to deal with large swarms. Akin to many natural systems, control objectives are formulated for the entire collective, and leaders and followers are allowed to plastically switch their role in time. We frame a density control problem, modeling the agents' population via a system of nonlinear partial differential equations. This approach allows for a compact description that inherently avoids the curse of dimensionality and improves analytical tractability. We derive analytical guarantees for the existence of desired steady-state solutions and their local stability for one-dimensional and higher-dimensional problems. We numerically validate our control methodology, offering support to the effectiveness, robustness, and versatility of our proposed bio-inspired control strategy.
<div id='section'>Paperid: <span id='pid'>223, <a href='https://arxiv.org/pdf/2506.00837.pdf' target='_blank'>https://arxiv.org/pdf/2506.00837.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiqing Luo, Yi Wang, Yingying He, Wei Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.00837">Improving Multi-Vehicle Perception Fusion with Millimeter-Wave Radar Assistance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cooperative perception enables vehicles to share sensor readings and has become a new paradigm to improve driving safety, where the key enabling technology for realizing this vision is to real-time and accurately align and fuse the perceptions. Recent advances to align the views rely on high-density LiDAR data or fine-grained image feature representations, which however fail to meet the requirements of accuracy, real-time, and adaptability for autonomous driving. To this end, we present MMatch, a lightweight system that enables accurate and real-time perception fusion with mmWave radar point clouds. The key insight is that fine-grained spatial information provided by the radar present unique associations with all the vehicles even in two separate views. As a result, by capturing and understanding the unique local and global position of the targets in this association, we can quickly find out all the co-visible vehicles for view alignment. We implement MMatch on both the datasets collected from the CARLA platform and the real-world traffic with over 15,000 radar point cloud pairs. Experimental results show that MMatch achieves decimeter-level accuracy within 59ms, which significantly improves the reliability for autonomous driving.
<div id='section'>Paperid: <span id='pid'>224, <a href='https://arxiv.org/pdf/2412.05839.pdf' target='_blank'>https://arxiv.org/pdf/2412.05839.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Juwon Kim, Hogyun Kim, Seokhwan Jeong, Youngsik Shin, Younggun Cho
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.05839">DiTer++: Diverse Terrain and Multi-modal Dataset for Multi-Robot SLAM in Multi-session Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We encounter large-scale environments where both structured and unstructured spaces coexist, such as on campuses. In this environment, lighting conditions and dynamic objects change constantly. To tackle the challenges of large-scale mapping under such conditions, we introduce DiTer++, a diverse terrain and multi-modal dataset designed for multi-robot SLAM in multi-session environments. According to our datasets' scenarios, Agent-A and Agent-B scan the area designated for efficient large-scale mapping day and night, respectively. Also, we utilize legged robots for terrain-agnostic traversing. To generate the ground-truth of each robot, we first build the survey-grade prior map. Then, we remove the dynamic objects and outliers from the prior map and extract the trajectory through scan-to-map matching. Our dataset and supplement materials are available at https://sites.google.com/view/diter-plusplus/.
<div id='section'>Paperid: <span id='pid'>225, <a href='https://arxiv.org/pdf/2411.04791.pdf' target='_blank'>https://arxiv.org/pdf/2411.04791.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Beniamino Di Lorenzo, Gian Carlo Maffettone, Mario di Bernardo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.04791">A Continuification-Based Control Solution for Large-Scale Shepherding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we address the large-scale shepherding control problem using a continuification-based strategy. We consider a scenario in which a large group of follower agents (targets) must be confined within a designated goal region through indirect interactions with a controllable set of leader agents (herders). Our approach transforms the microscopic agent-based dynamics into a macroscopic continuum model via partial differential equations (PDEs). This formulation enables efficient, scalable control design for the herders' behavior, with guarantees of global convergence. Numerical and experimental validations in a mixed-reality swarm robotics framework demonstrate the method's effectiveness.
<div id='section'>Paperid: <span id='pid'>226, <a href='https://arxiv.org/pdf/2410.08262.pdf' target='_blank'>https://arxiv.org/pdf/2410.08262.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mason B. Peterson, Yixuan Jia, Yulun Tian, Annika Thomas, Jonathan P. How
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.08262">ROMAN: Open-Set Object Map Alignment for Robust View-Invariant Global Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Global localization is a fundamental capability required for long-term and drift-free robot navigation. However, current methods fail to relocalize when faced with significantly different viewpoints. We present ROMAN (Robust Object Map Alignment Anywhere), a global localization method capable of localizing in challenging and diverse environments by creating and aligning maps of open-set and view-invariant objects. ROMAN formulates and solves a registration problem between object submaps using a unified graph-theoretic global data association approach with a novel incorporation of a gravity direction prior and object shape and semantic similarity. This work's open-set object mapping and information-rich object association algorithm enables global localization, even in instances when maps are created from robots traveling in opposite directions. Through a set of challenging global localization experiments in indoor, urban, and unstructured/forested environments, we demonstrate that ROMAN achieves higher relative pose estimation accuracy than other image-based pose estimation methods or segment-based registration methods. Additionally, we evaluate ROMAN as a loop closure module in large-scale multi-robot SLAM and show a 35% improvement in trajectory estimation error compared to standard SLAM systems using visual features for loop closures. Code and videos can be found at https://acl.mit.edu/roman.
<div id='section'>Paperid: <span id='pid'>227, <a href='https://arxiv.org/pdf/2407.08164.pdf' target='_blank'>https://arxiv.org/pdf/2407.08164.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pu Feng, Junkang Liang, Size Wang, Xin Yu, Xin Ji, Yiting Chen, Kui Zhang, Rongye Shi, Wenjun Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.08164">Hierarchical Consensus-Based Multi-Agent Reinforcement Learning for Multi-Robot Cooperation Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In multi-agent reinforcement learning (MARL), the Centralized Training with Decentralized Execution (CTDE) framework is pivotal but struggles due to a gap: global state guidance in training versus reliance on local observations in execution, lacking global signals. Inspired by human societal consensus mechanisms, we introduce the Hierarchical Consensus-based Multi-Agent Reinforcement Learning (HC-MARL) framework to address this limitation. HC-MARL employs contrastive learning to foster a global consensus among agents, enabling cooperative behavior without direct communication. This approach enables agents to form a global consensus from local observations, using it as an additional piece of information to guide collaborative actions during execution. To cater to the dynamic requirements of various tasks, consensus is divided into multiple layers, encompassing both short-term and long-term considerations. Short-term observations prompt the creation of an immediate, low-layer consensus, while long-term observations contribute to the formation of a strategic, high-layer consensus. This process is further refined through an adaptive attention mechanism that dynamically adjusts the influence of each consensus layer. This mechanism optimizes the balance between immediate reactions and strategic planning, tailoring it to the specific demands of the task at hand. Extensive experiments and real-world applications in multi-robot systems showcase our framework's superior performance, marking significant advancements over baselines.
<div id='section'>Paperid: <span id='pid'>228, <a href='https://arxiv.org/pdf/2407.06296.pdf' target='_blank'>https://arxiv.org/pdf/2407.06296.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aiman Munir, Ehsan Latif, Ramviyas Parasuraman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.06296">Anchor-Oriented Localized Voronoi Partitioning for GPS-denied Multi-Robot Coverage</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-robot coverage is crucial in numerous applications, including environmental monitoring, search and rescue operations, and precision agriculture. In modern applications, a multi-robot team must collaboratively explore unknown spatial fields in GPS-denied and extreme environments where global localization is unavailable. Coverage algorithms typically assume that the robot positions and the coverage environment are defined in a global reference frame. However, coordinating robot motion and ensuring coverage of the shared convex workspace without global localization is challenging. This paper proposes a novel anchor-oriented coverage (AOC) approach to generate dynamic localized Voronoi partitions based around a common anchor position. We further propose a consensus-based coordination algorithm that achieves agreement on the coverage workspace around the anchor in the robots' relative frames of reference. Through extensive simulations and real-world experiments, we demonstrate that the proposed anchor-oriented approach using localized Voronoi partitioning performs as well as the state-of-the-art coverage controller using GPS.
<div id='section'>Paperid: <span id='pid'>229, <a href='https://arxiv.org/pdf/2405.11726.pdf' target='_blank'>https://arxiv.org/pdf/2405.11726.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gaoming Chen, Kun Song, Xiang Xu, Wenhang Liu, Zhenhua Xiong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.11726">RHAML: Rendezvous-based Hierarchical Architecture for Mutual Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mutual localization serves as the foundation for collaborative perception and task assignment in multi-robot systems. Effectively utilizing limited onboard sensors for mutual localization between marker-less robots is a worthwhile goal. However, due to inadequate consideration of large scale variations of the observed robot and localization refinement, previous work has shown limited accuracy when robots are equipped only with RGB cameras. To enhance the precision of localization, this paper proposes a novel rendezvous-based hierarchical architecture for mutual localization (RHAML). Firstly, to learn multi-scale robot features, anisotropic convolutions are introduced into the network, yielding initial localization results. Then, the iterative refinement module with rendering is employed to adjust the observed robot poses. Finally, the pose graph is conducted to globally optimize all localization results, which takes into account multi-frame observations. Therefore, a flexible architecture is provided that allows for the selection of appropriate modules based on requirements. Simulations demonstrate that RHAML effectively addresses the problem of multi-robot mutual localization, achieving translation errors below 2 cm and rotation errors below 0.5 degrees when robots exhibit 5 m of depth variation. Moreover, its practical utility is validated by applying it to map fusion when multi-robots explore unknown environments.
<div id='section'>Paperid: <span id='pid'>230, <a href='https://arxiv.org/pdf/2405.04079.pdf' target='_blank'>https://arxiv.org/pdf/2405.04079.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Miquel Kegeleirs, David GarzÃ³n Ramos, Guillermo Legarda Herranz, Ilyes Gharbi, Jeanne Szpirer, Ken Hasselmann, Lorenzo Garattoni, Gianpiero Francesca, Mauro Birattari
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.04079">Leveraging swarm capabilities to assist other systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Most studies in swarm robotics treat the swarm as an isolated system of interest. We argue that the prevailing view of swarms as self-sufficient, independent systems limits the scope of potential applications for swarm robotics. A robot swarm could act as a support in an heterogeneous system comprising other robots and/or human operators, in particular by quickly providing access to a large amount of data acquired in large unknown environments. Tasks such as target identification & tracking, scouting, or monitoring/surveillance could benefit from this approach.
<div id='section'>Paperid: <span id='pid'>231, <a href='https://arxiv.org/pdf/2404.14649.pdf' target='_blank'>https://arxiv.org/pdf/2404.14649.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zechen Hu, Daigo Shishika, Xuesu Xiao, Xuan Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.14649">Bi-CL: A Reinforcement Learning Framework for Robots Coordination Through Bi-level Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In multi-robot systems, achieving coordinated missions remains a significant challenge due to the coupled nature of coordination behaviors and the lack of global information for individual robots. To mitigate these challenges, this paper introduces a novel approach, Bi-level Coordination Learning (Bi-CL), that leverages a bi-level optimization structure within a centralized training and decentralized execution paradigm. Our bi-level reformulation decomposes the original problem into a reinforcement learning level with reduced action space, and an imitation learning level that gains demonstrations from a global optimizer. Both levels contribute to improved learning efficiency and scalability. We note that robots' incomplete information leads to mismatches between the two levels of learning models. To address this, Bi-CL further integrates an alignment penalty mechanism, aiming to minimize the discrepancy between the two levels without degrading their training efficiency. We introduce a running example to conceptualize the problem formulation and apply Bi-CL to two variations of this example: route-based and graph-based scenarios. Simulation results demonstrate that Bi-CL can learn more efficiently and achieve comparable performance with traditional multi-agent reinforcement learning baselines for multi-robot coordination.
<div id='section'>Paperid: <span id='pid'>232, <a href='https://arxiv.org/pdf/2404.06413.pdf' target='_blank'>https://arxiv.org/pdf/2404.06413.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kunal Garg, Songyuan Zhang, Jacob Arkin, Chuchu Fan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.06413">Foundation Models to the Rescue: Deadlock Resolution in Connected Multi-Robot Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Connected multi-agent robotic systems (MRS) are prone to deadlocks in an obstacle environment where the robots can get stuck away from their desired locations under a smooth low-level control policy. Without an external intervention, often in terms of a high-level command, a low-level control policy cannot resolve such deadlocks. Utilizing the generalizability and low data requirements of foundation models, this paper explores the possibility of using text-based models, i.e., large language models (LLMs), and text-and-image-based models, i.e., vision-language models (VLMs), as high-level planners for deadlock resolution. We propose a hierarchical control framework where a foundation model-based high-level planner helps to resolve deadlocks by assigning a leader to the MRS along with a set of waypoints for the MRS leader. Then, a low-level distributed control policy based on graph neural networks is executed to safely follow these waypoints, thereby evading the deadlock. We conduct extensive experiments on various MRS environments using the best available pre-trained LLMs and VLMs. We compare their performance with a graph-based planner in terms of effectiveness in helping the MRS reach their target locations and computational time. Our results illustrate that, compared to grid-based planners, the foundation models perform better in terms of the goal-reaching rate and computational time for complex environments, which helps us conclude that foundation models can assist MRS operating in complex obstacle-cluttered environments to resolve deadlocks efficiently.
<div id='section'>Paperid: <span id='pid'>233, <a href='https://arxiv.org/pdf/2401.13540.pdf' target='_blank'>https://arxiv.org/pdf/2401.13540.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sven Lilge, Timothy D. Barfoot, Jessica Burgner-Kahrs
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.13540">State Estimation for Continuum Multi-Robot Systems on SE(3)</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In contrast to conventional robots, accurately modeling the kinematics and statics of continuum robots is challenging due to partially unknown material properties, parasitic effects, or unknown forces acting on the continuous body. Consequentially, state estimation approaches that utilize additional sensor information to predict the shape of continuum robots have garnered significant interest. This paper presents a novel approach to state estimation for systems with multiple coupled continuum robots, which allows estimating the shape and strain variables of multiple continuum robots in an arbitrary coupled topology. Simulations and experiments demonstrate the capabilities and versatility of the proposed method, while achieving accurate and continuous estimates for the state of such systems, resulting in average end-effector errors of 3.3 mm and 5.02Â° depending on the sensor setup. It is further shown, that the approach offers fast computation times of below 10 ms, enabling its utilization in quasi-static real-time scenarios with average update rates of 100-200 Hz. An open-source C++ implementation of the proposed state estimation method is made publicly available to the community.
<div id='section'>Paperid: <span id='pid'>234, <a href='https://arxiv.org/pdf/2310.20491.pdf' target='_blank'>https://arxiv.org/pdf/2310.20491.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peng Gao, Yu Shen, Ming C. Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.20491">Collaborative Decision-Making Using Spatiotemporal Graphs in Connected Autonomy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Collaborative decision-making is an essential capability for multi-robot systems, such as connected vehicles, to collaboratively control autonomous vehicles in accident-prone scenarios. Under limited communication bandwidth, capturing comprehensive situational awareness by integrating connected agents' observation is very challenging. In this paper, we propose a novel collaborative decision-making method that efficiently and effectively integrates collaborators' representations to control the ego vehicle in accident-prone scenarios. Our approach formulates collaborative decision-making as a classification problem. We first represent sequences of raw observations as spatiotemporal graphs, which significantly reduce the package size to share among connected vehicles. Then we design a novel spatiotemporal graph neural network based on heterogeneous graph learning, which analyzes spatial and temporal connections of objects in a unified way for collaborative decision-making. We evaluate our approach using a high-fidelity simulator that considers realistic traffic, communication bandwidth, and vehicle sensing among connected autonomous vehicles. The experimental results show that our representation achieves over 100x reduction in the shared data size that meets the requirements of communication bandwidth for connected autonomous driving. In addition, our approach achieves over 30% improvements in driving safety.
<div id='section'>Paperid: <span id='pid'>235, <a href='https://arxiv.org/pdf/2310.11805.pdf' target='_blank'>https://arxiv.org/pdf/2310.11805.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Khattiya Pongsirijinda, Zhiqiang Cao, Muhammad Shalihan, Benny Kai Kiat Ng, Billy Pik Lik Lau, Chau Yuen, U-Xuan Tan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.11805">GMC-Pos: Graph-Based Multi-Robot Coverage Positioning Method</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Nowadays, several real-world tasks require adequate environment coverage for maintaining communication between multiple robots, for example, target search tasks, environmental monitoring, and post-disaster rescues. In this study, we look into a situation where there are a human operator and multiple robots, and we assume that each human or robot covers a certain range of areas. We want them to maximize their area of coverage collectively. Therefore, in this paper, we propose the Graph-Based Multi-Robot Coverage Positioning Method (GMC-Pos) to find strategic positions for robots that maximize the area coverage. Our novel approach consists of two main modules: graph generation and node selection. Firstly, graph generation represents the environment using a weighted connected graph. Then, we present a novel generalized graph-based distance and utilize it together with the graph degrees to be the conditions for node selection in a recursive manner. Our method is deployed in three environments with different settings. The results show that it outperforms the benchmark method by 15.13% to 24.88% regarding the area coverage percentage.
<div id='section'>Paperid: <span id='pid'>236, <a href='https://arxiv.org/pdf/2310.08686.pdf' target='_blank'>https://arxiv.org/pdf/2310.08686.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohammed Ayman Shalaby, Charles Champagne Cossette, Jerome Le Ny, James Richard Forbes
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.08686">IMU Preintegration for Multi-Robot Systems in the Presence of Bias and Communication Constraints</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This document is in supplement to the paper titled "Multi-Robot Relative Pose Estimation and IMU Preintegration Using Passive UWB Transceivers", available at [1]. The purpose of this document is to show how IMU biases can be incorporated into the framework presented in [1], while maintaining the differential Sylvester equation form of the process model.
<div id='section'>Paperid: <span id='pid'>237, <a href='https://arxiv.org/pdf/2310.01573.pdf' target='_blank'>https://arxiv.org/pdf/2310.01573.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gian Carlo Maffettone, Lorenzo Liguori, Eduardo Palermo, Mario di Bernardo, Maurizio Porfiri
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.01573">Mixed Reality Environment and High-Dimensional Continuification Control for Swarm Robotics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Many new methodologies for the control of large-scale multi-agent systems are based on macroscopic representations of the emerging system dynamics, in the form of continuum approximations of large ensembles. These techniques, that are developed in the limit case of an infinite number of agents, are usually validated only through numerical simulations. In this paper, we introduce a mixed reality set-up for testing swarm robotics techniques, focusing on the macroscopic collective motion of robotic swarms. This hybrid apparatus combines both real differential drive robots and virtual agents to create a heterogeneous swarm of tunable size. We also extend continuification-based control methods for swarms to higher dimensions, and assess experimentally their validity in the new platform. Our study demonstrates the effectiveness of the platform for conducting large-scale swarm robotics experiments, and it contributes new theoretical insights into control algorithms exploiting continuification approaches.
<div id='section'>Paperid: <span id='pid'>238, <a href='https://arxiv.org/pdf/2309.17433.pdf' target='_blank'>https://arxiv.org/pdf/2309.17433.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dipam Patel, Phu Pham, Kshitij Tiwari, Aniket Bera
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.17433">DREAM: Decentralized Reinforcement Learning for Exploration and Efficient Energy Management in Multi-Robot Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Resource-constrained robots often suffer from energy inefficiencies, underutilized computational abilities due to inadequate task allocation, and a lack of robustness in dynamic environments, all of which strongly affect their performance. This paper introduces DREAM - Decentralized Reinforcement Learning for Exploration and Efficient Energy Management in Multi-Robot Systems, a comprehensive framework that optimizes the allocation of resources for efficient exploration. It advances beyond conventional heuristic-based task planning as observed conventionally. The framework incorporates Operational Range Estimation using Reinforcement Learning to perform exploration and obstacle avoidance in unfamiliar terrains. DREAM further introduces an Energy Consumption Model for goal allocation, thereby ensuring mission completion under constrained resources using a Graph Neural Network. This approach also ensures that the entire Multi-Robot System can survive for an extended period of time for further missions compared to the conventional approach of randomly allocating goals, which compromises one or more agents. Our approach adapts to prioritizing agents in real-time, showcasing remarkable resilience against dynamic environments. This robust solution was evaluated in various simulated environments, demonstrating adaptability and applicability across diverse scenarios. We observed a substantial improvement of about 25% over the baseline method, leading the way for future research in resource-constrained robotics.
<div id='section'>Paperid: <span id='pid'>239, <a href='https://arxiv.org/pdf/2309.15049.pdf' target='_blank'>https://arxiv.org/pdf/2309.15049.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Enrico Saccon, Ahmet Tikna, Davide De Martini, Edoardo Lamon, Marco Roveri, Luigi Palopoli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.15049">When Prolog meets generative models: a new approach for managing knowledge and planning in robotic applications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose a robot oriented knowledge management system based on the use of the Prolog language. Our framework hinges on a special organisation of knowledge base that enables: 1. its efficient population from natural language texts using semi-automated procedures based on Large Language Models, 2. the bumpless generation of temporal parallel plans for multi-robot systems through a sequence of transformations, 3. the automated translation of the plan into an executable formalism (the behaviour trees). The framework is supported by a set of open source tools and is shown on a realistic application.
<div id='section'>Paperid: <span id='pid'>240, <a href='https://arxiv.org/pdf/2307.10614.pdf' target='_blank'>https://arxiv.org/pdf/2307.10614.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ehsan Latif, Ramviyas Parasuraman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.10614">HGP-RL: Distributed Hierarchical Gaussian Processes for Wi-Fi-based Relative Localization in Multi-Robot Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Relative localization is crucial for multi-robot systems to perform cooperative tasks, especially in GPS-denied environments. Current techniques for multi-robot relative localization rely on expensive or short-range sensors such as cameras and LIDARs. As a result, these algorithms face challenges such as high computational complexity (e.g., map merging), dependencies on well-structured environments, etc. To remedy this gap, we propose a new distributed approach to perform relative localization (RL) using a common Access Point (AP). To achieve this efficiently, we propose a novel Hierarchical Gaussian Processes (HGP) mapping of the Radio Signal Strength Indicator (RSSI) values from a Wi-Fi AP to which the robots are connected. Each robot performs hierarchical inference using the HGP map to locate the AP in its reference frame, and the robots obtain relative locations of the neighboring robots leveraging AP-oriented algebraic transformations. The approach readily applies to resource-constrained devices and relies only on the ubiquitously-available WiFi RSSI measurement. We extensively validate the performance of the proposed HGR-PL in Robotarium simulations against several state-of-the-art methods. The results indicate superior performance of HGP-RL regarding localization accuracy, computation, and communication overheads. Finally, we showcase the utility of HGP-RL through a multi-robot cooperative experiment to achieve a rendezvous task in a team of three mobile robots.
<div id='section'>Paperid: <span id='pid'>241, <a href='https://arxiv.org/pdf/2306.12623.pdf' target='_blank'>https://arxiv.org/pdf/2306.12623.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ehsan Latif, Ramviyas Parasuraman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.12623">SEAL: Simultaneous Exploration and Localization in Multi-Robot Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The availability of accurate localization is critical for multi-robot exploration strategies; noisy or inconsistent localization causes failure in meeting exploration objectives. We aim to achieve high localization accuracy with contemporary exploration map belief and vice versa without needing global localization information. This paper proposes a novel simultaneous exploration and localization (SEAL) approach, which uses Gaussian Processes (GP)-based information fusion for maximum exploration while performing communication graph optimization for relative localization. Both these cross-dependent objectives were integrated through the Rao-Blackwellization technique. Distributed linearized convex hull optimization is used to select the next-best unexplored region for distributed exploration. SEAL outperformed cutting-edge methods on exploration and localization performance in extensive ROS-Gazebo simulations, illustrating the practicality of the approach in real-world applications.
<div id='section'>Paperid: <span id='pid'>242, <a href='https://arxiv.org/pdf/2302.07341.pdf' target='_blank'>https://arxiv.org/pdf/2302.07341.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongchao Zhang, Zhouchi Li, Shiyu Cheng, Andrew Clark
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.07341">Cooperative Perception for Safe Control of Autonomous Vehicles under LiDAR Spoofing Attacks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous vehicles rely on LiDAR sensors to detect obstacles such as pedestrians, other vehicles, and fixed infrastructures. LiDAR spoofing attacks have been demonstrated that either create erroneous obstacles or prevent detection of real obstacles, resulting in unsafe driving behaviors. In this paper, we propose an approach to detect and mitigate LiDAR spoofing attacks by leveraging LiDAR scan data from other neighboring vehicles. This approach exploits the fact that spoofing attacks can typically only be mounted on one vehicle at a time, and introduce additional points into the victim's scan that can be readily detected by comparison from other, non-modified scans. We develop a Fault Detection, Identification, and Isolation procedure that identifies non-existing obstacle, physical removal, and adversarial object attacks, while also estimating the actual locations of obstacles. We propose a control algorithm that guarantees that these estimated object locations are avoided. We validate our framework using a CARLA simulation study, in which we verify that our FDII algorithm correctly detects each attack pattern.
<div id='section'>Paperid: <span id='pid'>243, <a href='https://arxiv.org/pdf/2509.26324.pdf' target='_blank'>https://arxiv.org/pdf/2509.26324.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruiyang Wang, Hao-Lun Hsu, David Hunt, Shaocheng Luo, Jiwoo Kim, Miroslav Pajic
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26324">LLM-MCoX: Large Language Model-based Multi-robot Coordinated Exploration and Search</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous exploration and object search in unknown indoor environments remain challenging for multi-robot systems (MRS). Traditional approaches often rely on greedy frontier assignment strategies with limited inter-robot coordination. In this work, we introduce LLM-MCoX (LLM-based Multi-robot Coordinated Exploration and Search), a novel framework that leverages Large Language Models (LLMs) for intelligent coordination of both homogeneous and heterogeneous robot teams tasked with efficient exploration and target object search. Our approach combines real-time LiDAR scan processing for frontier cluster extraction and doorway detection with multimodal LLM reasoning (e.g., GPT-4o) to generate coordinated waypoint assignments based on shared environment maps and robot states. LLM-MCoX demonstrates superior performance compared to existing methods, including greedy and Voronoi-based planners, achieving 22.7% faster exploration times and 50% improved search efficiency in large environments with 6 robots. Notably, LLM-MCoX enables natural language-based object search capabilities, allowing human operators to provide high-level semantic guidance that traditional algorithms cannot interpret.
<div id='section'>Paperid: <span id='pid'>244, <a href='https://arxiv.org/pdf/2509.26324.pdf' target='_blank'>https://arxiv.org/pdf/2509.26324.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruiyang Wang, Hao-Lun Hsu, David Hunt, Shaocheng Luo, Jiwoo Kim, Miroslav Pajic
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26324">LLM-MCoX: Large Language Model-based Multi-robot Coordinated Exploration and Search</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous exploration and object search in unknown indoor environments remain challenging for multi-robot systems (MRS). Traditional approaches often rely on greedy frontier assignment strategies with limited inter-robot coordination. In this work, we introduce LLM-MCoX (LLM-based Multi-robot Coordinated Exploration and Search), a novel framework that leverages Large Language Models (LLMs) for intelligent coordination of both homogeneous and heterogeneous robot teams tasked with efficient exploration and target object search. Our approach combines real-time LiDAR scan processing for frontier cluster extraction and doorway detection with multimodal LLM reasoning (e.g., GPT-4o) to generate coordinated waypoint assignments based on shared environment maps and robot states. LLM-MCoX demonstrates superior performance compared to existing methods, including greedy and Voronoi-based planners, achieving 22.7% faster exploration times and 50% improved search efficiency in large environments with 6 robots. Notably, LLM-MCoX enables natural language-based object search capabilities, allowing human operators to provide high-level semantic guidance that traditional algorithms cannot interpret.
<div id='section'>Paperid: <span id='pid'>245, <a href='https://arxiv.org/pdf/2508.02287.pdf' target='_blank'>https://arxiv.org/pdf/2508.02287.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Markus Buchholz, Ignacio Carlucho, Zebin Huang, Michele Grimaldi, Pierre Nicolay, Sumer Tuncay, Yvan R. Petillot
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02287">Framework for Robust Motion Planning of Tethered Multi-Robot Systems in Marine Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces CoralGuide, a novel framework designed for path planning and trajectory optimization for tethered multi-robot systems. We focus on marine robotics, which commonly have tethered configurations of an Autonomous Surface Vehicle (ASV) and an Autonomous Underwater Vehicle (AUV). CoralGuide provides safe navigation in marine environments by enhancing the A* algorithm with specialized heuristics tailored for tethered ASV-AUV systems. Our method integrates catenary curve modelling for tether management and employs Bezier curve interpolation for smoother trajectory planning, ensuring efficient and synchronized operations without compromising safety. Through simulations and real-world experiments, we have validated CoralGuides effectiveness in improving path planning and trajectory optimization, demonstrating its potential to significantly enhance operational capabilities in marine research and infrastructure inspection.
<div id='section'>Paperid: <span id='pid'>246, <a href='https://arxiv.org/pdf/2508.02264.pdf' target='_blank'>https://arxiv.org/pdf/2508.02264.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Markus Buchholz, Ignacio Carlucho, Michele Grimaldi, Yvan R. Petillot
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02264">Tethered Multi-Robot Systems in Marine Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces a novel simulation framework for evaluating motion control in tethered multi-robot systems within dynamic marine environments. Specifically, it focuses on the coordinated operation of an Autonomous Underwater Vehicle (AUV) and an Autonomous Surface Vehicle(ASV). The framework leverages GazeboSim, enhanced with realistic marine environment plugins and ArduPilots SoftwareIn-The-Loop (SITL) mode, to provide a high-fidelity simulation platform. A detailed tether model, combining catenary equations and physical simulation, is integrated to accurately represent the dynamic interactions between the vehicles and the environment. This setup facilitates the development and testing of advanced control strategies under realistic conditions, demonstrating the frameworks capability to analyze complex tether interactions and their impact on system performance.
<div id='section'>Paperid: <span id='pid'>247, <a href='https://arxiv.org/pdf/2504.03126.pdf' target='_blank'>https://arxiv.org/pdf/2504.03126.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tohid Kargar Tasooji, Sakineh Khodadadi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.03126">Distributed Linear Quadratic Gaussian for Multi-Robot Coordination with Localization Uncertainty</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper addresses the problem of distributed coordination control for multi-robot systems (MRSs) in the presence of localization uncertainty using a Linear Quadratic Gaussian (LQG) approach. We introduce a stochastic LQG control strategy that ensures the coordination of mobile robots while optimizing a performance criterion. The proposed control framework accounts for the inherent uncertainty in localization measurements, enabling robust decision-making and coordination. We analyze the stability of the system under the proposed control protocol, deriving conditions for the convergence of the multi-robot network. The effectiveness of the proposed approach is demonstrated through experimental validation using Robotrium simulation experiments, showcasing the practical applicability of the control strategy in real-world scenarios with localization uncertainty.
<div id='section'>Paperid: <span id='pid'>248, <a href='https://arxiv.org/pdf/2503.20723.pdf' target='_blank'>https://arxiv.org/pdf/2503.20723.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tohid Kargar Tasooji, Sakineh Khodadadi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.20723">Multi-Robot Coordination Under Physical Limitations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-robot coordination is fundamental to various applications, including autonomous exploration, search and rescue, and cooperative transportation. This paper presents an optimal consensus framework for multi-robot systems (MRSs) that ensures efficient rendezvous while minimizing energy consumption and addressing actuator constraints. A critical challenge in real-world deployments is actuator limitations, particularly wheel velocity saturation, which can significantly degrade control performance. To address this issue, we incorporate Pontryagin Minimum Principle (PMP) into the control design, facilitating constrained optimization while ensuring system stability and feasibility. The resulting optimal control policy effectively balances coordination efficiency and energy consumption, even in the presence of actuation constraints. The proposed framework is validated through extensive numerical simulations and real-world experiments conducted using a team of Robotarium mobile robots. The experimental results confirm that our control strategies achieve reliable and efficient coordinated rendezvous while addressing real-world challenges such as communication delays, sensor noise, and packet loss.
<div id='section'>Paperid: <span id='pid'>249, <a href='https://arxiv.org/pdf/2503.18192.pdf' target='_blank'>https://arxiv.org/pdf/2503.18192.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ahmad Sarlak, Rahul Amin, Abolfazl Razi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.18192">Extended Visibility of Autonomous Vehicles via Optimized Cooperative Perception under Imperfect Communication</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous Vehicles (AVs) rely on individual perception systems to navigate safely. However, these systems face significant challenges in adverse weather conditions, complex road geometries, and dense traffic scenarios. Cooperative Perception (CP) has emerged as a promising approach to extending the perception quality of AVs by jointly processing shared camera feeds and sensor readings across multiple vehicles. This work presents a novel CP framework designed to optimize vehicle selection and networking resource utilization under imperfect communications. Our optimized CP formation considers critical factors such as the helper vehicles' spatial position, visual range, motion blur, and available communication budgets. Furthermore, our resource optimization module allocates communication channels while adjusting power levels to maximize data flow efficiency between the ego and helper vehicles, considering realistic models of modern vehicular communication systems, such as LTE and 5G NR-V2X. We validate our approach through extensive experiments on pedestrian detection in challenging scenarios, using synthetic data generated by the CARLA simulator. The results demonstrate that our method significantly improves upon the perception quality of individual AVs with about 10% gain in detection accuracy. This substantial gain uncovers the unleashed potential of CP to enhance AV safety and performance in complex situations.
<div id='section'>Paperid: <span id='pid'>250, <a href='https://arxiv.org/pdf/2502.16531.pdf' target='_blank'>https://arxiv.org/pdf/2502.16531.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Davide Peron, Victor Nan Fernandez-Ayala, Eleftherios E. Vlahakis, Dimos V. Dimarogonas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.16531">Efficient Coordination and Synchronization of Multi-Robot Systems Under Recurring Linear Temporal Logic</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We consider multi-robot systems under recurring tasks formalized as linear temporal logic (LTL) specifications. To solve the planning problem efficiently, we propose a bottom-up approach combining offline plan synthesis with online coordination, dynamically adjusting plans via real-time communication. To address action delays, we introduce a synchronization mechanism ensuring coordinated task execution, leading to a multi-agent coordination and synchronization framework that is adaptable to a wide range of multi-robot applications. The software package is developed in Python and ROS2 for broad deployment. We validate our findings through lab experiments involving nine robots showing enhanced adaptability compared to previous methods. Additionally, we conduct simulations with up to ninety agents to demonstrate the reduced computational complexity and the scalability features of our work.
<div id='section'>Paperid: <span id='pid'>251, <a href='https://arxiv.org/pdf/2411.18519.pdf' target='_blank'>https://arxiv.org/pdf/2411.18519.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Prajit KrisshnaKumar, Steve Paul, Souma Chowdhury
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.18519">A Talent-infused Policy-gradient Approach to Efficient Co-Design of Morphology and Task Allocation Behavior of Multi-Robot Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Interesting and efficient collective behavior observed in multi-robot or swarm systems emerges from the individual behavior of the robots. The functional space of individual robot behaviors is in turn shaped or constrained by the robot's morphology or physical design. Thus the full potential of multi-robot systems can be realized by concurrently optimizing the morphology and behavior of individual robots, informed by the environment's feedback about their collective performance, as opposed to treating morphology and behavior choices disparately or in sequence (the classical approach). This paper presents an efficient concurrent design or co-design method to explore this potential and understand how morphology choices impact collective behavior, particularly in an MRTA problem focused on a flood response scenario, where the individual behavior is designed via graph reinforcement learning. Computational efficiency in this case is attributed to a new way of near exact decomposition of the co-design problem into a series of simpler optimization and learning problems. This is achieved through i) the identification and use of the Pareto front of Talent metrics that represent morphology-dependent robot capabilities, and ii) learning the selection of Talent best trade-offs and individual robot policy that jointly maximizes the MRTA performance. Applied to a multi-unmanned aerial vehicle flood response use case, the co-design outcomes are shown to readily outperform sequential design baselines. Significant differences in morphology and learned behavior are also observed when comparing co-designed single robot vs. co-designed multi-robot systems for similar operations.
<div id='section'>Paperid: <span id='pid'>252, <a href='https://arxiv.org/pdf/2410.11966.pdf' target='_blank'>https://arxiv.org/pdf/2410.11966.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Abhinav Chakraborty, Pritam Goswami, Satakshi Ghosh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.11966">Min-Max Gathering on Infinite Grid</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Gathering is a fundamental coordination problem in swarm robotics, where the objective is to bring robots together at a point not known to them at the beginning. While most research focuses on continuous domains, some studies also examine the discrete domain. This paper addresses the optimal gathering problem on an infinite grid, aiming to improve the energy efficiency by minimizing the maximum distance any robot must travel. The robots are autonomous, anonymous, homogeneous, identical, and oblivious. We identify all initial configurations where the optimal gathering problem is unsolvable. For the remaining configurations, we introduce a deterministic distributed algorithm that effectively gathers $n$ robots ($n\ge 9$). The algorithm ensures that the robots gathers at one of the designated min-max nodes in the grid. Additionally, we provide a comprehensive characterization of the subgraph formed by the min-max nodes in this infinite grid model.
<div id='section'>Paperid: <span id='pid'>253, <a href='https://arxiv.org/pdf/2409.00766.pdf' target='_blank'>https://arxiv.org/pdf/2409.00766.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Robinroy Peter, Lavanya Ratnabala, Eugene Yugarajah Andrew Charles, Dzmitry Tsetserukou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.00766">Dynamic Subgoal based Path Formation and Task Allocation: A NeuroFleets Approach to Scalable Swarm Robotics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper addresses the challenges of exploration and navigation in unknown environments from the perspective of evolutionary swarm robotics. A key focus is on path formation, which is essential for enabling cooperative swarm robots to navigate effectively. We designed the task allocation and path formation process based on a finite state machine, ensuring systematic decision-making and efficient state transitions. The approach is decentralized, allowing each robot to make decisions independently based on local information, which enhances scalability and robustness. We present a novel subgoal-based path formation method that establishes paths between locations by leveraging visually connected subgoals. Simulation experiments conducted in the Argos simulator show that this method successfully forms paths in the majority of trials. However, inter-collision (traffic) among numerous robots during path formation can negatively impact performance. To address this issue, we propose a task allocation strategy that uses local communication protocols and light signal-based communication to manage robot deployment. This strategy assesses the distance between points and determines the optimal number of robots needed for the path formation task, thereby reducing unnecessary exploration and traffic congestion. The performance of both the subgoal-based path formation method and the task allocation strategy is evaluated by comparing the path length, time, and resource usage against the A* algorithm. Simulation results demonstrate the effectiveness of our approach, highlighting its scalability, robustness, and fault tolerance.
<div id='section'>Paperid: <span id='pid'>254, <a href='https://arxiv.org/pdf/2408.06553.pdf' target='_blank'>https://arxiv.org/pdf/2408.06553.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aryo Jamshidpey, Mostafa Wahby, Michael Allwright, Weixu Zhu, Marco Dorigo, Mary Katherine Heinrich
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.06553">Centralization vs. decentralization in multi-robot sweep coverage with ground robots and UAVs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In swarm robotics, decentralized control is often proposed as a more scalable and fault-tolerant alternative to centralized control. However, centralized behaviors are often faster and more efficient than their decentralized counterparts. In any given application, the goals and constraints of the task being solved should guide the choice to use centralized control, decentralized control, or a combination of the two. Currently, the exact trade-offs that exist between centralization and decentralization are not well defined. In this paper, we compare the performance of centralization and decentralization in the example task of sweep coverage, across five different types of multi-robot control structures: random walk, decentralized with beacons, hybrid formation control using self-organizing hierarchy, centralized formation control, and predetermined. In all five approaches, the coverage task is completed by a group of ground robots. In each approach, except for the random walk, the ground robots are assisted by UAVs, acting as supervisors or beacons. We compare the approaches in terms of three performance metrics for which centralized approaches are expected to have an advantage -- coverage completeness, coverage uniformity, and sweep completion time -- and two metrics for which decentralized approaches are expected to have an advantage -- scalability (4, 8, or 16 ground robots) and fault tolerance (0%, 25%, 50%, or 75% ground robot failure).
<div id='section'>Paperid: <span id='pid'>255, <a href='https://arxiv.org/pdf/2408.06553.pdf' target='_blank'>https://arxiv.org/pdf/2408.06553.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aryo Jamshidpey, Mostafa Wahby, Michael Allwright, Weixu Zhu, Marco Dorigo, Mary Katherine Heinrich
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.06553">Centralization vs. decentralization in multi-robot sweep coverage with ground robots and UAVs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In swarm robotics, decentralized control is often proposed as a more scalable and fault-tolerant alternative to centralized control. However, centralized behaviors are often faster and more efficient than their decentralized counterparts. In any given application, the goals and constraints of the task being solved should guide the choice to use centralized control, decentralized control, or a combination of the two. Currently, the exact trade-offs that exist between centralization and decentralization are not well defined. In this paper, we compare the performance of centralization and decentralization in the example task of sweep coverage, across five different types of multi-robot control structures: random walk, decentralized with beacons, hybrid formation control using self-organizing hierarchy, centralized formation control, and predetermined. In all five approaches, the coverage task is completed by a group of ground robots. In each approach, except for the random walk, the ground robots are assisted by UAVs, acting as supervisors or beacons. We compare the approaches in terms of three performance metrics for which centralized approaches are expected to have an advantage -- coverage completeness, coverage uniformity, and sweep completion time -- and two metrics for which decentralized approaches are expected to have an advantage -- scalability (4, 8, or 16 ground robots) and fault tolerance (0%, 25%, 50%, or 75% ground robot failure).
<div id='section'>Paperid: <span id='pid'>256, <a href='https://arxiv.org/pdf/2408.06553.pdf' target='_blank'>https://arxiv.org/pdf/2408.06553.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aryo Jamshidpey, Mostafa Wahby, Michael Allwright, Weixu Zhu, Marco Dorigo, Mary Katherine Heinrich
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.06553">Centralization vs. decentralization in multi-robot sweep coverage with ground robots and UAVs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In swarm robotics, decentralized control is often proposed as a more scalable and fault-tolerant alternative to centralized control. However, centralized behaviors are often faster and more efficient than their decentralized counterparts. In any given application, the goals and constraints of the task being solved should guide the choice to use centralized control, decentralized control, or a combination of the two. Currently, the exact trade-offs that exist between centralization and decentralization are not well defined. In this paper, we compare the performance of centralization and decentralization in the example task of sweep coverage, across five different types of multi-robot control structures: random walk, decentralized with beacons, hybrid formation control using self-organizing hierarchy, centralized formation control, and predetermined. In all five approaches, the coverage task is completed by a group of ground robots. In each approach, except for the random walk, the ground robots are assisted by UAVs, acting as supervisors or beacons. We compare the approaches in terms of three performance metrics for which centralized approaches are expected to have an advantage -- coverage completeness, coverage uniformity, and sweep completion time -- and two metrics for which decentralized approaches are expected to have an advantage -- scalability (4, 8, or 16 ground robots) and fault tolerance (0%, 25%, 50%, or 75% ground robot failure).
<div id='section'>Paperid: <span id='pid'>257, <a href='https://arxiv.org/pdf/2408.02605.pdf' target='_blank'>https://arxiv.org/pdf/2408.02605.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Thomas G. Kelly, Mohammad D. Soorati, Klaus-Peter Zauner, Sarvapali D. Ramchurn, and Danesh Tarapore
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.02605">Trade-offs of Dynamic Control Structure in Human-swarm Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Swarm robotics is a study of simple robots that exhibit complex behaviour only by interacting locally with other robots and their environment. The control in swarm robotics is mainly distributed whereas centralised control is widely used in other fields of robotics. Centralised and decentralised control strategies both pose a unique set of benefits and drawbacks for the control of multi-robot systems. While decentralised systems are more scalable and resilient, they are less efficient compared to the centralised systems and they lead to excessive data transmissions to the human operators causing cognitive overload. We examine the trade-offs of each of these approaches in a human-swarm system to perform an environmental monitoring task and propose a flexible hybrid approach, which combines elements of hierarchical and decentralised systems. We find that a flexible hybrid system can outperform a centralised system (in our environmental monitoring task by 19.2%) while reducing the number of messages sent to a human operator (here by 23.1%). We conclude that establishing centralisation for a system is not always optimal for performance and that utilising aspects of centralised and decentralised systems can keep the swarm from hindering its performance.
<div id='section'>Paperid: <span id='pid'>258, <a href='https://arxiv.org/pdf/2407.11592.pdf' target='_blank'>https://arxiv.org/pdf/2407.11592.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ayomide O. Agunloye, Sarvapali D. Ramchurn, Mohammad D. Soorati
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.11592">Learning to Imitate Spatial Organization in Multi-robot Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding collective behavior and how it evolves is important to ensure that robot swarms can be trusted in a shared environment. One way to understand the behavior of the swarm is through collective behavior reconstruction using prior demonstrations. Existing approaches often require access to the swarm controller which may not be available. We reconstruct collective behaviors in distinct swarm scenarios involving shared environments without using swarm controller information. We achieve this by transforming prior demonstrations into features that describe multi-agent interactions before behavior reconstruction with multi-agent generative adversarial imitation learning (MA-GAIL). We show that our approach outperforms existing algorithms in spatial organization, and can be used to observe and reconstruct a swarm's behavior for further analysis and testing, which might be impractical or undesirable on the original robot swarm.
<div id='section'>Paperid: <span id='pid'>259, <a href='https://arxiv.org/pdf/2407.06541.pdf' target='_blank'>https://arxiv.org/pdf/2407.06541.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Arif Kerem DayÄ±, Orhan Eren AkgÃ¼n, Stephanie Gil, Michal Yemini, Angelia NediÄ
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.06541">Fast Distributed Optimization over Directed Graphs under Malicious Attacks using Trust</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we introduce the Resilient Projected Push-Pull (RP3) algorithm designed for distributed optimization in multi-agent cyber-physical systems with directed communication graphs and the presence of malicious agents. Our algorithm leverages stochastic inter-agent trust values and gradient tracking to achieve geometric convergence rates in expectation even in adversarial environments. We introduce growing constraint sets to limit the impact of the malicious agents without compromising the geometric convergence rate of the algorithm. We prove that RP3 converges to the nominal optimal solution almost surely and in the $r$-th mean for any $r\geq 1$, provided the step sizes are sufficiently small and the constraint sets are appropriately chosen. We validate our approach with numerical studies on average consensus and multi-robot target tracking problems, demonstrating that RP3 effectively mitigates the impact of malicious agents and achieves the desired geometric convergence.
<div id='section'>Paperid: <span id='pid'>260, <a href='https://arxiv.org/pdf/2406.10199.pdf' target='_blank'>https://arxiv.org/pdf/2406.10199.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guangyao Shi, Gaurav S. Sukhatme
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.10199">Inverse Risk-sensitive Multi-Robot Task Allocation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We consider a new variant of the multi-robot task allocation problem - Inverse Risk-sensitive Multi-Robot Task Allocation (IR-MRTA).
  "Forward" MRTA - the process of deciding which robot should perform a task given the reward (cost)-related parameters, is widely studied in the multi-robot literature. In this setting, the reward (cost)-related parameters are assumed to be already known: parameters are first fixed offline by domain experts, followed by coordinating robots online. What if we need these parameters to be adjusted by non-expert human supervisors who oversee the robots during tasks to adapt to new situations? We are interested in the case where the human supervisor's perception of the allocation risk may change and suggest different allocations for robots compared to that from the MRTA algorithm. In such cases, the robots need to change the parameters of the allocation problem based on evolving human preferences. We study such problems through the lens of inverse task allocation, i.e., the process of finding parameters given solutions to the problem. Specifically, we propose a new formulation IR-MRTA, in which we aim to find a new set of parameters of the human behavioral risk model that minimally deviates from the current MRTA parameters and can make a greedy task allocation algorithm allocate robot resources in line with those suggested by humans. We show that even in the simple case such a problem is a non-convex optimization problem. We propose a Branch $\&$ Bound algorithm (BB-IR-MRTA) to solve such problems. In numerical simulations of a case study on multi-robot target capture, we demonstrate how to use BB-IR-MRTA and we show that the proposed algorithm achieves significant advantages in running time and peak memory usage compared to a brute-force baseline.
<div id='section'>Paperid: <span id='pid'>261, <a href='https://arxiv.org/pdf/2406.04780.pdf' target='_blank'>https://arxiv.org/pdf/2406.04780.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Roberto Casadei, Gianluca Aguzzi, Giorgio Audrito, Ferruccio Damiani, Danilo Pianini, Giordano Scarso, Gianluca Torta, Mirko Viroli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.04780">Software Engineering for Collective Cyber-Physical Ecosystems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Today's distributed and pervasive computing addresses large-scale cyber-physical ecosystems, characterised by dense and large networks of devices capable of computation, communication and interaction with the environment and people. While most research focusses on treating these systems as "composites" (i.e., heterogeneous functional complexes), recent developments in fields such as self-organising systems and swarm robotics have opened up a complementary perspective: treating systems as "collectives" (i.e., uniform, collaborative, and self-organising groups of entities). This article explores the motivations, state of the art, and implications of this "collective computing paradigm" in software engineering, discusses its peculiar challenges, and outlines a path for future research, touching on aspects such as macroprogramming, collective intelligence, self-adaptive middleware, learning, synthesis, and experimentation of collective behaviour.
<div id='section'>Paperid: <span id='pid'>262, <a href='https://arxiv.org/pdf/2404.08013.pdf' target='_blank'>https://arxiv.org/pdf/2404.08013.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ahmad Sarlak, Hazim Alzorgan, Sayed Pedram Haeri Boroujeni, Abolfazl Razi, Rahul Amin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.08013">Enhanced Cooperative Perception for Autonomous Vehicles Using Imperfect Communication</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Sharing and joint processing of camera feeds and sensor measurements, known as Cooperative Perception (CP), has emerged as a new technique to achieve higher perception qualities. CP can enhance the safety of Autonomous Vehicles (AVs) where their individual visual perception quality is compromised by adverse weather conditions (haze as foggy weather), low illumination, winding roads, and crowded traffic. To cover the limitations of former methods, in this paper, we propose a novel approach to realize an optimized CP under constrained communications. At the core of our approach is recruiting the best helper from the available list of front vehicles to augment the visual range and enhance the Object Detection (OD) accuracy of the ego vehicle. In this two-step process, we first select the helper vehicles that contribute the most to CP based on their visual range and lowest motion blur. Next, we implement a radio block optimization among the candidate vehicles to further improve communication efficiency. We specifically focus on pedestrian detection as an exemplary scenario. To validate our approach, we used the CARLA simulator to create a dataset of annotated videos for different driving scenarios where pedestrian detection is challenging for an AV with compromised vision. Our results demonstrate the efficacy of our two-step optimization process in improving the overall performance of cooperative perception in challenging scenarios, substantially improving driving safety under adverse conditions. Finally, we note that the networking assumptions are adopted from LTE Release 14 Mode 4 side-link communication, commonly used for Vehicle-to-Vehicle (V2V) communication. Nonetheless, our method is flexible and applicable to arbitrary V2V communications.
<div id='section'>Paperid: <span id='pid'>263, <a href='https://arxiv.org/pdf/2403.16275.pdf' target='_blank'>https://arxiv.org/pdf/2403.16275.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ishaan Mehta, Junseo Kim, Sharareh Taghipour, Sajad Saeedi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.16275">M^3RS: Multi-robot, Multi-objective, and Multi-mode Routing and Scheduling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The quality of task execution can significantly impact a multi-robot mission. While higher quality is desirable, it may not always be feasible due to mission constraints. Existing multi-robot task allocation literature generally overlooks quality of service as a decision variable. Addressing this gap, we introduce the multi-robot, multi-objective, and multi-mode routing and scheduling (M^3RS) problem, designed for time-bound, multi-robot, multi-objective missions. In M^3RS, each task offers multiple execution modes, each with different resource requirements, execution time, and quality. M^3RS optimizes task sequences and execution modes for each agent. The need for M^3RS comes from multi-robot applications in which a trade-off between multiple criteria can be achieved by varying the task level quality of service through task execution modes. Such ability is particularly useful for service robot applications. We use M^3RS for the application of multi-robot disinfection in healthcare environments and other public locations. The objectives considered for disinfection application are disinfection quality and number of tasks completed. A mixed-integer linear programming (MIP) model is proposed for M^3RS. Further, a clustering-based column generation (CCG) algorithm is proposed to handle larger problem instances. Through synthetic, simulated, and hardware case studies, we demonstrate the advantages of M^3RS, showing it provides flexibility and strong performance across multiple metrics. Our CCG algorithm generates solutions 2.5x faster than a baseline MIP optimizer, maintaining competitive performance. The videos for the experiments are available on the project website: https://sites.google.com/view/g-robot/m3rs/
<div id='section'>Paperid: <span id='pid'>264, <a href='https://arxiv.org/pdf/2403.07131.pdf' target='_blank'>https://arxiv.org/pdf/2403.07131.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Steve Paul, Nathan Maurer, Souma Chowdhury
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.07131">Bigraph Matching Weighted with Learnt Incentive Function for Multi-Robot Task Allocation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Most real-world Multi-Robot Task Allocation (MRTA) problems require fast and efficient decision-making, which is often achieved using heuristics-aided methods such as genetic algorithms, auction-based methods, and bipartite graph matching methods. These methods often assume a form that lends better explainability compared to an end-to-end (learnt) neural network based policy for MRTA. However, deriving suitable heuristics can be tedious, risky and in some cases impractical if problems are too complex. This raises the question: can these heuristics be learned? To this end, this paper particularly develops a Graph Reinforcement Learning (GRL) framework to learn the heuristics or incentives for a bipartite graph matching approach to MRTA. Specifically a Capsule Attention policy model is used to learn how to weight task/robot pairings (edges) in the bipartite graph that connects the set of tasks to the set of robots. The original capsule attention network architecture is fundamentally modified by adding encoding of robots' state graph, and two Multihead Attention based decoders whose output are used to construct a LogNormal distribution matrix from which positive bigraph weights can be drawn. The performance of this new bigraph matching approach augmented with a GRL-derived incentive is found to be at par with the original bigraph matching approach that used expert-specified heuristics, with the former offering notable robustness benefits. During training, the learned incentive policy is found to get initially closer to the expert-specified incentive and then slightly deviate from its trend.
<div id='section'>Paperid: <span id='pid'>265, <a href='https://arxiv.org/pdf/2311.11047.pdf' target='_blank'>https://arxiv.org/pdf/2311.11047.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pablo Pueyo, Eduardo Montijano, Ana C. Murillo, Mac Schwager
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.11047">CLIPSwarm: Converting text into formations of robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present CLIPSwarm, an algorithm to generate robot swarm formations from natural language descriptions. CLIPSwarm receives an input text and finds the position of the robots to form a shape that corresponds to the given text. To do so, we implement a variation of the Montecarlo particle filter to obtain a matching formation iteratively. In every iteration, we generate a set of new formations and evaluate their Clip Similarity with the given text, selecting the best formations according to this metric. This metric is obtained using Clip, [1], an existing foundation model trained to encode images and texts into vectors within a common latent space. The comparison between these vectors determines how likely the given text describes the shapes. Our initial proof of concept shows the potential of this solution to generate robot swarm formations just from natural language descriptions and demonstrates a novel application of foundation models, such as CLIP, in the field of multi-robot systems. In this first approach, we create formations using a Convex-Hull approach. Next steps include more robust and generic representation and optimization steps in the process of obtaining a suitable swarm formation.
<div id='section'>Paperid: <span id='pid'>266, <a href='https://arxiv.org/pdf/2311.07082.pdf' target='_blank'>https://arxiv.org/pdf/2311.07082.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yan Gao, Chenggang Bai, Quan Quan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.07082">A Survey on Passing-through Control of Multi-Robot Systems in Cluttered Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This survey presents a comprehensive review of various methods and algorithms related to passing-through control of multi-robot systems in cluttered environments. Numerous studies have investigated this area, and we identify several avenues for enhancing existing methods. This survey describes some models of robots and commonly considered control objectives, followed by an in-depth analysis of four types of algorithms that can be employed for passing-through control: leader-follower formation control, multi-robot trajectory planning, control-based methods, and virtual tube planning and control. Furthermore, we conduct a comparative analysis of these techniques and provide some subjective and general evaluations.
<div id='section'>Paperid: <span id='pid'>267, <a href='https://arxiv.org/pdf/2310.12910.pdf' target='_blank'>https://arxiv.org/pdf/2310.12910.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yasin Findik, Hamid Osooli, Paul Robinette, Kshitij Jerath, S. Reza Ahmadzadeh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.12910">Influence of Team Interactions on Multi-Robot Cooperation: A Relational Network Perspective</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Relational networks within a team play a critical role in the performance of many real-world multi-robot systems. To successfully accomplish tasks that require cooperation and coordination, different agents (e.g., robots) necessitate different priorities based on their positioning within the team. Yet, many of the existing multi-robot cooperation algorithms regard agents as interchangeable and lack a mechanism to guide the type of cooperation strategy the agents should exhibit. To account for the team structure in cooperative tasks, we propose a novel algorithm that uses a relational network comprising inter-agent relationships to prioritize certain agents over others. Through appropriate design of the team's relational network, we can guide the cooperation strategy, resulting in the emergence of new behaviors that accomplish the specified task. We conducted six experiments in a multi-robot setting with a cooperative task. Our results demonstrate that the proposed method can effectively influence the type of solution that the algorithm converges to by specifying the relationships between the agents, making it a promising approach for tasks that require cooperation among agents with a specified team structure.
<div id='section'>Paperid: <span id='pid'>268, <a href='https://arxiv.org/pdf/2309.16098.pdf' target='_blank'>https://arxiv.org/pdf/2309.16098.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuhan Zhao, Quanyan Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.16098">Stackelberg Game-Theoretic Trajectory Guidance for Multi-Robot Systems with Koopman Operator</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Guided trajectory planning involves a leader robot strategically directing a follower robot to collaboratively reach a designated destination. However, this task becomes notably challenging when the leader lacks complete knowledge of the follower's decision-making model. There is a need for learning-based methods to effectively design the cooperative plan. To this end, we develop a Stackelberg game-theoretic approach based on the Koopman operator to address the challenge. We first formulate the guided trajectory planning problem through the lens of a dynamic Stackelberg game. We then leverage Koopman operator theory to acquire a learning-based linear system model that approximates the follower's feedback dynamics. Based on this learned model, the leader devises a collision-free trajectory to guide the follower using receding horizon planning. We use simulations to elaborate on the effectiveness of our approach in generating learning models that accurately predict the follower's multi-step behavior when compared to alternative learning techniques. Moreover, our approach successfully accomplishes the guidance task and notably reduces the leader's planning time to nearly half when contrasted with the model-based baseline method.
<div id='section'>Paperid: <span id='pid'>269, <a href='https://arxiv.org/pdf/2306.04570.pdf' target='_blank'>https://arxiv.org/pdf/2306.04570.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ofer Dagan, Tycho L. Cinquini, Luke Morrissey, Kristen Such, Nisar R. Ahmed, Christoffer Heckman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.04570">Towards Decentralized Heterogeneous Multi-Robot SLAM and Target Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In many robotics problems, there is a significant gain in collaborative information sharing between multiple robots, for exploration, search and rescue, tracking multiple targets, or mapping large environments. One of the key implicit assumptions when solving cooperative multi-robot problems is that all robots use the same (homogeneous) underlying algorithm. However, in practice, we want to allow collaboration between robots possessing different capabilities and that therefore must rely on heterogeneous algorithms. We present a system architecture and the supporting theory, to enable collaboration in a decentralized network of robots, where each robot relies on different estimation algorithms. To develop our approach, we focus on multi-robot simultaneous localization and mapping (SLAM) with multi-target tracking. Our theoretical framework builds on our idea of exploiting the conditional independence structure inherent to many robotics applications to separate between each robot's local inference (estimation) tasks and fuse only relevant parts of their non-equal, but overlapping probability density function (pdfs). We present a new decentralized graph-based approach to the multi-robot SLAM and tracking problem. We leverage factor graphs to split between different parts of the problem for efficient data sharing between robots in the network while enabling robots to use different local sparse landmark/dense/metric-semantic SLAM algorithms.
<div id='section'>Paperid: <span id='pid'>270, <a href='https://arxiv.org/pdf/2303.08933.pdf' target='_blank'>https://arxiv.org/pdf/2303.08933.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Steve Paul, Wenyuan Li, Brian Smyth, Yuzhou Chen, Yulia Gel, Souma Chowdhury
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.08933">Efficient Planning of Multi-Robot Collective Transport using Graph Reinforcement Learning with Higher Order Topological Abstraction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Efficient multi-robot task allocation (MRTA) is fundamental to various time-sensitive applications such as disaster response, warehouse operations, and construction. This paper tackles a particular class of these problems that we call MRTA-collective transport or MRTA-CT -- here tasks present varying workloads and deadlines, and robots are subject to flight range, communication range, and payload constraints. For large instances of these problems involving 100s-1000's of tasks and 10s-100s of robots, traditional non-learning solvers are often time-inefficient, and emerging learning-based policies do not scale well to larger-sized problems without costly retraining. To address this gap, we use a recently proposed encoder-decoder graph neural network involving Capsule networks and multi-head attention mechanism, and innovatively add topological descriptors (TD) as new features to improve transferability to unseen problems of similar and larger size. Persistent homology is used to derive the TD, and proximal policy optimization is used to train our TD-augmented graph neural network. The resulting policy model compares favorably to state-of-the-art non-learning baselines while being much faster. The benefit of using TD is readily evident when scaling to test problems of size larger than those used in training.
<div id='section'>Paperid: <span id='pid'>271, <a href='https://arxiv.org/pdf/2209.08401.pdf' target='_blank'>https://arxiv.org/pdf/2209.08401.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ofer Dagan, Tycho L. Cinquini, Nisar R. Ahmed
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.08401">Nonlinear Heterogeneous Bayesian Decentralized Data Fusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The factor graph decentralized data fusion (FG-DDF) framework was developed for the analysis and exploitation of conditional independence in {heterogeneous Bayesian decentralized fusion problems, in which robots update and fuse pdfs over different, but overlapping subsets of random states. This allows robots to efficiently use smaller probabilistic models and sparse message passing to accurately and scalably fuse relevant local parts of a larger global joint state pdf while accounting for data dependencies between robots. Whereas prior work required limiting assumptions about network connectivity and model linearity, this paper relaxes these to explore the applicability and robustness of FG-DDF in more general settings. We develop a new heterogeneous fusion rule which generalizes the homogeneous covariance intersection algorithm for such cases and test it in multi-robot tracking and localization scenarios with non-linear motion/observation models under communication dropouts. Simulation and hardware experiments show that, in practice, the FG-DDF continues to provide consistent filtered estimates under these more practical operating conditions, while reducing computation and communication costs by more than 99\%, thus enabling the design of scalable real-world multi-robot systems.
<div id='section'>Paperid: <span id='pid'>272, <a href='https://arxiv.org/pdf/2202.06071.pdf' target='_blank'>https://arxiv.org/pdf/2202.06071.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuda Chen, Meng Guo, Zhongkui Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2202.06071">Deadlock Resolution and Recursive Feasibility in MPC-based Multi-robot Trajectory Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Online collision-free trajectory generation within a shared workspace is fundamental for most multi-robot applications. However, many widely-used methods based on model predictive control (MPC) lack theoretical guarantees on the feasibility of underlying optimization. Furthermore, when applied in a distributed manner without a central coordinator, deadlocks often occur where several robots block each other indefinitely. Whereas heuristic methods such as introducing random perturbations exist, no profound analyses are given to validate these measures. Towards this end, we propose a systematic method called infinite-horizon model predictive control with deadlock resolution. The MPC is formulated as a convex optimization over the proposed modified buffered Voronoi with warning band. Based on this formulation, the condition of deadlocks is formally analyzed and proven to be analogous to a force equilibrium. A detection-resolution scheme is proposed, which can effectively detect deadlocks online before they even happen. Once detected, it utilizes an adaptive resolution scheme to resolve deadlocks, under which no stable deadlocks can exist under minor conditions. In addition, the proposed planning algorithm ensures recursive feasibility of the underlying optimization at each time step under both input and model constraints, is concurrent for all robots and requires only local communication. Comprehensive simulation and experiment studies are conducted over large-scale multi-robot systems. Significant improvements on success rate are reported, in comparison with other state-of-the-art methods and especially in crowded and high-speed scenarios.
<div id='section'>Paperid: <span id='pid'>273, <a href='https://arxiv.org/pdf/2106.13285.pdf' target='_blank'>https://arxiv.org/pdf/2106.13285.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ofer Dagan, Nisar R. Ahmed
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2106.13285">Factor Graphs for Heterogeneous Bayesian Decentralized Data Fusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper explores the use of factor graphs as an inference and analysis tool for Bayesian peer-to-peer decentralized data fusion. We propose a framework by which agents can each use local factor graphs to represent relevant partitions of a complex global joint probability distribution, thus allowing them to avoid reasoning over the entirety of a more complex model and saving communication as well as computation cost. This allows heterogeneous multi-robot systems to cooperate on a variety of real world, task oriented missions, where scalability and modularity are key. To develop the initial theory and analyze the limits of this approach, we focus our attention on static linear Gaussian systems in tree-structured networks and use Channel Filters (also represented by factor graphs) to explicitly track common information. We discuss how this representation can be used to describe various multi-robot applications and to design and analyze new heterogeneous data fusion algorithms. We validate our method in simulations of a multi-agent multi-target tracking and cooperative multi-agent mapping problems, and discuss the computation and communication gains of this approach.
<div id='section'>Paperid: <span id='pid'>274, <a href='https://arxiv.org/pdf/2007.13897.pdf' target='_blank'>https://arxiv.org/pdf/2007.13897.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tamzidul Mina, Shyam Sundar Kannan, Wonse Jo, Byung-Cheol Min
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2007.13897">Adaptive Workload Allocation for Multi-human Multi-robot Teams for Independent and Homogeneous Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-human multi-robot (MH-MR) systems have the ability to combine the potential advantages of robotic systems with those of having humans in the loop. Robotic systems contribute precision performance and long operation on repetitive tasks without tiring, while humans in the loop improve situational awareness and enhance decision-making abilities. A system's ability to adapt allocated workload to changing conditions and the performance of each individual (human and robot) during the mission is vital to maintaining overall system performance. Previous works from literature including market-based and optimization approaches have attempted to address the task/workload allocation problem with focus on maximizing the system output without regarding individual agent conditions, lacking in real-time processing and have mostly focused exclusively on multi-robot systems. Given the variety of possible combination of teams (autonomous robots and human-operated robots: any number of human operators operating any number of robots at a time) and the operational scale of MH-MR systems, development of a generalized framework of workload allocation has been a particularly challenging task. In this paper, we present such a framework for independent homogeneous missions, capable of adaptively allocating the system workload in relation to health conditions and work performances of human-operated and autonomous robots in real-time. The framework consists of removable modular function blocks ensuring its applicability to different MH-MR scenarios. A new workload transition function block ensures smooth transition without the workload change having adverse effects on individual agents. The effectiveness and scalability of the system's workload adaptability is validated by experiments applying the proposed framework in a MH-MR patrolling scenario with changing human and robot condition, and failing robots.
<div id='section'>Paperid: <span id='pid'>275, <a href='https://arxiv.org/pdf/2508.21205.pdf' target='_blank'>https://arxiv.org/pdf/2508.21205.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Usman A. Khan, Mouhacine Benosman, Wenliang Liu, Federico Pecora, Joseph W. Durham
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.21205">Multi-robot Path Planning and Scheduling via Model Predictive Optimal Transport (MPC-OT)</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose a novel methodology for path planning and scheduling for multi-robot navigation that is based on optimal transport theory and model predictive control. We consider a setup where $N$ robots are tasked to navigate to $M$ targets in a common space with obstacles. Mapping robots to targets first and then planning paths can result in overlapping paths that lead to deadlocks. We derive a strategy based on optimal transport that not only provides minimum cost paths from robots to targets but also guarantees non-overlapping trajectories. We achieve this by discretizing the space of interest into $K$ cells and by imposing a ${K\times K}$ cost structure that describes the cost of transitioning from one cell to another. Optimal transport then provides \textit{optimal and non-overlapping} cell transitions for the robots to reach the targets that can be readily deployed without any scheduling considerations. The proposed solution requires $\unicode{x1D4AA}(K^3\log K)$ computations in the worst-case and $\unicode{x1D4AA}(K^2\log K)$ for well-behaved problems. To further accommodate potentially overlapping trajectories (unavoidable in certain situations) as well as robot dynamics, we show that a temporal structure can be integrated into optimal transport with the help of \textit{replans} and \textit{model predictive control}.
<div id='section'>Paperid: <span id='pid'>276, <a href='https://arxiv.org/pdf/2508.21205.pdf' target='_blank'>https://arxiv.org/pdf/2508.21205.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Usman A. Khan, Mouhacine Benosman, Wenliang Liu, Federico Pecora, Joseph W. Durham
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.21205">Multi-robot Path Planning and Scheduling via Model Predictive Optimal Transport (MPC-OT)</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose a novel methodology for path planning and scheduling for multi-robot navigation that is based on optimal transport theory and model predictive control. We consider a setup where $N$ robots are tasked to navigate to $M$ targets in a common space with obstacles. Mapping robots to targets first and then planning paths can result in overlapping paths that lead to deadlocks. We derive a strategy based on optimal transport that not only provides minimum cost paths from robots to targets but also guarantees non-overlapping trajectories. We achieve this by discretizing the space of interest into $K$ cells and by imposing a ${K\times K}$ cost structure that describes the cost of transitioning from one cell to another. Optimal transport then provides \textit{optimal and non-overlapping} cell transitions for the robots to reach the targets that can be readily deployed without any scheduling considerations. The proposed solution requires $\unicode{x1D4AA}(K^3\log K)$ computations in the worst-case and $\unicode{x1D4AA}(K^2\log K)$ for well-behaved problems. To further accommodate potentially overlapping trajectories (unavoidable in certain situations) as well as robot dynamics, we show that a temporal structure can be integrated into optimal transport with the help of \textit{replans} and \textit{model predictive control}.
<div id='section'>Paperid: <span id='pid'>277, <a href='https://arxiv.org/pdf/2508.21205.pdf' target='_blank'>https://arxiv.org/pdf/2508.21205.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Usman A. Khan, Mouhacine Benosman, Wenliang Liu, Federico Pecora, Joseph W. Durham
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.21205">Multi-robot Path Planning and Scheduling via Model Predictive Optimal Transport (MPC-OT)</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose a novel methodology for path planning and scheduling for multi-robot navigation that is based on optimal transport theory and model predictive control. We consider a setup where $N$ robots are tasked to navigate to $M$ targets in a common space with obstacles. Mapping robots to targets first and then planning paths can result in overlapping paths that lead to deadlocks. We derive a strategy based on optimal transport that not only provides minimum cost paths from robots to targets but also guarantees non-overlapping trajectories. We achieve this by discretizing the space of interest into $K$ cells and by imposing a ${K\times K}$ cost structure that describes the cost of transitioning from one cell to another. Optimal transport then provides \textit{optimal and non-overlapping} cell transitions for the robots to reach the targets that can be readily deployed without any scheduling considerations. The proposed solution requires $\unicode{x1D4AA}(K^3\log K)$ computations in the worst-case and $\unicode{x1D4AA}(K^2\log K)$ for well-behaved problems. To further accommodate potentially overlapping trajectories (unavoidable in certain situations) as well as robot dynamics, we show that a temporal structure can be integrated into optimal transport with the help of \textit{replans} and \textit{model predictive control}.
<div id='section'>Paperid: <span id='pid'>278, <a href='https://arxiv.org/pdf/2507.07315.pdf' target='_blank'>https://arxiv.org/pdf/2507.07315.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ricardo Vega, Cameron Nowzari
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07315">Classifying Emergence in Robot Swarms: An Observer-Dependent Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Emergence and swarms are widely discussed topics, yet no consensus exists on their formal definitions. This lack of agreement makes it difficult not only for new researchers to grasp these concepts, but also for experts who may use the same terms to mean different things. Many attempts have been made to objectively define 'swarm' or 'emergence,' with recent work highlighting the role of the external observer. Still, several researchers argue that once an observer's vantage point (e.g., scope, resolution, context) is established, the terms can be made objective or measured quantitatively. In this note, we propose a framework to discuss these ideas rigorously by separating externally observable states from latent, unobservable ones. This allows us to compare and contrast existing definitions of swarms and emergence on common ground. We argue that these concepts are ultimately subjective-shaped less by the system itself than by the perception and tacit knowledge of the observer. Specifically, we suggest that a 'swarm' is not defined by its group behavior alone, but by the process generating that behavior. Our broader goal is to support the design and deployment of robotic swarm systems, highlighting the critical distinction between multi-robot systems and true swarms.
<div id='section'>Paperid: <span id='pid'>279, <a href='https://arxiv.org/pdf/2506.16710.pdf' target='_blank'>https://arxiv.org/pdf/2506.16710.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aditya Bhatt, Mary Katherine Corra, Franklin Merlo, Prajit KrisshnaKumar, Souma Chowdhury
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.16710">Experimental Setup and Software Pipeline to Evaluate Optimization based Autonomous Multi-Robot Search Algorithms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Signal source localization has been a problem of interest in the multi-robot systems domain given its applications in search & rescue and hazard localization in various industrial and outdoor settings. A variety of multi-robot search algorithms exist that usually formulate and solve the associated autonomous motion planning problem as a heuristic model-free or belief model-based optimization process. Most of these algorithms however remains tested only in simulation, thereby losing the opportunity to generate knowledge about how such algorithms would compare/contrast in a real physical setting in terms of search performance and real-time computing performance. To address this gap, this paper presents a new lab-scale physical setup and associated open-source software pipeline to evaluate and benchmark multi-robot search algorithms. The presented physical setup innovatively uses an acoustic source (that is safe and inexpensive) and small ground robots (e-pucks) operating in a standard motion-capture environment. This setup can be easily recreated and used by most robotics researchers. The acoustic source also presents interesting uncertainty in terms of its noise-to-signal ratio, which is useful to assess sim-to-real gaps. The overall software pipeline is designed to readily interface with any multi-robot search algorithm with minimal effort and is executable in parallel asynchronous form. This pipeline includes a framework for distributed implementation of multi-robot or swarm search algorithms, integrated with a ROS (Robotics Operating System)-based software stack for motion capture supported localization. The utility of this novel setup is demonstrated by using it to evaluate two state-of-the-art multi-robot search algorithms, based on swarm optimization and batch-Bayesian Optimization (called Bayes-Swarm), as well as a random walk baseline.
<div id='section'>Paperid: <span id='pid'>280, <a href='https://arxiv.org/pdf/2506.08807.pdf' target='_blank'>https://arxiv.org/pdf/2506.08807.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Luca Ballotta, Ãron VÃ©kÃ¡ssy, Stephanie Gil, Michal Yemini
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.08807">Confidence Boosts Trust-Based Resilience in Cooperative Multi-Robot Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Wireless communication-based multi-robot systems open the door to cyberattacks that can disrupt safety and performance of collaborative robots. The physical channel supporting inter-robot communication offers an attractive opportunity to decouple the detection of malicious robots from task-relevant data exchange between legitimate robots. Yet, trustworthiness indications coming from physical channels are uncertain and must be handled with this in mind. In this paper, we propose a resilient protocol for multi-robot operation wherein a parameter Î»t accounts for how confident a robot is about the legitimacy of nearby robots that the physical channel indicates. Analytical results prove that our protocol achieves resilient coordination with arbitrarily many malicious robots under mild assumptions. Tuning Î»t allows a designer to trade between near-optimal inter-robot coordination and quick task execution; see Fig. 1. This is a fundamental performance tradeoff and must be carefully evaluated based on the task at hand. The effectiveness of our approach is numerically verified with experiments involving platoons of autonomous cars where some vehicles are maliciously spoofed.
<div id='section'>Paperid: <span id='pid'>281, <a href='https://arxiv.org/pdf/2506.05020.pdf' target='_blank'>https://arxiv.org/pdf/2506.05020.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haokun Liu, Zhaoqi Ma, Yunong Li, Junichiro Sugihara, Yicheng Chen, Jinjie Li, Moju Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.05020">Hierarchical Language Models for Semantic Navigation and Manipulation in an Aerial-Ground Robotic System</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Heterogeneous multi-robot systems show great potential in complex tasks requiring hybrid cooperation. However, traditional approaches relying on static models often struggle with task diversity and dynamic environments. This highlights the need for generalizable intelligence that can bridge high-level reasoning with low-level execution across heterogeneous agents. To address this, we propose a hierarchical framework integrating a prompted Large Language Model (LLM) and a GridMask-enhanced fine-tuned Vision Language Model (VLM). The LLM decomposes tasks and constructs a global semantic map, while the VLM extracts task-specified semantic labels and 2D spatial information from aerial images to support local planning. Within this framework, the aerial robot follows an optimized global semantic path and continuously provides bird-view images, guiding the ground robot's local semantic navigation and manipulation, including target-absent scenarios where implicit alignment is maintained. Experiments on real-world cube or object arrangement tasks demonstrate the framework's adaptability and robustness in dynamic environments. To the best of our knowledge, this is the first demonstration of an aerial-ground heterogeneous system integrating VLM-based perception with LLM-driven task reasoning and motion planning.
<div id='section'>Paperid: <span id='pid'>282, <a href='https://arxiv.org/pdf/2505.10355.pdf' target='_blank'>https://arxiv.org/pdf/2505.10355.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Khaled Wahba, Wolfgang HÃ¶nig
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.10355">pc-dbCBS: Kinodynamic Motion Planning of Physically-Coupled Robot Teams</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion planning problems for physically-coupled multi-robot systems in cluttered environments are challenging due to their high dimensionality. Existing methods combining sampling-based planners with trajectory optimization produce suboptimal results and lack theoretical guarantees. We propose Physically-coupled discontinuity-bounded Conflict-Based Search (pc-dbCBS), an anytime kinodynamic motion planner, that extends discontinuity-bounded CBS to rigidly-coupled systems. Our approach proposes a tri-level conflict detection and resolution framework that includes the physical coupling between the robots. Moreover, pc-dbCBS alternates iteratively between state space representations, thereby preserving probabilistic completeness and asymptotic optimality while relying only on single-robot motion primitives. Across 25 simulated and six real-world problems involving multirotors carrying a cable-suspended payload and differential-drive robots linked by rigid rods, pc-dbCBS solves up to 92% more instances than a state-of-the-art baseline and plans trajectories that are 50-60% faster while reducing planning time by an order of magnitude.
<div id='section'>Paperid: <span id='pid'>283, <a href='https://arxiv.org/pdf/2504.18899.pdf' target='_blank'>https://arxiv.org/pdf/2504.18899.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhongqi Wei, Xusheng Luo, Changliu Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.18899">Hierarchical Temporal Logic Task and Motion Planning for Multi-Robot Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Task and motion planning (TAMP) for multi-robot systems, which integrates discrete task planning with continuous motion planning, remains a challenging problem in robotics. Existing TAMP approaches often struggle to scale effectively for multi-robot systems with complex specifications, leading to infeasible solutions and prolonged computation times. This work addresses the TAMP problem in multi-robot settings where tasks are specified using expressive hierarchical temporal logic and task assignments are not pre-determined. Our approach leverages the efficiency of hierarchical temporal logic specifications for task-level planning and the optimization-based graph of convex sets method for motion-level planning, integrating them within a product graph framework. At the task level, we convert hierarchical temporal logic specifications into a single graph, embedding task allocation within its edges. At the motion level, we represent the feasible motions of multiple robots through convex sets in the configuration space, guided by a sampling-based motion planner. This formulation allows us to define the TAMP problem as a shortest path search within the product graph, where efficient convex optimization techniques can be applied. We prove that our approach is both sound and complete under mild assumptions. Additionally, we extend our framework to cooperative pick-and-place tasks involving object handovers between robots. We evaluate our method across various high-dimensional multi-robot scenarios, including simulated and real-world environments with quadrupeds, robotic arms, and automated conveyor systems. Our results show that our approach outperforms existing methods in execution time and solution optimality while effectively scaling with task complexity.
<div id='section'>Paperid: <span id='pid'>284, <a href='https://arxiv.org/pdf/2503.02719.pdf' target='_blank'>https://arxiv.org/pdf/2503.02719.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenliang Liu, Nathalie Majcherczyk, Federico Pecora
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.02719">Scalable Multi-Robot Task Allocation and Coordination under Signal Temporal Logic Specifications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion planning with simple objectives, such as collision-avoidance and goal-reaching, can be solved efficiently using modern planners. However, the complexity of the allowed tasks for these planners is limited. On the other hand, signal temporal logic (STL) can specify complex requirements, but STL-based motion planning and control algorithms often face scalability issues, especially in large multi-robot systems with complex dynamics. In this paper, we propose an algorithm that leverages the best of the two worlds. We first use a single-robot motion planner to efficiently generate a set of alternative reference paths for each robot. Then coordination requirements are specified using STL, which is defined over the assignment of paths and robots' progress along those paths. We use a Mixed Integer Linear Program (MILP) to compute task assignments and robot progress targets over time such that the STL specification is satisfied. Finally, a local controller is used to track the target progress. Simulations demonstrate that our method can handle tasks with complex constraints and scales to large multi-robot teams and intricate task allocation scenarios.
<div id='section'>Paperid: <span id='pid'>285, <a href='https://arxiv.org/pdf/2502.16589.pdf' target='_blank'>https://arxiv.org/pdf/2502.16589.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyu Zhang, Zewei Zhou, Zhaoyi Wang, Yangjie Ji, Yanjun Huang, Hong Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.16589">Co-MTP: A Cooperative Trajectory Prediction Framework with Multi-Temporal Fusion for Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vehicle-to-everything technologies (V2X) have become an ideal paradigm to extend the perception range and see through the occlusion. Exiting efforts focus on single-frame cooperative perception, however, how to capture the temporal cue between frames with V2X to facilitate the prediction task even the planning task is still underexplored. In this paper, we introduce the Co-MTP, a general cooperative trajectory prediction framework with multi-temporal fusion for autonomous driving, which leverages the V2X system to fully capture the interaction among agents in both history and future domains to benefit the planning. In the history domain, V2X can complement the incomplete history trajectory in single-vehicle perception, and we design a heterogeneous graph transformer to learn the fusion of the history feature from multiple agents and capture the history interaction. Moreover, the goal of prediction is to support future planning. Thus, in the future domain, V2X can provide the prediction results of surrounding objects, and we further extend the graph transformer to capture the future interaction among the ego planning and the other vehicles' intentions and obtain the final future scenario state under a certain planning action. We evaluate the Co-MTP framework on the real-world dataset V2X-Seq, and the results show that Co-MTP achieves state-of-the-art performance and that both history and future fusion can greatly benefit prediction.
<div id='section'>Paperid: <span id='pid'>286, <a href='https://arxiv.org/pdf/2502.02984.pdf' target='_blank'>https://arxiv.org/pdf/2502.02984.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dengyu Zhang, Chenghao, Feng Xue, Qingrui Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.02984">Learning Efficient Flocking Control based on Gibbs Random Fields</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Flocking control is essential for multi-robot systems in diverse applications, yet achieving efficient flocking in congested environments poses challenges regarding computation burdens, performance optimality, and motion safety. This paper addresses these challenges through a multi-agent reinforcement learning (MARL) framework built on Gibbs Random Fields (GRFs). With GRFs, a multi-robot system is represented by a set of random variables conforming to a joint probability distribution, thus offering a fresh perspective on flocking reward design. A decentralized training and execution mechanism, which enhances the scalability of MARL concerning robot quantity, is realized using a GRF-based credit assignment method. An action attention module is introduced to implicitly anticipate the motion intentions of neighboring robots, consequently mitigating potential non-stationarity issues in MARL. The proposed framework enables learning an efficient distributed control policy for multi-robot systems in challenging environments with success rate around $99\%$, as demonstrated through thorough comparisons with state-of-the-art solutions in simulations and experiments. Ablation studies are also performed to validate the efficiency of different framework modules.
<div id='section'>Paperid: <span id='pid'>287, <a href='https://arxiv.org/pdf/2410.14383.pdf' target='_blank'>https://arxiv.org/pdf/2410.14383.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Toby Godfrey, William Hunt, Mohammad D. Soorati
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.14383">MARLIN: Multi-Agent Reinforcement Learning Guided by Language-Based Inter-Robot Negotiation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning is a key method for training multi-robot systems over a series of episodes in which robots are rewarded or punished according to their performance; only once the system is trained to a suitable standard is it deployed in the real world. If the system is not trained enough, the task will likely not be completed and could pose a risk to the surrounding environment. We introduce Multi-Agent Reinforcement Learning guided by Language-based Inter-Robot Negotiation (MARLIN), in which the training process requires fewer training episodes to reach peak performance. Robots are equipped with large language models that negotiate and debate a task, producing plans used to guide the policy during training. The approach dynamically switches between using reinforcement learning and large language model-based action negotiation throughout training. This reduces the number of training episodes required, compared to standard multi-agent reinforcement learning, and hence allows the system to be deployed to physical hardware earlier. The performance of this approach is evaluated against multi-agent reinforcement learning, showing that our hybrid method achieves comparable results with significantly reduced training time.
<div id='section'>Paperid: <span id='pid'>288, <a href='https://arxiv.org/pdf/2410.05798.pdf' target='_blank'>https://arxiv.org/pdf/2410.05798.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yupeng Yang, Yiwei Lyu, Yanze Zhang, Ian Gao, Wenhao Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.05798">Integrating Online Learning and Connectivity Maintenance for Communication-Aware Multi-Robot Coordination</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper proposes a novel data-driven control strategy for maintaining connectivity in networked multi-robot systems. Existing approaches often rely on a pre-determined communication model specifying whether pairwise robots can communicate given their relative distance to guide the connectivity-aware control design, which may not capture real-world communication conditions. To relax that assumption, we present the concept of Data-driven Connectivity Barrier Certificates, which utilize Control Barrier Functions (CBF) and Gaussian Processes (GP) to characterize the admissible control space for pairwise robots based on communication performance observed online. This allows robots to maintain a satisfying level of pairwise communication quality (measured by the received signal strength) while in motion. Then we propose a Data-driven Connectivity Maintenance (DCM) algorithm that combines (1) online learning of the communication signal strength and (2) a bi-level optimization-based control framework for the robot team to enforce global connectivity of the realistic multi-robot communication graph and minimally deviate from their task-related motions. We provide theoretical proofs to justify the properties of our algorithm and demonstrate its effectiveness through simulations with up to 20 robots.
<div id='section'>Paperid: <span id='pid'>289, <a href='https://arxiv.org/pdf/2410.01070.pdf' target='_blank'>https://arxiv.org/pdf/2410.01070.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaige Qu, Zixiong Qin, Weihua Zhuang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.01070">Meta Learning Based Adaptive Cooperative Perception in Nonstationary Vehicular Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To accommodate high network dynamics in real-time cooperative perception (CP), reinforcement learning (RL) based adaptive CP schemes have been proposed, to allow adaptive switchings between CP and stand-alone perception modes among connected and autonomous vehicles. The traditional offline-training online-execution RL framework suffers from performance degradation under nonstationary network conditions. To achieve fast and efficient model adaptation, we formulate a set of Markov decision processes for adaptive CP decisions in each stationary local vehicular network (LVN). A meta RL solution is proposed, which trains a meta RL model that captures the general features among LVNs, thus facilitating fast model adaptation for each LVN with the meta RL model as an initial point. Simulation results show the superiority of meta RL in terms of the convergence speed without reward degradation. The impact of the customization level of meta models on the model adaptation performance has also been evaluated.
<div id='section'>Paperid: <span id='pid'>290, <a href='https://arxiv.org/pdf/2409.12780.pdf' target='_blank'>https://arxiv.org/pdf/2409.12780.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Valerio Brunacci, Alberto Dionigi, Alessio De Angelis, Gabriele Costante
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.12780">Infrastructure-less UWB-based Active Relative Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In multi-robot systems, relative localization between platforms plays a crucial role in many tasks, such as leader following, target tracking, or cooperative maneuvering. State of the Art (SotA) approaches either rely on infrastructure-based or on infrastructure-less setups. The former typically achieve high localization accuracy but require fixed external structures. The latter provide more flexibility, however, most of the works use cameras or lidars that require Line-of-Sight (LoS) to operate. Ultra Wide Band (UWB) devices are emerging as a viable alternative to build infrastructure-less solutions that do not require LoS. These approaches directly deploy the UWB sensors on the robots. However, they require that at least one of the platforms is static, limiting the advantages of an infrastructure-less setup. In this work, we remove this constraint and introduce an active method for infrastructure-less relative localization. Our approach allows the robot to adapt its position to minimize the relative localization error of the other platform. To this aim, we first design a specialized anchor placement for the active localization task. Then, we propose a novel UWB Relative Localization Loss that adapts the Geometric Dilution Of Precision metric to the infrastructure-less scenario. Lastly, we leverage this loss function to train an active Deep Reinforcement Learning-based controller for UWB relative localization. An extensive simulation campaign and real-world experiments validate our method, showing up to a 60% reduction of the localization error compared to current SotA approaches.
<div id='section'>Paperid: <span id='pid'>291, <a href='https://arxiv.org/pdf/2407.06812.pdf' target='_blank'>https://arxiv.org/pdf/2407.06812.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guobin Zhu, Qingrui Zhang, Bo Zhu, Tianjiang Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.06812">Heuristic Predictive Control for Multi-Robot Flocking in Congested Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-robot flocking possesses extraordinary advantages over a single-robot system in diverse domains, but it is challenging to ensure safe and optimal performance in congested environments. Hence, this paper is focused on the investigation of distributed optimal flocking control for multiple robots in crowded environments. A heuristic predictive control solution is proposed based on a Gibbs Random Field (GRF), in which bio-inspired potential functions are used to characterize robot-robot and robot-environment interactions. The optimal solution is obtained by maximizing a posteriori joint distribution of the GRF in a certain future time instant. A gradient-based heuristic solution is developed, which could significantly speed up the computation of the optimal control. Mathematical analysis is also conducted to show the validity of the heuristic solution. Multiple collision risk levels are designed to improve the collision avoidance performance of robots in dynamic environments. The proposed heuristic predictive control is evaluated comprehensively from multiple perspectives based on different metrics in a challenging simulation environment. The competence of the proposed algorithm is validated via the comparison with the non-heuristic predictive control and two existing popular flocking control methods. Real-life experiments are also performed using four quadrotor UAVs to further demonstrate the efficiency of the proposed design.
<div id='section'>Paperid: <span id='pid'>292, <a href='https://arxiv.org/pdf/2407.03569.pdf' target='_blank'>https://arxiv.org/pdf/2407.03569.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Zhou, Yanze Zhang, Wenhao Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.03569">Safety-Critical Control with Uncertainty Quantification using Adaptive Conformal Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Safety assurance is critical in the planning and control of robotic systems. For robots operating in the real world, the safety-critical design often needs to explicitly address uncertainties and the pre-computed guarantees often rely on the assumption of the particular distribution of the uncertainty. However, it is difficult to characterize the actual uncertainty distribution beforehand and thus the established safety guarantee may be violated due to possible distribution mismatch. In this paper, we propose a novel safe control framework that provides a high-probability safety guarantee for stochastic dynamical systems following unknown distributions of motion noise. Specifically, this framework adopts adaptive conformal prediction to dynamically quantify the prediction uncertainty from online observations and combines that with the probabilistic extension of the control barrier functions (CBFs) to characterize the uncertainty-aware control constraints. By integrating the constraints in the model predictive control scheme, it allows robots to adaptively capture the true prediction uncertainty online in a distribution-free setting and enjoys formally provable high-probability safety assurance. Simulation results on multi-robot systems with stochastic single-integrator dynamics and unicycle dynamics are provided to demonstrate the effectiveness of our framework.
<div id='section'>Paperid: <span id='pid'>293, <a href='https://arxiv.org/pdf/2405.03290.pdf' target='_blank'>https://arxiv.org/pdf/2405.03290.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Timo HÃ¤ckel, Luca von Roenn, Nemo Juchmann, Alexander Fay, Rinie Akkermans, Tim Tiedemann, Thomas C. Schmidt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.03290">Coordinating Cooperative Perception in Urban Air Mobility for Enhanced Environmental Awareness</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The trend for Urban Air Mobility (UAM) is growing with prospective air taxis, parcel deliverers, and medical and industrial services. Safe and efficient UAM operation relies on timely communication and reliable data exchange. In this paper, we explore Cooperative Perception (CP) for Unmanned Aircraft Systems (UAS), considering the unique communication needs involving high dynamics and a large number of UAS. We propose a hybrid approach combining local broadcast with a central CP service, inspired by centrally managed U-space and broadcast mechanisms from automotive and aviation domains. In a simulation study, we show that our approach significantly enhances the environmental awareness for UAS compared to fully distributed approaches, with an increased communication channel load, which we also evaluate. These findings prompt a discussion on communication strategies for CP in UAM and the potential of a centralized CP service in future research.
<div id='section'>Paperid: <span id='pid'>294, <a href='https://arxiv.org/pdf/2404.02324.pdf' target='_blank'>https://arxiv.org/pdf/2404.02324.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vishnunandan L. N. Venkatesh, Byung-Cheol Min
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.02324">Learning from Demonstration Framework for Multi-Robot Systems Using Interaction Keypoints and Soft Actor-Critic Methods</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning from Demonstration (LfD) is a promising approach to enable Multi-Robot Systems (MRS) to acquire complex skills and behaviors. However, the intricate interactions and coordination challenges in MRS pose significant hurdles for effective LfD. In this paper, we present a novel LfD framework specifically designed for MRS, which leverages visual demonstrations to capture and learn from robot-robot and robot-object interactions. Our framework introduces the concept of Interaction Keypoints (IKs) to transform the visual demonstrations into a representation that facilitates the inference of various skills necessary for the task. The robots then execute the task using sensorimotor actions and reinforcement learning (RL) policies when required. A key feature of our approach is the ability to handle unseen contact-based skills that emerge during the demonstration. In such cases, RL is employed to learn the skill using a classifier-based reward function, eliminating the need for manual reward engineering and ensuring adaptability to environmental changes. We evaluate our framework across a range of mobile robot tasks, covering both behavior-based and contact-based domains. The results demonstrate the effectiveness of our approach in enabling robots to learn complex multi-robot tasks and behaviors from visual demonstrations.
<div id='section'>Paperid: <span id='pid'>295, <a href='https://arxiv.org/pdf/2404.02318.pdf' target='_blank'>https://arxiv.org/pdf/2404.02318.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vishnunandan L. N. Venkatesh, Byung-Cheol Min
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.02318">ZeroCAP: Zero-Shot Multi-Robot Context Aware Pattern Formation via Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Incorporating language comprehension into robotic operations unlocks significant advancements in robotics, but also presents distinct challenges, particularly in executing spatially oriented tasks like pattern formation. This paper introduces ZeroCAP, a novel system that integrates large language models with multi-robot systems for zero-shot context aware pattern formation. Grounded in the principles of language-conditioned robotics, ZeroCAP leverages the interpretative power of language models to translate natural language instructions into actionable robotic configurations. This approach combines the synergy of vision-language models, cutting-edge segmentation techniques and shape descriptors, enabling the realization of complex, context-driven pattern formations in the realm of multi robot coordination. Through extensive experiments, we demonstrate the systems proficiency in executing complex context aware pattern formations across a spectrum of tasks, from surrounding and caging objects to infilling regions. This not only validates the system's capability to interpret and implement intricate context-driven tasks but also underscores its adaptability and effectiveness across varied environments and scenarios. The experimental videos and additional information about this work can be found at https://sites.google.com/view/zerocap/home.
<div id='section'>Paperid: <span id='pid'>296, <a href='https://arxiv.org/pdf/2402.16690.pdf' target='_blank'>https://arxiv.org/pdf/2402.16690.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuru Yang, Yunze Hu, Han Gao, Kang Ding, Zhaoyang Li, Pingping Zhu, Ying Sun, Chang Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.16690">Risk-Aware Non-Myopic Motion Planner for Large-Scale Robotic Swarm Using CVaR Constraints</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Swarm robotics has garnered significant attention due to its ability to accomplish elaborate and synchronized tasks. Existing methodologies for motion planning of swarm robotic systems mainly encounter difficulties in scalability and safety guarantee. To address these limitations, we propose a Risk-aware swarm mOtion planner using conditional ValuE at Risk (ROVER) that systematically navigates large-scale swarms through cluttered environments while ensuring safety. ROVER formulates a finite-time model predictive control (FTMPC) problem predicated upon the macroscopic state of the robot swarm represented by a Gaussian Mixture Model (GMM) and integrates conditional value-at-risk (CVaR) to ensure collision avoidance. The key component of ROVER is imposing a CVaR constraint on the distribution of the Signed Distance Function between the swarm GMM and obstacles in the FTMPC to enforce collision avoidance. Utilizing the analytical expression of CVaR of a GMM derived in this work, we develop a computationally efficient solution to solve the non-linear constrained FTMPC through sequential linear programming. Simulations and comparisons with representative benchmark approaches demonstrate the effectiveness of ROVER in flexibility, scalability, and risk mitigation.
<div id='section'>Paperid: <span id='pid'>297, <a href='https://arxiv.org/pdf/2402.06838.pdf' target='_blank'>https://arxiv.org/pdf/2402.06838.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haitong Wang, Aaron Hao Tan, Goldie Nejat
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.06838">NavFormer: A Transformer Architecture for Robot Target-Driven Navigation in Unknown and Dynamic Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In unknown cluttered and dynamic environments such as disaster scenes, mobile robots need to perform target-driven navigation in order to find people or objects of interest, while being solely guided by images of the targets. In this paper, we introduce NavFormer, a novel end-to-end transformer architecture developed for robot target-driven navigation in unknown and dynamic environments. NavFormer leverages the strengths of both 1) transformers for sequential data processing and 2) self-supervised learning (SSL) for visual representation to reason about spatial layouts and to perform collision-avoidance in dynamic settings. The architecture uniquely combines dual-visual encoders consisting of a static encoder for extracting invariant environment features for spatial reasoning, and a general encoder for dynamic obstacle avoidance. The primary robot navigation task is decomposed into two sub-tasks for training: single robot exploration and multi-robot collision avoidance. We perform cross-task training to enable the transfer of learned skills to the complex primary navigation task without the need for task-specific fine-tuning. Simulated experiments demonstrate that NavFormer can effectively navigate a mobile robot in diverse unknown environments, outperforming existing state-of-the-art methods in terms of success rate and success weighted by (normalized inverse) path length. Furthermore, a comprehensive ablation study is performed to evaluate the impact of the main design choices of the structure and training of NavFormer, further validating their effectiveness in the overall system.
<div id='section'>Paperid: <span id='pid'>298, <a href='https://arxiv.org/pdf/2402.02833.pdf' target='_blank'>https://arxiv.org/pdf/2402.02833.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Georg Heppner, David Oberacker, Arne Roennau, RÃ¼diger Dillmann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.02833">Behavior Tree Capabilities for Dynamic Multi-Robot Task Allocation with Heterogeneous Robot Teams</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While individual robots are becoming increasingly capable, with new sensors and actuators, the complexity of expected missions increased exponentially in comparison. To cope with this complexity, heterogeneous teams of robots have become a significant research interest in recent years. Making effective use of the robots and their unique skills in a team is challenging. Dynamic runtime conditions often make static task allocations infeasible, therefore requiring a dynamic, capability-aware allocation of tasks to team members. To this end, we propose and implement a system that allows a user to specify missions using Bheavior Trees (BTs), which can then, at runtime, be dynamically allocated to the current robot team. The system allows to statically model an individual robot's capabilities within our ros_bt_py BT framework. It offers a runtime auction system to dynamically allocate tasks to the most capable robot in the current team. The system leverages utility values and pre-conditions to ensure that the allocation improves the overall mission execution quality while preventing faulty assignments. To evaluate the system, we simulated a find-and-decontaminate mission with a team of three heterogeneous robots and analyzed the utilization and overall mission times as metrics. Our results show that our system can improve the overall effectiveness of a team while allowing for intuitive mission specification and flexibility in the team composition.
<div id='section'>Paperid: <span id='pid'>299, <a href='https://arxiv.org/pdf/2401.04003.pdf' target='_blank'>https://arxiv.org/pdf/2401.04003.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xusheng Luo, Changliu Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.04003">Simultaneous Task Allocation and Planning for Multi-Robots under Hierarchical Temporal Logic Specifications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Research in robotic planning with temporal logic specifications, such as Linear Temporal Logic (LTL), has relied on single formulas. However, as task complexity increases, LTL formulas become lengthy, making them difficult to interpret and generate, and straining the computational capacities of planners. To address this, we introduce a hierarchical structure for a widely used specification type -- LTL on finite traces (LTL$_f$). The resulting language, termed H-LTL$_f$, is defined with both its syntax and semantics. We further prove that H-LTL$_f$ is more expressive than its standard "flat" counterparts. Moreover, we conducted a user study that compared the standard LTL$_f$ with our hierarchical version and found that users could more easily comprehend complex tasks using the hierarchical structure. We develop a search-based approach to synthesize plans for multi-robot systems, achieving simultaneous task allocation and planning. This method approximates the search space by loosely interconnected sub-spaces, each corresponding to an LTL$_f$ specification. The search primarily focuses on a single sub-space, transitioning to another under conditions determined by the decomposition of automata. We develop multiple heuristics to significantly expedite the search. Our theoretical analysis, conducted under mild assumptions, addresses completeness and optimality. Compared to existing methods used in various simulators for service tasks, our approach improves planning times while maintaining comparable solution quality.
<div id='section'>Paperid: <span id='pid'>300, <a href='https://arxiv.org/pdf/2310.09700.pdf' target='_blank'>https://arxiv.org/pdf/2310.09700.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Muhammad Baqer Mollah, Honggang Wang, Mohammad Ataul Karim, Hua Fang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.09700">mmWave Enabled Connected Autonomous Vehicles: A Use Case with V2V Cooperative Perception</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Connected and autonomous vehicles (CAVs) will revolutionize tomorrow's intelligent transportation systems, being considered promising to improve transportation safety, traffic efficiency, and mobility. In fact, envisioned use cases of CAVs demand very high throughput, lower latency, highly reliable communications, and precise positioning capabilities. The availability of a large spectrum at millimeter-wave (mmWave) band potentially promotes new specifications in spectrum technologies capable of supporting such service requirements. In this article, we specifically focus on how mmWave communications are being approached in vehicular standardization activities, CAVs use cases and deployment challenges in realizing the future fully connected settings. Finally, we also present a detailed performance assessment on mmWave-enabled vehicle-to-vehicle (V2V) cooperative perception as an example case study to show the impact of different configurations.
<div id='section'>Paperid: <span id='pid'>301, <a href='https://arxiv.org/pdf/2305.02214.pdf' target='_blank'>https://arxiv.org/pdf/2305.02214.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kai Xiong, Zhihong Wang, Supeng Leng, Jianhua He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.02214">A Digital Twin Empowered Lightweight Model Sharing Scheme for Multi-Robot Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-robot system for manufacturing is an Industry Internet of Things (IIoT) paradigm with significant operational cost savings and productivity improvement, where Unmanned Aerial Vehicles (UAVs) are employed to control and implement collaborative productions without human intervention. This mission-critical system relies on 3-Dimension (3-D) scene recognition to improve operation accuracy in the production line and autonomous piloting. However, implementing 3-D point cloud learning, such as Pointnet, is challenging due to limited sensing and computing resources equipped with UAVs. Therefore, we propose a Digital Twin (DT) empowered Knowledge Distillation (KD) method to generate several lightweight learning models and select the optimal model to deploy on UAVs. With a digital replica of the UAVs preserved at the edge server, the DT system controls the model sharing network topology and learning model structure to improve recognition accuracy further. Moreover, we employ network calculus to formulate and solve the model sharing configuration problem toward minimal resource consumption, as well as convergence. Simulation experiments are conducted over a popular point cloud dataset to evaluate the proposed scheme. Experiment results show that the proposed model sharing scheme outperforms the individual model in terms of computing resource consumption and recognition accuracy.
<div id='section'>Paperid: <span id='pid'>302, <a href='https://arxiv.org/pdf/2304.14622.pdf' target='_blank'>https://arxiv.org/pdf/2304.14622.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Babar Shahzaad, Balsam Alkouz, Jermaine Janszen, Athman Bouguettaya
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.14622">Optimizing Drone Delivery in Smart Cities</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a novel context-aware drone delivery framework for optimizing package delivery through skyway networks in smart cities. We reformulate the problem of finding an optimal drone service delivery pathway as a more congruent and elegant drone delivery service composition problem. In this respect, we propose a novel line-of-sight heuristic-based context-aware composition algorithm that selects and composes near-optimal drone delivery services. We conducted an extensive experiment using a real dataset to show the robustness of our proposed approach.
<div id='section'>Paperid: <span id='pid'>303, <a href='https://arxiv.org/pdf/2303.12610.pdf' target='_blank'>https://arxiv.org/pdf/2303.12610.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Han Wang, Antonis Papachristodoulou, Kostas Margellos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.12610">Distributed Safe Control Design and Probabilistic Safety Verification for Multi-Agent Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose distributed iterative algorithms for safe control design and safety verification for networked multi-agent systems. These algorithms rely on distributing a control barrier function (CBF) related quadratic programming (QP) problem assuming the existence of CBFs. The proposed distributed algorithm addresses infeasibility issues of existing schemes via a cooperation mechanism between agents. The resulting control input is guaranteed to be optimal, and satisfies CBF constraints of all agents. Furthermore, a truncated algorithm is proposed to facilitate computational implementation. The performance of the truncated algorithm is evaluated using a distributed safety verification algorithm. The algorithm quantifies safety for multi-agent systems probabilistically by means of CBFs. Both upper and lower bounds on the probability of safety are obtained using the so called scenario approach. Both the scenario sampling and safety verification procedures are fully distributed. The efficacy of our algorithms is demonstrated by an example on multi-robot collision avoidance.
<div id='section'>Paperid: <span id='pid'>304, <a href='https://arxiv.org/pdf/2301.09018.pdf' target='_blank'>https://arxiv.org/pdf/2301.09018.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ricardo Vega, Kevin Zhu, Sean Luke, Maryam Parsa, Cameron Nowzari
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.09018">Simulate Less, Expect More: Bringing Robot Swarms to Life via Low-Fidelity Simulations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper proposes a novel methodology for addressing the simulation-reality gap for multi-robot swarm systems. Rather than immediately try to shrink or `bridge the gap' anytime a real-world experiment failed that worked in simulation, we characterize conditions under which this is actually necessary. When these conditions are not satisfied, we show how very simple simulators can still be used to both (i) design new multi-robot systems, and (ii) guide real-world swarming experiments towards certain emergent behaviors when the gap is very large. The key ideas are an iterative simulator-in-the-design-loop in which real-world experiments, simulator modifications, and simulated experiments are intimately coupled in a way that minds the gap without needing to shrink it, as well as the use of minimally viable phase diagrams to guide real world experiments. We demonstrate the usefulness of our methods on deploying a real multi-robot swarm system to successfully exhibit an emergent milling behavior.
<div id='section'>Paperid: <span id='pid'>305, <a href='https://arxiv.org/pdf/2509.20218.pdf' target='_blank'>https://arxiv.org/pdf/2509.20218.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohamed Manzour, Catherine M. Elias, Omar M. Shehata, RubÃ©n Izquierdo, Miguel Ãngel Sotelo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20218">Design Insights and Comparative Evaluation of a Hardware-Based Cooperative Perception Architecture for Lane Change Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Research on lane change prediction has gained attention in the last few years. Most existing works in this area have been conducted in simulation environments or with pre-recorded datasets, these works often rely on simplified assumptions about sensing, communication, and traffic behavior that do not always hold in practice. Real-world deployments of lane-change prediction systems are relatively rare, and when they are reported, the practical challenges, limitations, and lessons learned are often under-documented. This study explores cooperative lane-change prediction through a real hardware deployment in mixed traffic and shares the insights that emerged during implementation and testing. We highlight the practical challenges we faced, including bottlenecks, reliability issues, and operational constraints that shaped the behavior of the system. By documenting these experiences, the study provides guidance for others working on similar pipelines.
<div id='section'>Paperid: <span id='pid'>306, <a href='https://arxiv.org/pdf/2509.20218.pdf' target='_blank'>https://arxiv.org/pdf/2509.20218.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohamed Manzour, Catherine M. Elias, Omar M. Shehata, RubÃ©n Izquierdo, Miguel Ãngel Sotelo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20218">Design Insights and Comparative Evaluation of a Hardware-Based Cooperative Perception Architecture for Lane Change Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Research on lane change prediction has gained attention in the last few years. Most existing works in this area have been conducted in simulation environments or with pre-recorded datasets, these works often rely on simplified assumptions about sensing, communication, and traffic behavior that do not always hold in practice. Real-world deployments of lane-change prediction systems are relatively rare, and when they are reported, the practical challenges, limitations, and lessons learned are often under-documented. This study explores cooperative lane-change prediction through a real hardware deployment in mixed traffic and shares the insights that emerged during implementation and testing. We highlight the practical challenges we faced, including bottlenecks, reliability issues, and operational constraints that shaped the behavior of the system. By documenting these experiences, the study provides guidance for others working on similar pipelines.
<div id='section'>Paperid: <span id='pid'>307, <a href='https://arxiv.org/pdf/2509.19168.pdf' target='_blank'>https://arxiv.org/pdf/2509.19168.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mark Gonzales, Ethan Oh, Joseph Moore
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19168">A Multimodal Stochastic Planning Approach for Navigation and Multi-Robot Coordination</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we present a receding-horizon, sampling-based planner capable of reasoning over multimodal policy distributions. By using the cross-entropy method to optimize a multimodal policy under a common cost function, our approach increases robustness against local minima and promotes effective exploration of the solution space. We show that our approach naturally extends to multi-robot collision-free planning, enables agents to share diverse candidate policies to avoid deadlocks, and allows teams to minimize a global objective without incurring the computational complexity of centralized optimization. Numerical simulations demonstrate that employing multiple modes significantly improves success rates in trap environments and in multi-robot collision avoidance. Hardware experiments further validate the approach's real-time feasibility and practical performance.
<div id='section'>Paperid: <span id='pid'>308, <a href='https://arxiv.org/pdf/2509.19168.pdf' target='_blank'>https://arxiv.org/pdf/2509.19168.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mark Gonzales, Ethan Oh, Joseph Moore
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19168">A Multimodal Stochastic Planning Approach for Navigation and Multi-Robot Coordination</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we present a receding-horizon, sampling-based planner capable of reasoning over multimodal policy distributions. By using the cross-entropy method to optimize a multimodal policy under a common cost function, our approach increases robustness against local minima and promotes effective exploration of the solution space. We show that our approach naturally extends to multi-robot collision-free planning, enables agents to share diverse candidate policies to avoid deadlocks, and allows teams to minimize a global objective without incurring the computational complexity of centralized optimization. Numerical simulations demonstrate that employing multiple modes significantly improves success rates in trap environments and in multi-robot collision avoidance. Hardware experiments further validate the approach's real-time feasibility and practical performance.
<div id='section'>Paperid: <span id='pid'>309, <a href='https://arxiv.org/pdf/2509.08117.pdf' target='_blank'>https://arxiv.org/pdf/2509.08117.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruijie Du, Ruoyu Lin, Yanning Shen, Magnus Egerstedt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.08117">Online Learning and Coverage of Unknown Fields Using Random-Feature Gaussian Processes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper proposes a framework for multi-robot systems to perform simultaneous learning and coverage of the domain of interest characterized by an unknown and potentially time-varying density function. To overcome the limitations of Gaussian Process (GP) regression, we employ Random Feature GP (RFGP) and its online variant (O-RFGP) that enables online and incremental inference. By integrating these with Voronoi-based coverage control and Upper Confidence Bound (UCB) sampling strategy, a team of robots can adaptively focus on important regions while refining the learned spatial field for efficient coverage. Under mild assumptions, we provide theoretical guarantees and evaluate the framework through simulations in time-invariant scenarios. Furthermore, its effectiveness in time-varying settings is demonstrated through additional simulations and a physical experiment.
<div id='section'>Paperid: <span id='pid'>310, <a href='https://arxiv.org/pdf/2509.08117.pdf' target='_blank'>https://arxiv.org/pdf/2509.08117.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruijie Du, Ruoyu Lin, Yanning Shen, Magnus Egerstedt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.08117">Online Learning and Coverage of Unknown Fields Using Random-Feature Gaussian Processes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper proposes a framework for multi-robot systems to perform simultaneous learning and coverage of the domain of interest characterized by an unknown and potentially time-varying density function. To overcome the limitations of Gaussian Process (GP) regression, we employ Random Feature GP (RFGP) and its online variant (O-RFGP) that enables online and incremental inference. By integrating these with Voronoi-based coverage control and Upper Confidence Bound (UCB) sampling strategy, a team of robots can adaptively focus on important regions while refining the learned spatial field for efficient coverage. Under mild assumptions, we provide theoretical guarantees and evaluate the framework through simulations in time-invariant scenarios. Furthermore, its effectiveness in time-varying settings is demonstrated through additional simulations and a physical experiment.
<div id='section'>Paperid: <span id='pid'>311, <a href='https://arxiv.org/pdf/2509.03563.pdf' target='_blank'>https://arxiv.org/pdf/2509.03563.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Quan Quan, Jiwen Xu, Runxiao Liu, Yi Ding, Jiaxing Che, Kai-Yuan Cai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03563">Self-Organizing Aerial Swarm Robotics for Resilient Load Transportation : A Table-Mechanics-Inspired Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In comparison with existing approaches, which struggle with scalability, communication dependency, and robustness against dynamic failures, cooperative aerial transportation via robot swarms holds transformative potential for logistics and disaster response. Here, we present a physics-inspired cooperative transportation approach for flying robot swarms that imitates the dissipative mechanics of table-leg load distribution. By developing a decentralized dissipative force model, our approach enables autonomous formation stabilization and adaptive load allocation without the requirement of explicit communication. Based on local neighbor robots and the suspended payload, each robot dynamically adjusts its position. This is similar to energy-dissipating table leg reactions. The stability of the resultant control system is rigorously proved. Simulations demonstrate that the tracking errors of the proposed approach are 20%, 68%, 55.5%, and 21.9% of existing approaches under the cases of capability variation, cable uncertainty, limited vision, and payload variation, respectively. In real-world experiments with six flying robots, the cooperative aerial transportation system achieved a 94% success rate under single-robot failure, disconnection events, 25% payload variation, and 40% cable length uncertainty, demonstrating strong robustness under outdoor winds up to Beaufort scale 4. Overall, this physics-inspired approach bridges swarm intelligence and mechanical stability principles, offering a scalable framework for heterogeneous aerial systems to collectively handle complex transportation tasks in communication-constrained environments.
<div id='section'>Paperid: <span id='pid'>312, <a href='https://arxiv.org/pdf/2509.03563.pdf' target='_blank'>https://arxiv.org/pdf/2509.03563.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Quan Quan, Jiwen Xu, Runxiao Liu, Yi Ding, Jiaxing Che, Kai-Yuan Cai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03563">Self-Organizing Aerial Swarm Robotics for Resilient Load Transportation : A Table-Mechanics-Inspired Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In comparison with existing approaches, which struggle with scalability, communication dependency, and robustness against dynamic failures, cooperative aerial transportation via robot swarms holds transformative potential for logistics and disaster response. Here, we present a physics-inspired cooperative transportation approach for flying robot swarms that imitates the dissipative mechanics of table-leg load distribution. By developing a decentralized dissipative force model, our approach enables autonomous formation stabilization and adaptive load allocation without the requirement of explicit communication. Based on local neighbor robots and the suspended payload, each robot dynamically adjusts its position. This is similar to energy-dissipating table leg reactions. The stability of the resultant control system is rigorously proved. Simulations demonstrate that the tracking errors of the proposed approach are 20%, 68%, 55.5%, and 21.9% of existing approaches under the cases of capability variation, cable uncertainty, limited vision, and payload variation, respectively. In real-world experiments with six flying robots, the cooperative aerial transportation system achieved a 94% success rate under single-robot failure, disconnection events, 25% payload variation, and 40% cable length uncertainty, demonstrating strong robustness under outdoor winds up to Beaufort scale 4. Overall, this physics-inspired approach bridges swarm intelligence and mechanical stability principles, offering a scalable framework for heterogeneous aerial systems to collectively handle complex transportation tasks in communication-constrained environments.
<div id='section'>Paperid: <span id='pid'>313, <a href='https://arxiv.org/pdf/2507.09505.pdf' target='_blank'>https://arxiv.org/pdf/2507.09505.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tenghui Xie, Zhiying Song, Fuxi Wen, Jun Li, Guangzhao Liu, Zijian Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.09505">TruckV2X: A Truck-Centered Perception Dataset</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous trucking offers significant benefits, such as improved safety and reduced costs, but faces unique perception challenges due to trucks' large size and dynamic trailer movements. These challenges include extensive blind spots and occlusions that hinder the truck's perception and the capabilities of other road users. To address these limitations, cooperative perception emerges as a promising solution. However, existing datasets predominantly feature light vehicle interactions or lack multi-agent configurations for heavy-duty vehicle scenarios. To bridge this gap, we introduce TruckV2X, the first large-scale truck-centered cooperative perception dataset featuring multi-modal sensing (LiDAR and cameras) and multi-agent cooperation (tractors, trailers, CAVs, and RSUs). We further investigate how trucks influence collaborative perception needs, establishing performance benchmarks while suggesting research priorities for heavy vehicle perception. The dataset provides a foundation for developing cooperative perception systems with enhanced occlusion handling capabilities, and accelerates the deployment of multi-agent autonomous trucking systems. The TruckV2X dataset is available at https://huggingface.co/datasets/XieTenghu1/TruckV2X.
<div id='section'>Paperid: <span id='pid'>314, <a href='https://arxiv.org/pdf/2506.22942.pdf' target='_blank'>https://arxiv.org/pdf/2506.22942.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kartik A. Pant, Jaehyeok Kim, James M. Goppert, Inseok Hwang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.22942">Energy-Constrained Resilient Multi-Robot Coverage Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The problem of multi-robot coverage control becomes significantly challenging when multiple robots leave the mission space simultaneously to charge their batteries, disrupting the underlying network topology for communication and sensing. To address this, we propose a resilient network design and control approach that allows robots to achieve the desired coverage performance while satisfying energy constraints and maintaining network connectivity throughout the mission. We model the combined motion, energy, and network dynamics of the multirobot systems (MRS) as a hybrid system with three modes, i.e., coverage, return-to-base, and recharge, respectively. We show that ensuring the energy constraints can be transformed into designing appropriate guard conditions for mode transition between each of the three modes. Additionally, we present a systematic procedure to design, maintain, and reconfigure the underlying network topology using an energy-aware bearing rigid network design, enhancing the structural resilience of the MRS even when a subset of robots departs to charge their batteries. Finally, we validate our proposed method using numerical simulations.
<div id='section'>Paperid: <span id='pid'>315, <a href='https://arxiv.org/pdf/2505.06399.pdf' target='_blank'>https://arxiv.org/pdf/2505.06399.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Siwei Cai, Yuwei Wu, Lifeng Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.06399">LLM-Land: Large Language Models for Context-Aware Drone Landing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous landing is essential for drones deployed in emergency deliveries, post-disaster response, and other large-scale missions. By enabling self-docking on charging platforms, it facilitates continuous operation and significantly extends mission endurance. However, traditional approaches often fall short in dynamic, unstructured environments due to limited semantic awareness and reliance on fixed, context-insensitive safety margins. To address these limitations, we propose a hybrid framework that integrates large language model (LLMs) with model predictive control (MPC). Our approach begins with a vision-language encoder (VLE) (e.g., BLIP), which transforms real-time images into concise textual scene descriptions. These descriptions are processed by a lightweight LLM (e.g., Qwen 2.5 1.5B or LLaMA 3.2 1B) equipped with retrieval-augmented generation (RAG) to classify scene elements and infer context-aware safety buffers, such as 3 meters for pedestrians and 5 meters for vehicles. The resulting semantic flags and unsafe regions are then fed into an MPC module, enabling real-time trajectory replanning that avoids collisions while maintaining high landing precision. We validate our framework in the ROS-Gazebo simulator, where it consistently outperforms conventional vision-based MPC baselines. Our results show a significant reduction in near-miss incidents with dynamic obstacles, while preserving accurate landings in cluttered environments.
<div id='section'>Paperid: <span id='pid'>316, <a href='https://arxiv.org/pdf/2505.00747.pdf' target='_blank'>https://arxiv.org/pdf/2505.00747.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiying Song, Tenghui Xie, Fuxi Wen, Jun Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.00747">Wireless Communication as an Information Sensor for Multi-agent Cooperative Perception: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cooperative perception extends the perception capabilities of autonomous vehicles by enabling multi-agent information sharing via Vehicle-to-Everything (V2X) communication. Unlike traditional onboard sensors, V2X acts as a dynamic "information sensor" characterized by limited communication, heterogeneity, mobility, and scalability. This survey provides a comprehensive review of recent advancements from the perspective of information-centric cooperative perception, focusing on three key dimensions: information representation, information fusion, and large-scale deployment. We categorize information representation into data-level, feature-level, and object-level schemes, and highlight emerging methods for reducing data volume and compressing messages under communication constraints. In information fusion, we explore techniques under both ideal and non-ideal conditions, including those addressing heterogeneity, localization errors, latency, and packet loss. Finally, we summarize system-level approaches to support scalability in dense traffic scenarios. Compared with existing surveys, this paper introduces a new perspective by treating V2X communication as an information sensor and emphasizing the challenges of deploying cooperative perception in real-world intelligent transportation systems.
<div id='section'>Paperid: <span id='pid'>317, <a href='https://arxiv.org/pdf/2503.12876.pdf' target='_blank'>https://arxiv.org/pdf/2503.12876.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Di Meng, Tianhao Zhao, Chaoyu Xue, Jun Wu, Qiuguo Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.12876">A Hierarchical Region-Based Approach for Efficient Multi-Robot Exploration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-robot autonomous exploration in an unknown environment is an important application in robotics.Traditional exploration methods only use information around frontier points or viewpoints, ignoring spatial information of unknown areas. Moreover, finding the exact optimal solution for multi-robot task allocation is NP-hard, resulting in significant computational time consumption. To address these issues, we present a hierarchical multi-robot exploration framework using a new modeling method called RegionGraph. The proposed approach makes two main contributions: 1) A new modeling method for unexplored areas that preserves their spatial information across the entire space in a weighted graph called RegionGraph. 2) A hierarchical multi-robot exploration framework that decomposes the global exploration task into smaller subtasks, reducing the frequency of global planning and enabling asynchronous exploration. The proposed method is validated through both simulation and real-world experiments, demonstrating a 20% improvement in efficiency compared to existing methods.
<div id='section'>Paperid: <span id='pid'>318, <a href='https://arxiv.org/pdf/2502.16460.pdf' target='_blank'>https://arxiv.org/pdf/2502.16460.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kartik A. Pant, Vishnu Vijay, Minhyun Cho, Inseok Hwang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.16460">On Enhancing Structural Resilience of Multirobot Coverage Control with Bearing Rigidity</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The problem of multi-robot coverage control has been widely studied to efficiently coordinate a team of robots to cover a desired area of interest. However, this problem faces significant challenges when some robots are lost or deviate from their desired formation during the mission due to faults or cyberattacks. Since a majority of multi-robot systems (MRSs) rely on communication and relative sensing for their efficient operation, a failure in one robot could result in a cascade of failures in the entire system. In this work, we propose a hierarchical framework for area coverage, combining centralized coordination by leveraging Voronoi partitioning with decentralized reference tracking model predictive control (MPC) for control design. In addition to reference tracking, the decentralized MPC also performs bearing maintenance to enforce a rigid MRS network, thereby enhancing the structural resilience, i.e., the ability to detect and mitigate the effects of localization errors and robot loss during the mission. Furthermore, we show that the resulting control architecture guarantees the recovery of the MRS network in the event of robot loss while maintaining a minimally rigid structure. The effectiveness of the proposed algorithm is validated through numerical simulations.
<div id='section'>Paperid: <span id='pid'>319, <a href='https://arxiv.org/pdf/2501.18309.pdf' target='_blank'>https://arxiv.org/pdf/2501.18309.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Giorgio Cignarale, Stephan Felber, Eric Goubault, Bernardo Hummes Flores, Hugo Rincon Galeana
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.18309">Knowledge in multi-robot systems: an interplay of dynamics, computation and communication</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we provide a framework integrating distributed multi-robot systems and temporal epistemic logic. We show that continuous-discrete hybrid systems are compatible with logical models of knowledge already used in distributed computing, and demonstrate its usefulness by deriving sufficient epistemic conditions for exploration and gathering robot tasks to be solvable. We provide a separation of the physical and computational aspects of a robotic system, allowing us to decouple the problems related to each and directly use methods from control theory and distributed computing, fields that are traditionally distant in the literature. Finally, we demonstrate a novel approach for reasoning about the knowledge in multi-robot systems through a principled method of converting a switched hybrid dynamical system into a temporal-epistemic logic model, passing through an abstract state machine representation. This creates space for methods and results to be exchanged across the fields of control theory, distributed computing and temporal-epistemic logic, while reasoning about multi-robot systems.
<div id='section'>Paperid: <span id='pid'>320, <a href='https://arxiv.org/pdf/2501.18309.pdf' target='_blank'>https://arxiv.org/pdf/2501.18309.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Giorgio Cignarale, Stephan Felber, Eric Goubault, Bernardo Hummes Flores, Hugo Rincon Galeana
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.18309">Knowledge in multi-robot systems: an interplay of dynamics, computation and communication</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we provide a framework integrating distributed multi-robot systems and temporal epistemic logic. We show that continuous-discrete hybrid systems are compatible with logical models of knowledge already used in distributed computing, and demonstrate its usefulness by deriving sufficient epistemic conditions for exploration and gathering robot tasks to be solvable. We provide a separation of the physical and computational aspects of a robotic system, allowing us to decouple the problems related to each and directly use methods from control theory and distributed computing, fields that are traditionally distant in the literature. Finally, we demonstrate a novel approach for reasoning about the knowledge in multi-robot systems through a principled method of converting a switched hybrid dynamical system into a temporal-epistemic logic model, passing through an abstract state machine representation. This creates space for methods and results to be exchanged across the fields of control theory, distributed computing and temporal-epistemic logic, while reasoning about multi-robot systems.
<div id='section'>Paperid: <span id='pid'>321, <a href='https://arxiv.org/pdf/2501.18309.pdf' target='_blank'>https://arxiv.org/pdf/2501.18309.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Giorgio Cignarale, Stephan Felber, Eric Goubault, Bernardo Hummes Flores, Hugo Rincon Galeana
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.18309">Knowledge in multi-robot systems: an interplay of dynamics, computation and communication</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we provide a framework integrating distributed multi-robot systems and temporal epistemic logic. We show that continuous-discrete hybrid systems are compatible with logical models of knowledge already used in distributed computing, and demonstrate its usefulness by deriving sufficient epistemic conditions for exploration and gathering robot tasks to be solvable. We provide a separation of the physical and computational aspects of a robotic system, allowing us to decouple the problems related to each and directly use methods from control theory and distributed computing, fields that are traditionally distant in the literature. Finally, we demonstrate a novel approach for reasoning about the knowledge in multi-robot systems through a principled method of converting a switched hybrid dynamical system into a temporal-epistemic logic model, passing through an abstract state machine representation. This creates space for methods and results to be exchanged across the fields of control theory, distributed computing and temporal-epistemic logic, while reasoning about multi-robot systems.
<div id='section'>Paperid: <span id='pid'>322, <a href='https://arxiv.org/pdf/2501.12263.pdf' target='_blank'>https://arxiv.org/pdf/2501.12263.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bingyi Liu, Jian Teng, Hongfei Xue, Enshu Wang, Chuanhui Zhu, Pu Wang, Libing Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.12263">mmCooper: A Multi-agent Multi-stage Communication-efficient and Collaboration-robust Cooperative Perception Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Collaborative perception significantly enhances individual vehicle perception performance through the exchange of sensory information among agents. However, real-world deployment faces challenges due to bandwidth constraints and inevitable calibration errors during information exchange. To address these issues, we propose mmCooper, a novel multi-agent, multi-stage, communication-efficient, and collaboration-robust cooperative perception framework. Our framework leverages a multi-stage collaboration strategy that dynamically and adaptively balances intermediate- and late-stage information to share among agents, enhancing perceptual performance while maintaining communication efficiency. To support robust collaboration despite potential misalignments and calibration errors, our framework prevents misleading low-confidence sensing information from transmission and refines the received detection results from collaborators to improve accuracy. The extensive evaluation results on both real-world and simulated datasets demonstrate the effectiveness of the mmCooper framework and its components.
<div id='section'>Paperid: <span id='pid'>323, <a href='https://arxiv.org/pdf/2501.03907.pdf' target='_blank'>https://arxiv.org/pdf/2501.03907.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lauren Bramblett, Jonathan Reasoner, Nicola Bezzo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.03907">Implicit Coordination using Active Epistemic Inference for Multi-Robot Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A Multi-robot system (MRS) provides significant advantages for intricate tasks such as environmental monitoring, underwater inspections, and space missions. However, addressing potential communication failures or the lack of communication infrastructure in these fields remains a challenge. A significant portion of MRS research presumes that the system can maintain communication with proximity constraints, but this approach does not solve situations where communication is either non-existent, unreliable, or poses a security risk. Some approaches tackle this issue using predictions about other robots while not communicating, but these methods generally only permit agents to utilize first-order reasoning, which involves reasoning based purely on their own observations. In contrast, to deal with this problem, our proposed framework utilizes Theory of Mind (ToM), employing higher-order reasoning by shifting a robot's perspective to reason about a belief of others observations. Our approach has two main phases: i) an efficient runtime plan adaptation using active inference to signal intentions and reason about a robot's own belief and the beliefs of others in the system, and ii) a hierarchical epistemic planning framework to iteratively reason about the current MRS mission state. The proposed framework outperforms greedy and first-order reasoning approaches and is validated using simulations and experiments with heterogeneous robotic systems.
<div id='section'>Paperid: <span id='pid'>324, <a href='https://arxiv.org/pdf/2412.10083.pdf' target='_blank'>https://arxiv.org/pdf/2412.10083.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dolev Mutzari, Yonatan Aumann, Sarit Kraus
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.10083">Heterogeneous Multi-Robot Graph Coverage with Proximity and Movement Constraints</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Robot Coverage problems have been extensively studied in robotics, planning and multi-agent systems. In this work, we consider the coverage problem when there are constraints on the proximity (e.g., maximum distance between the agents, or a blue agent must be adjacent to a red agent) and the movement (e.g., terrain traversability and material load capacity) of the robots. Such constraints naturally arise in many real-world applications, e.g. in search-and-rescue and maintenance operations. Given such a setting, the goal is to compute a covering tour of the graph with a minimum number of steps, and that adheres to the proximity and movement constraints. For this problem, our contributions are four: (i) a formal formulation of the problem, (ii) an exact algorithm that is FPT in F, d and tw, the set of robot formations that encode the proximity constraints, the maximum nodes degree, and the tree-width of the graph, respectively, (iii) for the case that the graph is a tree: a PTAS approximation scheme, that given an approximation parameter epsilon, produces a tour that is within a epsilon times error(||F||, d) of the optimal one, and the computation runs in time poly(n) times h(1/epsilon,||F||). (iv) for the case that the graph is a tree, with $k=3$ robots, and the constraint is that all agents are connected: a PTAS scheme with multiplicative approximation error of 1+O(epsilon), independent of the maximal degree d.
<div id='section'>Paperid: <span id='pid'>325, <a href='https://arxiv.org/pdf/2410.22031.pdf' target='_blank'>https://arxiv.org/pdf/2410.22031.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Quan Quan, Shuhan Huang, Kai-Yuan Cai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.22031">A Degree of Flowability for Virtual Tubes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid development of robotics swarm technology, there are more tasks that require the swarm to pass through complicated environments safely and efficiently. Virtual tube technology is a novel way to achieve this goal. Virtual tubes are free spaces connecting two places that provide safety boundaries and direction of motion for swarm robotics. How to determine the design quality of a virtual tube is a fundamental problem. For such a purpose, this paper presents a degree of flowability (DOF) for two-dimensional virtual tubes according to a minimum energy principle. After that, methods to calculate DOF are proposed with a feasibility analysis. Simulations of swarm robotics in different kinds of two-dimensional virtual tubes are performed to demonstrate the effectiveness of the proposed method of calculating DOF.
<div id='section'>Paperid: <span id='pid'>326, <a href='https://arxiv.org/pdf/2410.16686.pdf' target='_blank'>https://arxiv.org/pdf/2410.16686.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jumman Hossain, Emon Dey, Snehalraj Chugh, Masud Ahmed, MS Anwar, Abu-Zaher Faridee, Jason Hoppes, Theron Trout, Anjon Basak, Rafidh Chowdhury, Rishabh Mistry, Hyun Kim, Jade Freeman, Niranjan Suri, Adrienne Raglin, Carl Busart, Timothy Gregory, Anuradha Ravi, Nirmalya Roy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.16686">SERN: Simulation-Enhanced Realistic Navigation for Multi-Agent Robotic Systems in Contested Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The increasing deployment of autonomous systems in complex environments necessitates efficient communication and task completion among multiple agents. This paper presents SERN (Simulation-Enhanced Realistic Navigation), a novel framework integrating virtual and physical environments for real-time collaborative decision-making in multi-robot systems. SERN addresses key challenges in asset deployment and coordination through our bi-directional SERN ROS Bridge communication framework. Our approach advances the SOTA through: accurate real-world representation in virtual environments using Unity high-fidelity simulator; synchronization of physical and virtual robot movements; efficient ROS data distribution between remote locations; and integration of SOTA semantic segmentation for enhanced environmental perception. Additionally, we introduce a Multi-Metric Cost Function (MMCF) that dynamically balances latency, reliability, computational overhead, and bandwidth consumption to optimize system performance in contested environments. We further provide theoretical justification for synchronization accuracy by proving that the positional error between physical and virtual robots remains bounded under varying network conditions. Our evaluations show a 15% to 24% improvement in latency and up to a 15% increase in processing efficiency compared to traditional ROS setups. Real-world and virtual simulation experiments with multiple robots (Clearpath Jackal and Husky) demonstrate synchronization accuracy, achieving less than $5\text{ cm}$ positional error and under $2^\circ$ rotational error. These results highlight SERN's potential to enhance situational awareness and multi-agent coordination in diverse, contested environments.
<div id='section'>Paperid: <span id='pid'>327, <a href='https://arxiv.org/pdf/2410.15814.pdf' target='_blank'>https://arxiv.org/pdf/2410.15814.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pei Liu, Nanfang Zheng, Yiqun Li, Junlan Chen, Ziyuan Pu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.15814">Kaninfradet3D:A Road-side Camera-LiDAR Fusion 3D Perception Model based on Nonlinear Feature Extraction and Intrinsic Correlation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the development of AI-assisted driving, numerous methods have emerged for ego-vehicle 3D perception tasks, but there has been limited research on roadside perception. With its ability to provide a global view and a broader sensing range, the roadside perspective is worth developing. LiDAR provides precise three-dimensional spatial information, while cameras offer semantic information. These two modalities are complementary in 3D detection. However, adding camera data does not increase accuracy in some studies since the information extraction and fusion procedure is not sufficiently reliable. Recently, Kolmogorov-Arnold Networks (KANs) have been proposed as replacements for MLPs, which are better suited for high-dimensional, complex data. Both the camera and the LiDAR provide high-dimensional information, and employing KANs should enhance the extraction of valuable features to produce better fusion outcomes. This paper proposes Kaninfradet3D, which optimizes the feature extraction and fusion modules. To extract features from complex high-dimensional data, the model's encoder and fuser modules were improved using KAN Layers. Cross-attention was applied to enhance feature fusion, and visual comparisons verified that camera features were more evenly integrated. This addressed the issue of camera features being abnormally concentrated, negatively impacting fusion. Compared to the benchmark, our approach shows improvements of +9.87 mAP and +10.64 mAP in the two viewpoints of the TUMTraf Intersection Dataset and an improvement of +1.40 mAP in the roadside end of the TUMTraf V2X Cooperative Perception Dataset. The results indicate that Kaninfradet3D can effectively fuse features, demonstrating the potential of applying KANs in roadside perception tasks.
<div id='section'>Paperid: <span id='pid'>328, <a href='https://arxiv.org/pdf/2409.04736.pdf' target='_blank'>https://arxiv.org/pdf/2409.04736.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruoyu Zhou, Zhiwei Zhang, Haocheng Han, Xiaodong Zhang, Zehan Chen, Jun Sun, Yulong Shen, Dehai Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.04736">RSFuzz: A Robustness-Guided Swarm Fuzzing Framework Based on Behavioral Constraints</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-robot swarms play an essential role in complex missions including battlefield reconnaissance, agricultural pest monitoring, as well as disaster search and rescue. Unfortunately, given the complexity of swarm algorithms, logical vulnerabilities are inevitable and often lead to severe safety and security consequences. Although various methods have been presented for detecting logical vulnerabilities through software testing, when they are used in swarm environments, these techniques face significant challenges: 1) Due to the swarm's vast composable parameter space, it is extremely difficult to generate failure-triggering scenarios, which is crucial to effectively expose logical vulnerabilities; 2) Because of the swarm's high flexibility and dynamism, it is challenging to model and evaluate the global swarm state, particularly in terms of cooperative behaviors, which makes it difficult to detect logical vulnerabilities. In this work, we propose RSFuzz, a robustness-guided swarm fuzzing framework designed to detect logical vulnerabilities in multi-robot systems. It leverages the robustness of behavioral constraints to quantitatively evaluate the swarm state and guide the generation of failure-triggering scenarios. In addition, RSFuzz identifies and targets key swarm nodes for perturbations, effectively reducing the input space. Upon the RSFuzz framework, we construct two swarm fuzzing schemes, Single Attacker Fuzzing (SA-Fuzzing) and Multiple Attacker Fuzzing (MA-Fuzzing), which employ single and multiple attackers, respectively, during fuzzing to disturb swarm mission execution. We evaluated RSFuzz's performance with three popular swarm algorithms in simulated environments. The results show that RSFuzz outperforms the state-of-the-art with an average improvement of 17.75\% in effectiveness and a 38.4\% increase in efficiency. We validated some vulnerabilities in real world.
<div id='section'>Paperid: <span id='pid'>329, <a href='https://arxiv.org/pdf/2409.04736.pdf' target='_blank'>https://arxiv.org/pdf/2409.04736.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruoyu Zhou, Zhiwei Zhang, Haocheng Han, Xiaodong Zhang, Zehan Chen, Jun Sun, Yulong Shen, Dehai Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.04736">RSFuzz: A Robustness-Guided Swarm Fuzzing Framework Based on Behavioral Constraints</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-robot swarms play an essential role in complex missions including battlefield reconnaissance, agricultural pest monitoring, as well as disaster search and rescue. Unfortunately, given the complexity of swarm algorithms, logical vulnerabilities are inevitable and often lead to severe safety and security consequences. Although various methods have been presented for detecting logical vulnerabilities through software testing, when they are used in swarm environments, these techniques face significant challenges: 1) Due to the swarm's vast composable parameter space, it is extremely difficult to generate failure-triggering scenarios, which is crucial to effectively expose logical vulnerabilities; 2) Because of the swarm's high flexibility and dynamism, it is challenging to model and evaluate the global swarm state, particularly in terms of cooperative behaviors, which makes it difficult to detect logical vulnerabilities. In this work, we propose RSFuzz, a robustness-guided swarm fuzzing framework designed to detect logical vulnerabilities in multi-robot systems. It leverages the robustness of behavioral constraints to quantitatively evaluate the swarm state and guide the generation of failure-triggering scenarios. In addition, RSFuzz identifies and targets key swarm nodes for perturbations, effectively reducing the input space. Upon the RSFuzz framework, we construct two swarm fuzzing schemes, Single Attacker Fuzzing (SA-Fuzzing) and Multiple Attacker Fuzzing (MA-Fuzzing), which employ single and multiple attackers, respectively, during fuzzing to disturb swarm mission execution. We evaluated RSFuzz's performance with three popular swarm algorithms in simulated environments. The results show that RSFuzz outperforms the state-of-the-art with an average improvement of 17.75\% in effectiveness and a 38.4\% increase in efficiency. We validated some vulnerabilities in real world.
<div id='section'>Paperid: <span id='pid'>330, <a href='https://arxiv.org/pdf/2409.01900.pdf' target='_blank'>https://arxiv.org/pdf/2409.01900.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alexandre Pacheco, SÃ©bastien De Vos, Andreagiovanni Reina, Marco Dorigo, Volker Strobel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.01900">Securing Federated Learning in Robot Swarms using Blockchain Technology</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Federated learning is a new approach to distributed machine learning that offers potential advantages such as reducing communication requirements and distributing the costs of training algorithms. Therefore, it could hold great promise in swarm robotics applications. However, federated learning usually requires a centralized server for the aggregation of the models. In this paper, we present a proof-of-concept implementation of federated learning in a robot swarm that does not compromise decentralization. To do so, we use blockchain technology to enable our robot swarm to securely synchronize a shared model that is the aggregation of the individual models without relying on a central server. We then show that introducing a single malfunctioning robot can, however, heavily disrupt the training process. To prevent such situations, we devise protection mechanisms that are implemented through secure and tamper-proof blockchain smart contracts. Our experiments are conducted in ARGoS, a physics-based simulator for swarm robotics, using the Ethereum blockchain protocol which is executed by each simulated robot.
<div id='section'>Paperid: <span id='pid'>331, <a href='https://arxiv.org/pdf/2408.11155.pdf' target='_blank'>https://arxiv.org/pdf/2408.11155.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vishnu Vijay, Kartik A. Pant, Minhyun Cho, Yifan Guo, James M. Goppert, Inseok Hwang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.11155">Range-based Multi-Robot Integrity Monitoring Against Cyberattacks and Faults: An Anchor-Free Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Coordination of multi-robot systems (MRSs) relies on efficient sensing and reliable communication among the robots. However, the sensors and communication channels of these robots are often vulnerable to cyberattacks and faults, which can disrupt their individual behavior and the overall objective of the MRS. In this work, we present a multi-robot integrity monitoring framework that utilizes inter-robot range measurements to (i) detect the presence of cyberattacks or faults affecting the MRS, (ii) identify the affected robot(s), and (iii) reconstruct the resulting localization error of these robot(s). The proposed iterative algorithm leverages sequential convex programming and alternating direction of multipliers method to enable real-time and distributed implementation. Our approach is validated using numerical simulations and demonstrated using PX4-SiTL in Gazebo on an MRS, where certain agents deviate from their desired position due to a GNSS spoofing attack. Furthermore, we demonstrate the scalability and interoperability of our algorithm through mixed-reality experiments by forming a heterogeneous MRS comprising real Crazyflie UAVs and virtual PX4-SiTL UAVs working in tandem.
<div id='section'>Paperid: <span id='pid'>332, <a href='https://arxiv.org/pdf/2407.06630.pdf' target='_blank'>https://arxiv.org/pdf/2407.06630.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alexandre Pacheco, Ulysse Denis, Raina Zakir, Volker Strobel, Andreagiovanni Reina, Marco Dorigo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.06630">Toychain: A Simple Blockchain for Research in Swarm Robotics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This technical report describes the implementation of Toychain: a simple, lightweight blockchain implemented in Python, designed for ease of deployment and practicality in robotics research. It can be integrated with various software and simulation tools used in robotics (we have integrated it with ARGoS, Gazebo, and ROS2), and also be deployed on real robots capable of Wi-Fi communications. The Toychain package supports the deployment of smart contracts written in Python (computer programs that can be executed by and synchronized across a distributed network). The nodes in the blockchain can execute smart contract functions by broadcasting transactions, which update the state of the blockchain upon agreement by all other nodes. The conditions for this agreement are established by a consensus protocol. The Toychain package allows for custom implementations of the consensus protocol, which can be useful for research or meeting specific application requirements. Currently, Proof-of-Work and Proof-of-Authority are implemented.
<div id='section'>Paperid: <span id='pid'>333, <a href='https://arxiv.org/pdf/2405.01771.pdf' target='_blank'>https://arxiv.org/pdf/2405.01771.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pujie Xin, Zhanteng Xie, Philip Dames
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.01771">Towards Predicting Collective Performance in Multi-Robot Teams</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The increased deployment of multi-robot systems (MRS) in various fields has led to the need for analysis of system-level performance. However, creating consistent metrics for MRS is challenging due to the wide range of system and environmental factors, such as team size and environment size. This paper presents a new analytical framework for MRS based on dimensionless variable analysis, a mathematical technique typically used to simplify complex physical systems. This approach effectively condenses the complex parameters influencing MRS performance into a manageable set of dimensionless variables. We form dimensionless variables which encapsulate key parameters of the robot team and task. Then we use these dimensionless variables to fit a parametric model of team performance. Our model successfully identifies critical performance determinants and their interdependencies, providing insight for MRS design and optimization. The application of dimensionless variable analysis to MRS offers a promising method for MRS analysis that effectively reduces complexity, enhances comprehension of system behaviors, and informs the design and management of future MRS deployments.
<div id='section'>Paperid: <span id='pid'>334, <a href='https://arxiv.org/pdf/2404.18321.pdf' target='_blank'>https://arxiv.org/pdf/2404.18321.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Arash Asgharivaskasi, Fritz Girke, Nikolay Atanasov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.18321">Riemannian Optimization for Active Mapping with Robot Teams</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous exploration of unknown environments using a team of mobile robots demands distributed perception and planning strategies to enable efficient and scalable performance. Ideally, each robot should update its map and plan its motion not only relying on its own observations, but also considering the observations of its peers. Centralized solutions to multi-robot coordination are susceptible to central node failure and require a sophisticated communication infrastructure for reliable operation. Current decentralized active mapping methods consider simplistic robot models with linear-Gaussian observations and Euclidean robot states. In this work, we present a distributed multi-robot mapping and planning method, called Riemannian Optimization for Active Mapping (ROAM). We formulate an optimization problem over a graph with node variables belonging to a Riemannian manifold and a consensus constraint requiring feasible solutions to agree on the node variables. We develop a distributed Riemannian optimization algorithm that relies only on one-hop communication to solve the problem with consensus and optimality guarantees. We show that multi-robot active mapping can be achieved via two applications of our distributed Riemannian optimization over different manifolds: distributed estimation of a 3-D semantic map and distributed planning of SE(3) trajectories that minimize map uncertainty. We demonstrate the performance of ROAM in simulation and real-world experiments using a team of robots with RGB-D cameras.
<div id='section'>Paperid: <span id='pid'>335, <a href='https://arxiv.org/pdf/2403.17392.pdf' target='_blank'>https://arxiv.org/pdf/2403.17392.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yang Bai, Phuoc Thanh Tran Ngoc, Huu Duoc Nguyen, Duc Long Le, Quang Huy Ha, Kazuki Kai, Yu Xiang See To, Yaosheng Deng, Jie Song, Naoki Wakamiya, Hirotaka Sato, Masaki Ogura
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.17392">Swarm navigation of cyborg-insects in unknown obstructed soft terrain</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cyborg insects refer to hybrid robots that integrate living insects with miniature electronic controllers to enable robotic-like programmable control. These creatures exhibit advantages over conventional robots in adaption to complex terrain and sustained energy efficiency. Nevertheless, there is a lack of literature on the control of multi-cyborg systems. This research gap is due to the difficulty in coordinating the movements of a cyborg system under the presence of insects' inherent individual variability in their reactions to control input. Regarding this issue, we propose a swarm navigation algorithm and verify it under experiments. This research advances swarm robotics by integrating biological organisms with control theory to develop intelligent autonomous systems for real-world applications.
<div id='section'>Paperid: <span id='pid'>336, <a href='https://arxiv.org/pdf/2403.13093.pdf' target='_blank'>https://arxiv.org/pdf/2403.13093.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anthony Goeckner, Yueyuan Sui, Nicolas Martinet, Xinliang Li, Qi Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.13093">Graph Neural Network-based Multi-agent Reinforcement Learning for Resilient Distributed Coordination of Multi-Robot Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing multi-agent coordination techniques are often fragile and vulnerable to anomalies such as agent attrition and communication disturbances, which are quite common in the real-world deployment of systems like field robotics. To better prepare these systems for the real world, we present a graph neural network (GNN)-based multi-agent reinforcement learning (MARL) method for resilient distributed coordination of a multi-robot system. Our method, Multi-Agent Graph Embedding-based Coordination (MAGEC), is trained using multi-agent proximal policy optimization (PPO) and enables distributed coordination around global objectives under agent attrition, partial observability, and limited or disturbed communications. We use a multi-robot patrolling scenario to demonstrate our MAGEC method in a ROS 2-based simulator and then compare its performance with prior coordination approaches. Results demonstrate that MAGEC outperforms existing methods in several experiments involving agent attrition and communication disturbance, and provides competitive results in scenarios without such anomalies.
<div id='section'>Paperid: <span id='pid'>337, <a href='https://arxiv.org/pdf/2403.00641.pdf' target='_blank'>https://arxiv.org/pdf/2403.00641.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lauren Bramblett, Branko Miloradovic, Patrick Sherman, Alessandro V. Papadopoulos, Nicola Bezzo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.00641">Robust Online Epistemic Replanning of Multi-Robot Missions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As Multi-Robot Systems (MRS) become more affordable and computing capabilities grow, they provide significant advantages for complex applications such as environmental monitoring, underwater inspections, or space exploration. However, accounting for potential communication loss or the unavailability of communication infrastructures in these application domains remains an open problem. Much of the applicable MRS research assumes that the system can sustain communication through proximity regulations and formation control or by devising a framework for separating and adhering to a predetermined plan for extended periods of disconnection. The latter technique enables an MRS to be more efficient, but breakdowns and environmental uncertainties can have a domino effect throughout the system, particularly when the mission goal is intricate or time-sensitive. To deal with this problem, our proposed framework has two main phases: i) a centralized planner to allocate mission tasks by rewarding intermittent rendezvous between robots to mitigate the effects of the unforeseen events during mission execution, and ii) a decentralized replanning scheme leveraging epistemic planning to formalize belief propagation and a Monte Carlo tree search for policy optimization given distributed rational belief updates. The proposed framework outperforms a baseline heuristic and is validated using simulations and experiments with aerial vehicles.
<div id='section'>Paperid: <span id='pid'>338, <a href='https://arxiv.org/pdf/2401.09321.pdf' target='_blank'>https://arxiv.org/pdf/2401.09321.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Miguel Fernandez-Cortizas, David Perez-Saura, Ricardo Sanz, Martin Molina, Pascual Campoy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.09321">The landscape of Collective Awareness in multi-robot systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The development of collective-aware multi-robot systems is crucial for enhancing the efficiency and robustness of robotic applications in multiple fields. These systems enable collaboration, coordination, and resource sharing among robots, leading to improved scalability, adaptability to dynamic environments, and increased overall system robustness. In this work, we want to provide a brief overview of this research topic and identify open challenges.
<div id='section'>Paperid: <span id='pid'>339, <a href='https://arxiv.org/pdf/2401.07784.pdf' target='_blank'>https://arxiv.org/pdf/2401.07784.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yingjian Wang, Xiangyong Wen, Fei Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.07784">Certifiable Mutual Localization and Trajectory Planning for Bearing-Based Robot Swarm</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Bearing measurements,as the most common modality in nature, have recently gained traction in multi-robot systems to enhance mutual localization and swarm collaboration. Despite their advantages, challenges such as sensory noise, obstacle occlusion, and uncoordinated swarm motion persist in real-world scenarios, potentially leading to erroneous state estimation and undermining the system's flexibility, practicality, and robustness.In response to these challenges, in this paper we address theoretical and practical problem related to both mutual localization and swarm planning.Firstly, we propose a certifiable mutual localization algorithm.It features a concise problem formulation coupled with lossless convex relaxation, enabling independence from initial values and globally optimal relative pose recovery.Then, to explore how detection noise and swarm motion influence estimation optimality, we conduct a comprehensive analysis on the interplay between robots' mutual spatial relationship and mutual localization. We develop a differentiable metric correlated with swarm trajectories to explicitly evaluate the noise resistance of optimal estimation.By establishing a finite and pre-computable threshold for this metric and accordingly generating swarm trajectories, the estimation optimality can be strictly guaranteed under arbitrary noise. Based on these findings, an optimization-based swarm planner is proposed to generate safe and smooth trajectories, with consideration of both inter-robot visibility and estimation optimality.Through numerical simulations, we evaluate the optimality and certifiablity of our estimator, and underscore the significance of our planner in enhancing estimation performance.The results exhibit considerable potential of our methods to pave the way for advanced closed-loop intelligence in swarm systems.
<div id='section'>Paperid: <span id='pid'>340, <a href='https://arxiv.org/pdf/2312.16606.pdf' target='_blank'>https://arxiv.org/pdf/2312.16606.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lavanya Ratnabala, Robinroy Peter, E. Y. A. Charles
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.16606">Evolutionary Swarm Robotics: Dynamic Subgoal-Based Path Formation and Task Allocation for Exploration and Navigation in Unknown Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This research paper addresses the challenges of exploration and navigation in unknown environments from an evolutionary swarm robotics perspective. Path formation plays a crucial role in enabling cooperative swarm robots to accomplish these tasks. The paper presents a method called the sub-goal-based path formation, which establishes a path between two different locations by exploiting visually connected sub-goals. Simulation experiments conducted in the Argos simulator demonstrate the successful formation of paths in the majority of trials.
  Furthermore, the paper tackles the problem of inter-collision (traffic) among a large number of robots engaged in path formation, which negatively impacts the performance of the sub-goal-based method. To mitigate this issue, a task allocation strategy is proposed, leveraging local communication protocols and light signal-based communication. The strategy evaluates the distance between points and determines the required number of robots for the path formation task, reducing unwanted exploration and traffic congestion. The performance of the sub-goal-based path formation and task allocation strategy is evaluated by comparing path length, time, and resource reduction against the A* algorithm. The simulation experiments demonstrate promising results, showcasing the scalability, robustness, and fault tolerance characteristics of the proposed approach.
<div id='section'>Paperid: <span id='pid'>341, <a href='https://arxiv.org/pdf/2310.15846.pdf' target='_blank'>https://arxiv.org/pdf/2310.15846.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Canlun Zheng, Yize Mi, Hanqing Guo, Huaben Chen, Zhiyun Lin, Shiyu Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.15846">Optimal Spatial-Temporal Triangulation for Bearing-Only Cooperative Motion Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-based cooperative motion estimation is an important problem for many multi-robot systems such as cooperative aerial target pursuit. This problem can be formulated as bearing-only cooperative motion estimation, where the visual measurement is modeled as a bearing vector pointing from the camera to the target. The conventional approaches for bearing-only cooperative estimation are mainly based on the framework distributed Kalman filtering (DKF). In this paper, we propose a new optimal bearing-only cooperative estimation algorithm, named spatial-temporal triangulation, based on the method of distributed recursive least squares, which provides a more flexible framework for designing distributed estimators than DKF. The design of the algorithm fully incorporates all the available information and the specific triangulation geometric constraint. As a result, the algorithm has superior estimation performance than the state-of-the-art DKF algorithms in terms of both accuracy and convergence speed as verified by numerical simulation. We rigorously prove the exponential convergence of the proposed algorithm. Moreover, to verify the effectiveness of the proposed algorithm under practical challenging conditions, we develop a vision-based cooperative aerial target pursuit system, which is the first of such fully autonomous systems so far to the best of our knowledge.
<div id='section'>Paperid: <span id='pid'>342, <a href='https://arxiv.org/pdf/2310.12816.pdf' target='_blank'>https://arxiv.org/pdf/2310.12816.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Saray Bakker, Luzia Knoedler, Max Spahn, Wendelin BÃ¶hmer, Javier Alonso-Mora
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.12816">Multi-Robot Local Motion Planning Using Dynamic Optimization Fabrics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we address the problem of real-time motion planning for multiple robotic manipulators that operate in close proximity. We build upon the concept of dynamic fabrics and extend them to multi-robot systems, referred to as Multi-Robot Dynamic Fabrics (MRDF). This geometric method enables a very high planning frequency for high-dimensional systems at the expense of being reactive and prone to deadlocks. To detect and resolve deadlocks, we propose Rollout Fabrics where MRDF are forward simulated in a decentralized manner. We validate the methods in simulated close-proximity pick-and-place scenarios with multiple manipulators, showing high success rates and real-time performance.
<div id='section'>Paperid: <span id='pid'>343, <a href='https://arxiv.org/pdf/2308.00937.pdf' target='_blank'>https://arxiv.org/pdf/2308.00937.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ran Gong, Xiaofeng Gao, Qiaozi Gao, Suhaila Shakiah, Govind Thattai, Gaurav S. Sukhatme
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.00937">LEMMA: Learning Language-Conditioned Multi-Robot Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Complex manipulation tasks often require robots with complementary capabilities to collaborate. We introduce a benchmark for LanguagE-Conditioned Multi-robot MAnipulation (LEMMA) focused on task allocation and long-horizon object manipulation based on human language instructions in a tabletop setting. LEMMA features 8 types of procedurally generated tasks with varying degree of complexity, some of which require the robots to use tools and pass tools to each other. For each task, we provide 800 expert demonstrations and human instructions for training and evaluations. LEMMA poses greater challenges compared to existing benchmarks, as it requires the system to identify each manipulator's limitations and assign sub-tasks accordingly while also handling strong temporal dependencies in each task. To address these challenges, we propose a modular hierarchical planning approach as a baseline. Our results highlight the potential of LEMMA for developing future language-conditioned multi-robot systems.
<div id='section'>Paperid: <span id='pid'>344, <a href='https://arxiv.org/pdf/2308.00579.pdf' target='_blank'>https://arxiv.org/pdf/2308.00579.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lauren Bramblett, Nicola Bezzo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.00579">Epistemic Planning for Heterogeneous Robotic Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In applications such as search and rescue or disaster relief, heterogeneous multi-robot systems (MRS) can provide significant advantages for complex objectives that require a suite of capabilities. However, within these application spaces, communication is often unreliable, causing inefficiencies or outright failures to arise in most MRS algorithms. Many researchers tackle this problem by requiring all robots to either maintain communication using proximity constraints or assuming that all robots will execute a predetermined plan over long periods of disconnection. The latter method allows for higher levels of efficiency in a MRS, but failures and environmental uncertainties can have cascading effects across the system, especially when a mission objective is complex or time-sensitive. To solve this, we propose an epistemic planning framework that allows robots to reason about the system state, leverage heterogeneous system makeups, and optimize information dissemination to disconnected neighbors. Dynamic epistemic logic formalizes the propagation of belief states, and epistemic task allocation and gossip is accomplished via a mixed integer program using the belief states for utility predictions and planning. The proposed framework is validated using simulations and experiments with heterogeneous vehicles.
<div id='section'>Paperid: <span id='pid'>345, <a href='https://arxiv.org/pdf/2305.17181.pdf' target='_blank'>https://arxiv.org/pdf/2305.17181.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hsu-kuang Chiu, Stephen F. Smith
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.17181">Selective Communication for Cooperative Perception in End-to-End Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The reliability of current autonomous driving systems is often jeopardized in situations when the vehicle's field-of-view is limited by nearby occluding objects. To mitigate this problem, vehicle-to-vehicle communication to share sensor information among multiple autonomous driving vehicles has been proposed. However, to enable timely processing and use of shared sensor data, it is necessary to constrain communication bandwidth, and prior work has done so by restricting the number of other cooperative vehicles and randomly selecting the subset of vehicles to exchange information with from all those that are within communication range. Although simple and cost effective from a communication perspective, this selection approach suffers from its susceptibility to missing those vehicles that possess the perception information most critical to navigation planning. Inspired by recent multi-agent path finding research, we propose a novel selective communication algorithm for cooperative perception to address this shortcoming. Implemented with a lightweight perception network and a previously developed control network, our algorithm is shown to produce higher success rates than a random selection approach on previously studied safety-critical driving scenario simulations, with minimal additional communication overhead.
<div id='section'>Paperid: <span id='pid'>346, <a href='https://arxiv.org/pdf/2304.12033.pdf' target='_blank'>https://arxiv.org/pdf/2304.12033.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiying Song, Tenghui Xie, Hailiang Zhang, Jiaxin Liu, Fuxi Wen, Jun Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.12033">A Spatial Calibration Method for Robust Cooperative Perception</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cooperative perception is a promising technique for intelligent and connected vehicles through vehicle-to-everything (V2X) cooperation, provided that accurate pose information and relative pose transforms are available. Nevertheless, obtaining precise positioning information often entails high costs associated with navigation systems. {Hence, it is required to calibrate relative pose information for multi-agent cooperative perception.} This paper proposes a simple but effective object association approach named context-based matching (CBM), which identifies inter-agent object correspondences using intra-agent geometrical context. In detail, this method constructs contexts using the relative position of the detected bounding boxes, followed by local context matching and global consensus maximization. The optimal relative pose transform is estimated based on the matched correspondences, followed by cooperative perception fusion. Extensive experiments are conducted on both the simulated and real-world datasets. Even with larger inter-agent localization errors, high object association precision and decimeter-level relative pose calibration accuracy are achieved among the cooperating agents.
<div id='section'>Paperid: <span id='pid'>347, <a href='https://arxiv.org/pdf/2304.11821.pdf' target='_blank'>https://arxiv.org/pdf/2304.11821.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shunli Ren, Zixing Lei, Zi Wang, Mehrdad Dianati, Yafei Wang, Siheng Chen, Wenjun Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.11821">Interruption-Aware Cooperative Perception for V2X Communication-Aided Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cooperative perception can significantly improve the perception performance of autonomous vehicles beyond the limited perception ability of individual vehicles by exchanging information with neighbor agents through V2X communication. However, most existing work assume ideal communication among agents, ignoring the significant and common \textit{interruption issues} caused by imperfect V2X communication, where cooperation agents can not receive cooperative messages successfully and thus fail to achieve cooperative perception, leading to safety risks. To fully reap the benefits of cooperative perception in practice, we propose V2X communication INterruption-aware COoperative Perception (V2X-INCOP), a cooperative perception system robust to communication interruption for V2X communication-aided autonomous driving, which leverages historical cooperation information to recover missing information due to the interruptions and alleviate the impact of the interruption issue. To achieve comprehensive recovery, we design a communication-adaptive multi-scale spatial-temporal prediction model to extract multi-scale spatial-temporal features based on V2X communication conditions and capture the most significant information for the prediction of the missing information. To further improve recovery performance, we adopt a knowledge distillation framework to give explicit and direct supervision to the prediction model and a curriculum learning strategy to stabilize the training of the model. Experiments on three public cooperative perception datasets demonstrate that the proposed method is effective in alleviating the impacts of communication interruption on cooperative perception.
<div id='section'>Paperid: <span id='pid'>348, <a href='https://arxiv.org/pdf/2302.13629.pdf' target='_blank'>https://arxiv.org/pdf/2302.13629.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohsen Raoufi, Pawel Romanczuk, Heiko Hamann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.13629">Estimation of continuous environments by robot swarms: Correlated networks and decision-making</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Collective decision-making is an essential capability of large-scale multi-robot systems to establish autonomy on the swarm level. A large portion of literature on collective decision-making in swarm robotics focuses on discrete decisions selecting from a limited number of options. Here we assign a decentralized robot system with the task of exploring an unbounded environment, finding consensus on the mean of a measurable environmental feature, and aggregating at areas where that value is measured (e.g., a contour line). A unique quality of this task is a causal loop between the robots' dynamic network topology and their decision-making. For example, the network's mean node degree influences time to convergence while the currently agreed-on mean value influences the swarm's aggregation location, hence, also the network structure as well as the precision error. We propose a control algorithm and study it in real-world robot swarm experiments in different environments. We show that our approach is effective and achieves higher precision than a control experiment. We anticipate applications, for example, in containing pollution with surface vehicles.
<div id='section'>Paperid: <span id='pid'>349, <a href='https://arxiv.org/pdf/2301.10704.pdf' target='_blank'>https://arxiv.org/pdf/2301.10704.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kacper Wardega, Max von Hippel, Roberto Tron, Cristina Nita-Rotaru, Wenchao Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.10704">HoLA Robots: Mitigating Plan-Deviation Attacks in Multi-Robot Systems with Co-Observations and Horizon-Limiting Announcements</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Emerging multi-robot systems rely on cooperation between humans and robots, with robots following automatically generated motion plans to service application-level tasks. Given the safety requirements associated with operating in proximity to humans and expensive infrastructure, it is important to understand and mitigate the security vulnerabilities of such systems caused by compromised robots who diverge from their assigned plans. We focus on centralized systems, where a *central entity* (CE) is responsible for determining and transmitting the motion plans to the robots, which report their location as they move following the plan. The CE checks that robots follow their assigned plans by comparing their expected location to the location they self-report. We show that this self-reporting monitoring mechanism is vulnerable to *plan-deviation attacks* where compromised robots don't follow their assigned plans while trying to conceal their movement by mis-reporting their location. We propose a two-pronged mitigation for plan-deviation attacks: (1) an attack detection technique leveraging both the robots' local sensing capabilities to report observations of other robots and *co-observation schedules* generated by the CE, and (2) a prevention technique where the CE issues *horizon-limiting announcements* to the robots, reducing their instantaneous knowledge of forward lookahead steps in the global motion plan. On a large-scale automated warehouse benchmark, we show that our solution enables attack prevention guarantees from a stealthy attacker that has compromised multiple robots.
<div id='section'>Paperid: <span id='pid'>350, <a href='https://arxiv.org/pdf/2301.06977.pdf' target='_blank'>https://arxiv.org/pdf/2301.06977.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kacper Wardega, Max von Hippel, Roberto Tron, Cristina Nita-Rotaru, Wenchao Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.06977">Byzantine Resilience at Swarm Scale: A Decentralized Blocklist Protocol from Inter-robot Accusations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Weighted-Mean Subsequence Reduced (W-MSR) algorithm, the state-of-the-art method for Byzantine-resilient design of decentralized multi-robot systems, is based on discarding outliers received over Linear Consensus Protocol (LCP). Although W-MSR provides well-understood theoretical guarantees relating robust network connectivity to the convergence of the underlying consensus, the method comes with several limitations preventing its use at scale: (1) the number of Byzantine robots, F, to tolerate should be known a priori, (2) the requirement that each robot maintains 2F+1 neighbors is impractical for large F, (3) information propagation is hindered by the requirement that F+1 robots independently make local measurements of the consensus property in order for the swarm's decision to change, and (4) W-MSR is specific to LCP and does not generalize to applications not implemented over LCP. In this work, we propose a Decentralized Blocklist Protocol (DBP) based on inter-robot accusations. Accusations are made on the basis of locally-made observations of misbehavior, and once shared by cooperative robots across the network are used as input to a graph matching algorithm that computes a blocklist. DBP generalizes to applications not implemented via LCP, is adaptive to the number of Byzantine robots, and allows for fast information propagation through the multi-robot system while simultaneously reducing the required network connectivity relative to W-MSR. On LCP-type applications, DBP reduces the worst-case connectivity requirement of W-MSR from (2F+1)-connected to (F+1)-connected and the number of cooperative observers required to propagate new information from F+1 to just 1 observer. We demonstrate empirically that our approach to Byzantine resilience scales to hundreds of robots on cooperative target tracking, time synchronization, and localization case studies.
<div id='section'>Paperid: <span id='pid'>351, <a href='https://arxiv.org/pdf/2210.06289.pdf' target='_blank'>https://arxiv.org/pdf/2210.06289.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiying Song, Fuxi Wen, Hailiang Zhang, Jun Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.06289">A Cooperative Perception System Robust to Localization Errors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cooperative perception is challenging for safety-critical autonomous driving applications.The errors in the shared position and pose cause an inaccurate relative transform estimation and disrupt the robust mapping of the Ego vehicle. We propose a distributed object-level cooperative perception system called OptiMatch, in which the detected 3D bounding boxes and local state information are shared between the connected vehicles. To correct the noisy relative transform, the local measurements of both connected vehicles (bounding boxes) are utilized, and an optimal transport theory-based algorithm is developed to filter out those objects jointly detected by the vehicles along with their correspondence, constructing an associated co-visible set. A correction transform is estimated from the matched object pairs and further applied to the noisy relative transform, followed by global fusion and dynamic mapping. Experiment results show that robust performance is achieved for different levels of location and heading errors, and the proposed framework outperforms the state-of-the-art benchmark fusion schemes, including early, late, and intermediate fusion, on average precision by a large margin when location and/or heading errors occur.
<div id='section'>Paperid: <span id='pid'>352, <a href='https://arxiv.org/pdf/2207.07824.pdf' target='_blank'>https://arxiv.org/pdf/2207.07824.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenyuan Yuan, Minghui Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2207.07824">Distributed Safe Learning and Planning for Multi-robot Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper considers the problem of online multi-robot motion planning with general nonlinear dynamics subject to unknown external disturbances. We propose dSLAP, a distributed safe learning and planning framework that allows the robots to safely navigate through the environments by coupling online learning and motion planning. Gaussian process regression is used to online learn the disturbances with uncertainty quantification. The planning algorithm ensures collision avoidance against the learning uncertainty and utilizes set-valued analysis to achieve fast adaptation in response to the newly learned models. A set-valued model predictive control problem is then formulated and solved to return a control policy that balances between actively exploring the unknown disturbances and reaching goal regions. Sufficient conditions are established to guarantee the safety of the robots in the absence of backup policy. Monte Carlo simulations are conducted for evaluation.
<div id='section'>Paperid: <span id='pid'>353, <a href='https://arxiv.org/pdf/2510.03504.pdf' target='_blank'>https://arxiv.org/pdf/2510.03504.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yutong Wang, Yichun Qu, Tengxiang Wang, Lishuo Pan, Nora Ayanian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03504">Distributed Connectivity Maintenance and Recovery for Quadrotor Motion Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Maintaining connectivity is crucial in many multi-robot applications, yet fragile to obstacles and visual occlusions. We present a real-time distributed framework for multi-robot navigation certified by high-order control barrier functions (HOCBFs) that controls inter-robot proximity to maintain connectivity while avoiding collisions. We incorporate control Lyapunov functions to enable connectivity recovery from initial disconnected configurations and temporary losses, providing robust connectivity during navigation in obstacle-rich environments. Our trajectory generation framework concurrently produces planning and control through a Bezier-parameterized trajectory, which naturally provides smooth curves with arbitrary degree of derivatives. The main contribution is the unified MPC-CLF-CBF framework, a continuous-time trajectory generation and control method for connectivity maintenance and recovery of multi-robot systems. We validate the framework through extensive simulations and a physical experiment with 4 Crazyflie nano-quadrotors.
<div id='section'>Paperid: <span id='pid'>354, <a href='https://arxiv.org/pdf/2510.03504.pdf' target='_blank'>https://arxiv.org/pdf/2510.03504.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yutong Wang, Yichun Qu, Tengxiang Wang, Lishuo Pan, Nora Ayanian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03504">Distributed Connectivity Maintenance and Recovery for Quadrotor Motion Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Maintaining connectivity is crucial in many multi-robot applications, yet fragile to obstacles and visual occlusions. We present a real-time distributed framework for multi-robot navigation certified by high-order control barrier functions (HOCBFs) that controls inter-robot proximity to maintain connectivity while avoiding collisions. We incorporate control Lyapunov functions to enable connectivity recovery from initial disconnected configurations and temporary losses, providing robust connectivity during navigation in obstacle-rich environments. Our trajectory generation framework concurrently produces planning and control through a Bezier-parameterized trajectory, which naturally provides smooth curves with arbitrary degree of derivatives. The main contribution is the unified MPC-CLF-CBF framework, a continuous-time trajectory generation and control method for connectivity maintenance and recovery of multi-robot systems. We validate the framework through extensive simulations and a physical experiment with 4 Crazyflie nano-quadrotors.
<div id='section'>Paperid: <span id='pid'>355, <a href='https://arxiv.org/pdf/2509.24903.pdf' target='_blank'>https://arxiv.org/pdf/2509.24903.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lantao Li, Kang Yang, Rui Song, Chen Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24903">DRCP: Diffusion on Reinforced Cooperative Perception for Perceiving Beyond Limits</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cooperative perception enabled by Vehicle-to-Everything communication has shown great promise in enhancing situational awareness for autonomous vehicles and other mobile robotic platforms. Despite recent advances in perception backbones and multi-agent fusion, real-world deployments remain challenged by hard detection cases, exemplified by partial detections and noise accumulation which limit downstream detection accuracy. This work presents Diffusion on Reinforced Cooperative Perception (DRCP), a real-time deployable framework designed to address aforementioned issues in dynamic driving environments. DRCP integrates two key components: (1) Precise-Pyramid-Cross-Modality-Cross-Agent, a cross-modal cooperative perception module that leverages camera-intrinsic-aware angular partitioning for attention-based fusion and adaptive convolution to better exploit external features; and (2) Mask-Diffusion-Mask-Aggregation, a novel lightweight diffusion-based refinement module that encourages robustness against feature perturbations and aligns bird's-eye-view features closer to the task-optimal manifold. The proposed system achieves real-time performance on mobile platforms while significantly improving robustness under challenging conditions. Code will be released in late 2025.
<div id='section'>Paperid: <span id='pid'>356, <a href='https://arxiv.org/pdf/2509.24903.pdf' target='_blank'>https://arxiv.org/pdf/2509.24903.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lantao Li, Kang Yang, Rui Song, Chen Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24903">DRCP: Diffusion on Reinforced Cooperative Perception for Perceiving Beyond Limits</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cooperative perception enabled by Vehicle-to-Everything communication has shown great promise in enhancing situational awareness for autonomous vehicles and other mobile robotic platforms. Despite recent advances in perception backbones and multi-agent fusion, real-world deployments remain challenged by hard detection cases, exemplified by partial detections and noise accumulation which limit downstream detection accuracy. This work presents Diffusion on Reinforced Cooperative Perception (DRCP), a real-time deployable framework designed to address aforementioned issues in dynamic driving environments. DRCP integrates two key components: (1) Precise-Pyramid-Cross-Modality-Cross-Agent, a cross-modal cooperative perception module that leverages camera-intrinsic-aware angular partitioning for attention-based fusion and adaptive convolution to better exploit external features; and (2) Mask-Diffusion-Mask-Aggregation, a novel lightweight diffusion-based refinement module that encourages robustness against feature perturbations and aligns bird's-eye-view features closer to the task-optimal manifold. The proposed system achieves real-time performance on mobile platforms while significantly improving robustness under challenging conditions. Code will be released in late 2025.
<div id='section'>Paperid: <span id='pid'>357, <a href='https://arxiv.org/pdf/2509.15807.pdf' target='_blank'>https://arxiv.org/pdf/2509.15807.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuyang Zhang, Zhuoli Tian, Jinsheng Wei, Meng Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15807">FlyKites: Human-centric Interactive Exploration and Assistance under Limited Communication</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fleets of autonomous robots have been deployed for exploration of unknown scenes for features of interest, e.g., subterranean exploration, reconnaissance, search and rescue missions. During exploration, the robots may encounter un-identified targets, blocked passages, interactive objects, temporary failure, or other unexpected events, all of which require consistent human assistance with reliable communication for a time period. This however can be particularly challenging if the communication among the robots is severely restricted to only close-range exchange via ad-hoc networks, especially in extreme environments like caves and underground tunnels. This paper presents a novel human-centric interactive exploration and assistance framework called FlyKites, for multi-robot systems under limited communication. It consists of three interleaved components: (I) the distributed exploration and intermittent communication (called the "spread mode"), where the robots collaboratively explore the environment and exchange local data among the fleet and with the operator; (II) the simultaneous optimization of the relay topology, the operator path, and the assignment of robots to relay roles (called the "relay mode"), such that all requested assistance can be provided with minimum delay; (III) the human-in-the-loop online execution, where the robots switch between different roles and interact with the operator adaptively. Extensive human-in-the-loop simulations and hardware experiments are performed over numerous challenging scenes.
<div id='section'>Paperid: <span id='pid'>358, <a href='https://arxiv.org/pdf/2509.15807.pdf' target='_blank'>https://arxiv.org/pdf/2509.15807.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuyang Zhang, Zhuoli Tian, Jinsheng Wei, Meng Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15807">FlyKites: Human-centric Interactive Exploration and Assistance under Limited Communication</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fleets of autonomous robots have been deployed for exploration of unknown scenes for features of interest, e.g., subterranean exploration, reconnaissance, search and rescue missions. During exploration, the robots may encounter un-identified targets, blocked passages, interactive objects, temporary failure, or other unexpected events, all of which require consistent human assistance with reliable communication for a time period. This however can be particularly challenging if the communication among the robots is severely restricted to only close-range exchange via ad-hoc networks, especially in extreme environments like caves and underground tunnels. This paper presents a novel human-centric interactive exploration and assistance framework called FlyKites, for multi-robot systems under limited communication. It consists of three interleaved components: (I) the distributed exploration and intermittent communication (called the "spread mode"), where the robots collaboratively explore the environment and exchange local data among the fleet and with the operator; (II) the simultaneous optimization of the relay topology, the operator path, and the assignment of robots to relay roles (called the "relay mode"), such that all requested assistance can be provided with minimum delay; (III) the human-in-the-loop online execution, where the robots switch between different roles and interact with the operator adaptively. Extensive human-in-the-loop simulations and hardware experiments are performed over numerous challenging scenes.
<div id='section'>Paperid: <span id='pid'>359, <a href='https://arxiv.org/pdf/2509.10968.pdf' target='_blank'>https://arxiv.org/pdf/2509.10968.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Leo Cazenille, Loona Macabre, Nicolas Bredeche
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.10968">Pogosim -- a Simulator for Pogobot robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pogobots are a new type of open-source/open-hardware robots specifically designed for swarm robotics research. Their cost-effective and modular design, complemented by vibration-based and wheel-based locomotion, fast infrared communication and extensive software architecture facilitate the implementation of swarm intelligence algorithms. However, testing even simple distributed algorithms directly on robots is particularly labor-intensive. Scaling to more complex problems or calibrate user code parameters will have a prohibitively high strain on available resources. In this article we present Pogosim, a fast and scalable simulator for Pogobots, designed to reduce as much as possible algorithm development costs. The exact same code will be used in both simulation and to experimentally drive real robots. This article details the software architecture of Pogosim, explain how to write configuration files and user programs and how simulations approximate or differ from experiments. We describe how a large set of simulations can be launched in parallel, how to retrieve and analyze the simulation results, and how to optimize user code parameters using optimization algorithms.
<div id='section'>Paperid: <span id='pid'>360, <a href='https://arxiv.org/pdf/2509.10968.pdf' target='_blank'>https://arxiv.org/pdf/2509.10968.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Leo Cazenille, Loona Macabre, Nicolas Bredeche
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.10968">Pogosim -- a Simulator for Pogobot robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pogobots are a new type of open-source/open-hardware robots specifically designed for swarm robotics research. Their cost-effective and modular design, complemented by vibration-based and wheel-based locomotion, fast infrared communication and extensive software architecture facilitate the implementation of swarm intelligence algorithms. However, testing even simple distributed algorithms directly on robots is particularly labor-intensive. Scaling to more complex problems or calibrate user code parameters will have a prohibitively high strain on available resources. In this article we present Pogosim, a fast and scalable simulator for Pogobots, designed to reduce as much as possible algorithm development costs. The exact same code will be used in both simulation and to experimentally drive real robots. This article details the software architecture of Pogosim, explain how to write configuration files and user programs and how simulations approximate or differ from experiments. We describe how a large set of simulations can be launched in parallel, how to retrieve and analyze the simulation results, and how to optimize user code parameters using optimization algorithms.
<div id='section'>Paperid: <span id='pid'>361, <a href='https://arxiv.org/pdf/2508.16030.pdf' target='_blank'>https://arxiv.org/pdf/2508.16030.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinyue Song, Hansol Ku, Jayneel Vora, Nelson Lee, Ahmad Kamari, Prasant Mohapatra, Parth Pathak
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.16030">CoVeRaP: Cooperative Vehicular Perception through mmWave FMCW Radars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automotive FMCW radars remain reliable in rain and glare, yet their sparse, noisy point clouds constrain 3-D object detection. We therefore release CoVeRaP, a 21 k-frame cooperative dataset that time-aligns radar, camera, and GPS streams from multiple vehicles across diverse manoeuvres. Built on this data, we propose a unified cooperative-perception framework with middle- and late-fusion options. Its baseline network employs a multi-branch PointNet-style encoder enhanced with self-attention to fuse spatial, Doppler, and intensity cues into a common latent space, which a decoder converts into 3-D bounding boxes and per-point depth confidence. Experiments show that middle fusion with intensity encoding boosts mean Average Precision by up to 9x at IoU 0.9 and consistently outperforms single-vehicle baselines. CoVeRaP thus establishes the first reproducible benchmark for multi-vehicle FMCW-radar perception and demonstrates that affordable radar sharing markedly improves detection robustness. Dataset and code are publicly available to encourage further research.
<div id='section'>Paperid: <span id='pid'>362, <a href='https://arxiv.org/pdf/2505.03528.pdf' target='_blank'>https://arxiv.org/pdf/2505.03528.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenguang Liu, Jianjun Chen, Yunfei Chen, Yubei He, Zhuangkun Wei, Hongjian Sun, Haiyan Lu, Qi Hao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.03528">Coop-WD: Cooperative Perception with Weighting and Denoising for Robust V2V Communication</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cooperative perception, leveraging shared information from multiple vehicles via vehicle-to-vehicle (V2V) communication, plays a vital role in autonomous driving to alleviate the limitation of single-vehicle perception. Existing works have explored the effects of V2V communication impairments on perception precision, but they lack generalization to different levels of impairments. In this work, we propose a joint weighting and denoising framework, Coop-WD, to enhance cooperative perception subject to V2V channel impairments. In this framework, the self-supervised contrastive model and the conditional diffusion probabilistic model are adopted hierarchically for vehicle-level and pixel-level feature enhancement. An efficient variant model, Coop-WD-eco, is proposed to selectively deactivate denoising to reduce processing overhead. Rician fading, non-stationarity, and time-varying distortion are considered. Simulation results demonstrate that the proposed Coop-WD outperforms conventional benchmarks in all types of channels. Qualitative analysis with visual examples further proves the superiority of our proposed method. The proposed Coop-WD-eco achieves up to 50% reduction in computational cost under severe distortion while maintaining comparable accuracy as channel conditions improve.
<div id='section'>Paperid: <span id='pid'>363, <a href='https://arxiv.org/pdf/2504.15876.pdf' target='_blank'>https://arxiv.org/pdf/2504.15876.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qizhen Wu, Lei Chen, Kexin Liu, Jinhu Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.15876">Bidirectional Task-Motion Planning Based on Hierarchical Reinforcement Learning for Strategic Confrontation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In swarm robotics, confrontation scenarios, including strategic confrontations, require efficient decision-making that integrates discrete commands and continuous actions. Traditional task and motion planning methods separate decision-making into two layers, but their unidirectional structure fails to capture the interdependence between these layers, limiting adaptability in dynamic environments. Here, we propose a novel bidirectional approach based on hierarchical reinforcement learning, enabling dynamic interaction between the layers. This method effectively maps commands to task allocation and actions to path planning, while leveraging cross-training techniques to enhance learning across the hierarchical framework. Furthermore, we introduce a trajectory prediction model that bridges abstract task representations with actionable planning goals. In our experiments, it achieves over 80% in confrontation win rate and under 0.01 seconds in decision time, outperforming existing approaches. Demonstrations through large-scale tests and real-world robot experiments further emphasize the generalization capabilities and practical applicability of our method.
<div id='section'>Paperid: <span id='pid'>364, <a href='https://arxiv.org/pdf/2504.08686.pdf' target='_blank'>https://arxiv.org/pdf/2504.08686.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alessia Loi, Loona Macabre, JÃ©rÃ©my Fersula, Keivan Amini, Leo Cazenille, Fabien Caura, Alexandre Guerre, StÃ©phane Gourichon, Olivier Dauchot, Nicolas Bredeche
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.08686">Pobogot -- An Open-Hardware Open-Source Low Cost Robot for Swarm Robotics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper describes the Pogobot, an open-source and open-hardware platform specifically designed for research involving swarm robotics. Pogobot features vibration-based locomotion, infrared communication, and an array of sensors in a cost-effective package (approx. 250~euros/unit). The platform's modular design, comprehensive API, and extensible architecture facilitate the implementation of swarm intelligence algorithms and distributed online reinforcement learning algorithms. Pogobots offer an accessible alternative to existing platforms while providing advanced capabilities including directional communication between units. More than 200 Pogobots are already being used on a daily basis at Sorbonne UniversitÃ© and PSL to study self-organizing systems, programmable active matter, discrete reaction-diffusion-advection systems as well as models of social learning and evolution.
<div id='section'>Paperid: <span id='pid'>365, <a href='https://arxiv.org/pdf/2503.12787.pdf' target='_blank'>https://arxiv.org/pdf/2503.12787.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Takumi Ito, Riku Funada, Mitsuji Sampei, Gennaro Notomista
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.12787">Energy-Aware Task Allocation for Teams of Multi-mode Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work proposes a novel multi-robot task allocation framework for robots that can switch between multiple modes, e.g., flying, driving, or walking. We first provide a method to encode the multi-mode property of robots as a graph, where the mode of each robot is represented by a node. Next, we formulate a constrained optimization problem to decide both the task to be allocated to each robot as well as the mode in which the latter should execute the task. The robot modes are optimized based on the state of the robot and the environment, as well as the energy required to execute the allocated task. Moreover, the proposed framework is able to encompass kinematic and dynamic models of robots alike. Furthermore, we provide sufficient conditions for the convergence of task execution and allocation for both robot models.
<div id='section'>Paperid: <span id='pid'>366, <a href='https://arxiv.org/pdf/2502.01009.pdf' target='_blank'>https://arxiv.org/pdf/2502.01009.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lishuo Pan, Mattia Catellani, Lorenzo Sabattini, Nora Ayanian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.01009">Robust Trajectory Generation and Control for Quadrotor Motion Planning with Field-of-View Control Barrier Certification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Many approaches to multi-robot coordination are susceptible to failure due to communication loss and uncertainty in estimation. We present a real-time communication-free distributed algorithm for navigating robots to their desired goals certified by control barrier functions, that model and control the onboard sensing behavior to keep neighbors in the limited field of view for position estimation. The approach is robust to temporary tracking loss and directly synthesizes control in real time to stabilize visual contact through control Lyapunov-barrier functions. The main contributions of this paper are a continuous-time robust trajectory generation and control method certified by control barrier functions for distributed multi-robot systems and a discrete optimization procedure, namely, MPC-CBF, to approximate the certified controller. In addition, we propose a linear surrogate of high-order control barrier function constraints and use sequential quadratic programming to solve MPC-CBF efficiently. We demonstrate results in simulation with 10 robots and physical experiments with 2 custom-built UAVs. To the best of our knowledge, this work is the first of its kind to generate a robust continuous-time trajectory and controller concurrently, certified by control barrier functions utilizing piecewise splines.
<div id='section'>Paperid: <span id='pid'>367, <a href='https://arxiv.org/pdf/2501.06058.pdf' target='_blank'>https://arxiv.org/pdf/2501.06058.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kevin Fu, Shalin Anand Jain, Pierce Howell, Harish Ravichandar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.06058">Capability-Aware Shared Hypernetworks for Flexible Heterogeneous Multi-Robot Coordination</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances have enabled heterogeneous multi-robot teams to learn complex and effective coordination skills. However, existing neural architectures that support heterogeneous teaming tend to force a trade-off between expressivity and efficiency. Shared-parameter designs prioritize sample efficiency by enabling a single network to be shared across all or a pre-specified subset of robots (via input augmentations), but tend to limit behavioral diversity. In contrast, recent designs employ a separate policy for each robot, enabling greater diversity and expressivity at the cost of efficiency and generalization. Our key insight is that such tradeoffs can be avoided by viewing these design choices as ends of a broad spectrum. Inspired by recent work in transfer and meta learning, and building on prior work in multi-robot task allocation, we propose Capability-Aware Shared Hypernetworks (CASH), a soft weight sharing architecture that uses hypernetworks to efficiently learn a flexible shared policy that dynamically adapts to each robot post-training. By explicitly encoding the impact of robot capabilities (e.g., speed and payload) on collective behavior, CASH enables zero-shot generalization to unseen robots or team compositions. Our experiments involve multiple heterogeneous tasks, three learning paradigms (imitation learning, value-based, and policy-gradient RL), and SOTA multi-robot simulation (JaxMARL) and hardware (Robotarium) platforms. Across all conditions, we find that CASH generates appropriately-diverse behaviors and consistently outperforms baseline architectures in terms of performance and sample efficiency during both training and zero-shot generalization, all with 60%-80% fewer learnable parameters.
<div id='section'>Paperid: <span id='pid'>368, <a href='https://arxiv.org/pdf/2501.06058.pdf' target='_blank'>https://arxiv.org/pdf/2501.06058.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kevin Fu, Shalin Anand Jain, Pierce Howell, Harish Ravichandar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.06058">Capability-Aware Shared Hypernetworks for Flexible Heterogeneous Multi-Robot Coordination</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances have enabled heterogeneous multi-robot teams to learn complex and effective coordination skills. However, existing neural architectures that support heterogeneous teaming tend to force a trade-off between expressivity and efficiency. Shared-parameter designs prioritize sample efficiency by enabling a single network to be shared across all or a pre-specified subset of robots (via input augmentations), but tend to limit behavioral diversity. In contrast, recent designs employ a separate policy for each robot, enabling greater diversity and expressivity at the cost of efficiency and generalization. Our key insight is that such tradeoffs can be avoided by viewing these design choices as ends of a broad spectrum. Inspired by recent work in transfer and meta learning, and building on prior work in multi-robot task allocation, we propose Capability-Aware Shared Hypernetworks (CASH), a soft weight sharing architecture that uses hypernetworks to efficiently learn a flexible shared policy that dynamically adapts to each robot post-training. By explicitly encoding the impact of robot capabilities (e.g., speed and payload) on collective behavior, CASH enables zero-shot generalization to unseen robots or team compositions. Our experiments involve multiple heterogeneous tasks, three learning paradigms (imitation learning, value-based, and policy-gradient RL), and SOTA multi-robot simulation (JaxMARL) and hardware (Robotarium) platforms. Across all conditions, we find that CASH generates appropriately-diverse behaviors and consistently outperforms baseline architectures in terms of performance and sample efficiency during both training and zero-shot generalization, all with 60%-80% fewer learnable parameters.
<div id='section'>Paperid: <span id='pid'>369, <a href='https://arxiv.org/pdf/2411.01707.pdf' target='_blank'>https://arxiv.org/pdf/2411.01707.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingtao Tang, Zining Mao, Hang Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.01707">Large-Scale Multirobot Coverage Path Planning on Grids With Path Deconfliction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We study Multi-Robot Coverage Path Planning (MCPP) on a 4-neighbor 2D grid G, which aims to compute paths for multiple robots to cover all cells of G. Traditional approaches are limited as they first compute coverage trees on a quadrant coarsened grid H and then employ the Spanning Tree Coverage (STC) paradigm to generate paths on G, making them inapplicable to grids with partially obstructed 2x2 blocks. To address this limitation, we reformulate the problem directly on G, revolutionizing grid-based MCPP solving and establishing new NP-hardness results. We introduce Extended-STC (ESTC), a novel paradigm that extends STC to ensure complete coverage with bounded suboptimality, even when H includes partially obstructed blocks. Furthermore, we present LS-MCPP, a new algorithmic framework that integrates ESTC with three novel types of neighborhood operators within a local search strategy to optimize coverage paths directly on G. Unlike prior grid-based MCPP work, our approach also incorporates a versatile post-processing procedure that applies Multi-Agent Path Finding (MAPF) techniques to MCPP for the first time, enabling a fusion of these two important fields in multi-robot coordination. This procedure effectively resolves inter-robot conflicts and accommodates turning costs by solving a MAPF variant, making our MCPP solutions more practical for real-world applications. Extensive experiments demonstrate that our approach significantly improves solution quality and efficiency, managing up to 100 robots on grids as large as 256x256 within minutes of runtime. Validation with physical robots confirms the feasibility of our solutions under real-world conditions.
<div id='section'>Paperid: <span id='pid'>370, <a href='https://arxiv.org/pdf/2411.01274.pdf' target='_blank'>https://arxiv.org/pdf/2411.01274.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qingquan Lin, Weining Lu, Litong Meng, Chenxi Li, Bin Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.01274">Efficient Collaborative Navigation through Perception Fusion for Multi-Robots in Unknown Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>For tasks conducted in unknown environments with efficiency requirements, real-time navigation of multi-robot systems remains challenging due to unfamiliarity with surroundings.In this paper, we propose a novel multi-robot collaborative planning method that leverages the perception of different robots to intelligently select search directions and improve planning efficiency. Specifically, a foundational planner is employed to ensure reliable exploration towards targets in unknown environments and we introduce Graph Attention Architecture with Information Gain Weight(GIWT) to synthesizes the information from the target robot and its teammates to facilitate effective navigation around obstacles.In GIWT, after regionally encoding the relative positions of the robots along with their perceptual features, we compute the shared attention scores and incorporate the information gain obtained from neighboring robots as a supplementary weight. We design a corresponding expert data generation scheme to simulate real-world decision-making conditions for network training. Simulation experiments and real robot tests demonstrates that the proposed method significantly improves efficiency and enables collaborative planning for multiple robots. Our method achieves approximately 82% accuracy on the expert dataset and reduces the average path length by about 8% and 6% across two types of tasks compared to the fundamental planner in ROS tests, and a path length reduction of over 6% in real-world experiments.
<div id='section'>Paperid: <span id='pid'>371, <a href='https://arxiv.org/pdf/2410.02510.pdf' target='_blank'>https://arxiv.org/pdf/2410.02510.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>James Gao, Jacob Lee, Yuting Zhou, Yunze Hu, Chang Liu, Pingping Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.02510">SwarmCVT: Centroidal Voronoi Tessellation-Based Path Planning for Very-Large-Scale Robotics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Swarm robotics, or very large-scale robotics (VLSR), has many meaningful applications for complicated tasks. However, the complexity of motion control and energy costs stack up quickly as the number of robots increases. In addressing this problem, our previous studies have formulated various methods employing macroscopic and microscopic approaches. These methods enable microscopic robots to adhere to a reference Gaussian mixture model (GMM) distribution observed at the macroscopic scale. As a result, optimizing the macroscopic level will result in an optimal overall result. However, all these methods require systematic and global generation of Gaussian components (GCs) within obstacle-free areas to construct the GMM trajectories. This work utilizes centroidal Voronoi tessellation to generate GCs methodically. Consequently, it demonstrates performance improvement while also ensuring consistency and reliability.
<div id='section'>Paperid: <span id='pid'>372, <a href='https://arxiv.org/pdf/2409.10049.pdf' target='_blank'>https://arxiv.org/pdf/2409.10049.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenxi Li, Weining Lu, Qingquan Lin, Litong Meng, Haolu Li, Bin Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.10049">Nav-SCOPE: Swarm Robot Cooperative Perception and Coordinated Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper proposes a lightweight systematic solution for multi-robot coordinated navigation with decentralized cooperative perception. An information flow is first created to facilitate real-time observation sharing over unreliable ad-hoc networks. Then, the environmental uncertainties of each robot are reduced by interaction fields that deliver complementary information. Finally, path optimization is achieved, enabling self-organized coordination with effective convergence, divergence, and collision avoidance. Our method is fully interpretable and ready for deployment without gaps. Comprehensive simulations and real-world experiments demonstrate reduced path redundancy, robust performance across various tasks, and minimal demands on computation and communication.
<div id='section'>Paperid: <span id='pid'>373, <a href='https://arxiv.org/pdf/2409.10049.pdf' target='_blank'>https://arxiv.org/pdf/2409.10049.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenxi Li, Weining Lu, Qingquan Lin, Litong Meng, Haolu Li, Bin Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.10049">Nav-SCOPE: Swarm Robot Cooperative Perception and Coordinated Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper proposes a lightweight systematic solution for multi-robot coordinated navigation with decentralized cooperative perception. An information flow is first created to facilitate real-time observation sharing over unreliable ad-hoc networks. Then, the environmental uncertainties of each robot are reduced by interaction fields that deliver complementary information. Finally, path optimization is achieved, enabling self-organized coordination with effective convergence, divergence, and collision avoidance. Our method is fully interpretable and ready for deployment without gaps. Comprehensive simulations and real-world experiments demonstrate reduced path redundancy, robust performance across various tasks, and minimal demands on computation and communication.
<div id='section'>Paperid: <span id='pid'>374, <a href='https://arxiv.org/pdf/2409.09410.pdf' target='_blank'>https://arxiv.org/pdf/2409.09410.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoying Li, Qingcheng Zeng, Haoran Li, Yanglin Zhang, Junfeng Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.09410">Distributed Invariant Kalman Filter for Object-level Multi-robot Pose SLAM</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cooperative localization and target tracking are essential for multi-robot systems to implement high-level tasks. To this end, we propose a distributed invariant Kalman filter based on covariance intersection for effective multi-robot pose estimation. The paper utilizes the object-level measurement models, which have condensed information further reducing the communication burden. Besides, by modeling states on special Lie groups, the better linearity and consistency of the invariant Kalman filter structure can be stressed. We also use a combination of CI and KF to avoid overly confident or conservative estimates in multi-robot systems with intricate and unknown correlations, and some level of robot degradation is acceptable through multi-robot collaboration. The simulation and real data experiment validate the practicability and superiority of the proposed algorithm.
<div id='section'>Paperid: <span id='pid'>375, <a href='https://arxiv.org/pdf/2408.15428.pdf' target='_blank'>https://arxiv.org/pdf/2408.15428.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Deyuan Qu, Qi Chen, Yongqi Zhu, Yihao Zhu, Sergei S. Avedisov, Song Fu, Qing Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.15428">HEAD: A Bandwidth-Efficient Cooperative Perception Approach for Heterogeneous Connected and Autonomous Vehicles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In cooperative perception studies, there is often a trade-off between communication bandwidth and perception performance. While current feature fusion solutions are known for their excellent object detection performance, transmitting the entire sets of intermediate feature maps requires substantial bandwidth. Furthermore, these fusion approaches are typically limited to vehicles that use identical detection models. Our goal is to develop a solution that supports cooperative perception across vehicles equipped with different modalities of sensors. This method aims to deliver improved perception performance compared to late fusion techniques, while achieving precision similar to the state-of-art intermediate fusion, but requires an order of magnitude less bandwidth. We propose HEAD, a method that fuses features from the classification and regression heads in 3D object detection networks. Our method is compatible with heterogeneous detection networks such as LiDAR PointPillars, SECOND, VoxelNet, and camera Bird's-eye View (BEV) Encoder. Given the naturally smaller feature size in the detection heads, we design a self-attention mechanism to fuse the classification head and a complementary feature fusion layer to fuse the regression head. Our experiments, comprehensively evaluated on the V2V4Real and OPV2V datasets, demonstrate that HEAD is a fusion method that effectively balances communication bandwidth and perception performance.
<div id='section'>Paperid: <span id='pid'>376, <a href='https://arxiv.org/pdf/2407.02777.pdf' target='_blank'>https://arxiv.org/pdf/2407.02777.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lishuo Pan, Kevin Hsu, Nora Ayanian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.02777">Hierarchical Large Scale Multirobot Path (Re)Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We consider a large-scale multi-robot path planning problem in a cluttered environment. Our approach achieves real-time replanning by dividing the workspace into cells and utilizing a hierarchical planner. Specifically, we propose novel multi-commodity flow-based high-level planners that route robots through cells with reduced congestion, along with an anytime low-level planner that computes collision-free paths for robots within each cell in parallel. A highlight of our method is a significant improvement in computation time. Specifically, we show empirical results of a 500-times speedup in computation time compared to the baseline multi-agent pathfinding approach on the environments we study. We account for the robot's embodiment and support non-stop execution with continuous replanning. We demonstrate the real-time performance of our algorithm with up to 142 robots in simulation, and a representative 32 physical Crazyflie nano-quadrotor experiment.
<div id='section'>Paperid: <span id='pid'>377, <a href='https://arxiv.org/pdf/2406.07877.pdf' target='_blank'>https://arxiv.org/pdf/2406.07877.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qizhen Wu, Kexin Liu, Lei Chen, Jinhu LÃ¼
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.07877">Hierarchical Reinforcement Learning for Swarm Confrontation with High Uncertainty</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In swarm robotics, confrontation including the pursuit-evasion game is a key scenario. High uncertainty caused by unknown opponents' strategies, dynamic obstacles, and insufficient training complicates the action space into a hybrid decision process. Although the deep reinforcement learning method is significant for swarm confrontation since it can handle various sizes, as an end-to-end implementation, it cannot deal with the hybrid process. Here, we propose a novel hierarchical reinforcement learning approach consisting of a target allocation layer, a path planning layer, and the underlying dynamic interaction mechanism between the two layers, which indicates the quantified uncertainty. It decouples the hybrid process into discrete allocation and continuous planning layers, with a probabilistic ensemble model to quantify the uncertainty and regulate the interaction frequency adaptively. Furthermore, to overcome the unstable training process introduced by the two layers, we design an integration training method including pre-training and cross-training, which enhances the training efficiency and stability. Experiment results in both comparison, ablation, and real-robot studies validate the effectiveness and generalization performance of our proposed approach. In our defined experiments with twenty to forty agents, the win rate of the proposed method reaches around ninety percent, outperforming other traditional methods.
<div id='section'>Paperid: <span id='pid'>378, <a href='https://arxiv.org/pdf/2405.04000.pdf' target='_blank'>https://arxiv.org/pdf/2405.04000.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yizhi Zhou, Yufan Liu, Pengxiang Zhu, Xuan Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.04000">Distributed Invariant Kalman Filter for Cooperative Localization using Matrix Lie Groups</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper studies the problem of Cooperative Localization (CL) for multi-robot systems, where a group of mobile robots jointly localize themselves by using measurements from onboard sensors and shared information from other robots. We propose a novel distributed invariant Kalman Filter (DInEKF) based on the Lie group theory, to solve the CL problem in a 3-D environment. Unlike the standard EKF which computes the Jacobians based on the linearization at the state estimate, DInEKF defines the robots' motion model on matrix Lie groups and offers the advantage of state estimate-independent Jacobians. This significantly improves the consistency of the estimator. Moreover, the proposed algorithm is fully distributed, relying solely on each robot's ego-motion measurements and information received from its one-hop communication neighbors. The effectiveness of the proposed algorithm is validated in both Monte-Carlo simulations and real-world experiments. The results show that the proposed DInEKF outperforms the standard distributed EKF in terms of both accuracy and consistency.
<div id='section'>Paperid: <span id='pid'>379, <a href='https://arxiv.org/pdf/2404.01064.pdf' target='_blank'>https://arxiv.org/pdf/2404.01064.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yechi Ma, Yanan Li, Wei Hua, Shu Kong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.01064">Roadside Monocular 3D Detection Prompted by 2D Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Roadside monocular 3D detection requires detecting objects of predefined classes in an RGB frame and predicting their 3D attributes, such as bird's-eye-view (BEV) locations. It has broad applications in traffic control, vehicle-vehicle communication, and vehicle-infrastructure cooperative perception. To address this task, we introduce Promptable 3D Detector (Pro3D), a novel detector design that leverages 2D detections as prompts. We build our Pro3D upon two key insights. First, compared to a typical 3D detector, a 2D detector is ``easier'' to train due to fewer loss terms and performs significantly better at localizing objects w.r.t 2D metrics. Second, once 2D detections precisely locate objects in the image, a 3D detector can focus on lifting these detections into 3D BEV, especially when fixed camera pose or scene geometry provide an informative prior. To encode and incorporate 2D detections, we explore three methods: (a) concatenating features from both 2D and 3D detectors, (b) attentively fusing 2D and 3D detector features, and (c) encoding properties of predicted 2D bounding boxes \{$x$, $y$, width, height, label\} and attentively fusing them with the 3D detector feature. Interestingly, the third method significantly outperforms the others, underscoring the effectiveness of 2D detections as prompts that offer precise object targets and allow the 3D detector to focus on lifting them into 3D. Pro3D is adaptable for use with a wide range of 2D and 3D detectors with minimal modifications. Comprehensive experiments demonstrate that our Pro3D significantly enhances existing methods, achieving state-of-the-art results on two contemporary benchmarks.
<div id='section'>Paperid: <span id='pid'>380, <a href='https://arxiv.org/pdf/2403.17147.pdf' target='_blank'>https://arxiv.org/pdf/2403.17147.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Leo Cazenille, Nicolas Lobato-Dauzier, Alessia Loi, Mika Ito, Olivier Marchal, Nathanael Aubert-Kato, Nicolas Bredeche, Anthony J. Genot
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.17147">Hearing the shape of an arena with spectral swarm robotics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Swarm robotics promises adaptability to unknown situations and robustness against failures. However, it still struggles with global tasks that require understanding the broader context in which the robots operate, such as identifying the shape of the arena in which the robots are embedded. Biological swarms, such as shoals of fish, flocks of birds, and colonies of insects, routinely solve global geometrical problems through the diffusion of local cues. This paradigm can be explicitly described by mathematical models that could be directly computed and exploited by a robotic swarm. Diffusion over a domain is mathematically encapsulated by the Laplacian, a linear operator that measures the local curvature of a function. Crucially the geometry of a domain can generally be reconstructed from the eigenspectrum of its Laplacian. Here we introduce spectral swarm robotics where robots diffuse information to their neighbors to emulate the Laplacian operator - enabling them to "hear" the spectrum of their arena. We reveal a universal scaling that links the optimal number of robots (a global parameter) with their optimal radius of interaction (a local parameter). We validate experimentally spectral swarm robotics under challenging conditions with the one-shot classification of arena shapes using a sparse swarm of Kilobots. Spectral methods can assist with challenging tasks where robots need to build an emergent consensus on their environment, such as adaptation to unknown terrains, division of labor, or quorum sensing. Spectral methods may extend beyond robotics to analyze and coordinate swarms of agents of various natures, such as traffic or crowds, and to better understand the long-range dynamics of natural systems emerging from short-range interactions.
<div id='section'>Paperid: <span id='pid'>381, <a href='https://arxiv.org/pdf/2403.01710.pdf' target='_blank'>https://arxiv.org/pdf/2403.01710.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyi Wang, Jiwen Xu, Chuanxiang Gao, Yizhou Chen, Jihan Zhang, Chenggang Wang, Ben M. Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.01710">Sensor-based Multi-agent Coverage Control with Spatial Separation in Unstructured Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-robot systems have increasingly become instrumental in tackling search and coverage problems. However, the challenge of optimizing task efficiency without compromising task success still persists, particularly in expansive, unstructured environments with dense obstacles.
  This paper presents an innovative, decentralized Voronoi-based approach for search and coverage to reactively navigate these complexities while maintaining safety.
  This approach leverages the active sensing capabilities of multi-robot systems to supplement GIS (Geographic Information System), offering a more comprehensive and real-time understanding of the environment. Based on point cloud data, which is inherently non-convex and unstructured, this method efficiently generates collision-free Voronoi regions using only local sensing information through spatial decomposition and spherical mirroring techniques.
  Then, deadlock-aware guided map integrated with a gradient-optimized, centroid Voronoi-based coverage control policy, is constructed to improve efficiency by avoiding exhaustive searches and local sensing pitfalls.
  The effectiveness of our algorithm has been validated through extensive numerical simulations in high-fidelity environments, demonstrating significant improvements in both task success rate, coverage ratio, and task execution time compared with others.
<div id='section'>Paperid: <span id='pid'>382, <a href='https://arxiv.org/pdf/2402.01485.pdf' target='_blank'>https://arxiv.org/pdf/2402.01485.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mahboubeh Asadi, Kourosh Zareinia, Sajad Saeedi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.01485">Di-NeRF: Distributed NeRF for Collaborative Learning with Relative Pose Refinement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Collaborative mapping of unknown environments can be done faster and more robustly than a single robot. However, a collaborative approach requires a distributed paradigm to be scalable and deal with communication issues. This work presents a fully distributed algorithm enabling a group of robots to collectively optimize the parameters of a Neural Radiance Field (NeRF). The algorithm involves the communication of each robot's trained NeRF parameters over a mesh network, where each robot trains its NeRF and has access to its own visual data only. Additionally, the relative poses of all robots are jointly optimized alongside the model parameters, enabling mapping with less accurate relative camera poses. We show that multi-robot systems can benefit from differentiable and robust 3D reconstruction optimized from multiple NeRFs. Experiments on real-world and synthetic data demonstrate the efficiency of the proposed algorithm. See the website of the project for videos of the experiments and supplementary material (https://sites.google.com/view/di-nerf/home).
<div id='section'>Paperid: <span id='pid'>383, <a href='https://arxiv.org/pdf/2401.13127.pdf' target='_blank'>https://arxiv.org/pdf/2401.13127.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pierce Howell, Max Rudolph, Reza Torbati, Kevin Fu, Harish Ravichandar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.13127">Generalization of Heterogeneous Multi-Robot Policies via Awareness and Communication of Capabilities</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in multi-agent reinforcement learning (MARL) are enabling impressive coordination in heterogeneous multi-robot teams. However, existing approaches often overlook the challenge of generalizing learned policies to teams of new compositions, sizes, and robots. While such generalization might not be important in teams of virtual agents that can retrain policies on-demand, it is pivotal in multi-robot systems that are deployed in the real-world and must readily adapt to inevitable changes. As such, multi-robot policies must remain robust to team changes -- an ability we call adaptive teaming. In this work, we investigate if awareness and communication of robot capabilities can provide such generalization by conducting detailed experiments involving an established multi-robot test bed. We demonstrate that shared decentralized policies, that enable robots to be both aware of and communicate their capabilities, can achieve adaptive teaming by implicitly capturing the fundamental relationship between collective capabilities and effective coordination. Videos of trained policies can be viewed at: https://sites.google.com/view/cap-comm
<div id='section'>Paperid: <span id='pid'>384, <a href='https://arxiv.org/pdf/2312.10797.pdf' target='_blank'>https://arxiv.org/pdf/2312.10797.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingtao Tang, Hang Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.10797">Large-Scale Multi-Robot Coverage Path Planning via Local Search</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We study graph-based Multi-Robot Coverage Path Planning (MCPP) that aims to compute coverage paths for multiple robots to cover all vertices of a given 2D grid terrain graph $G$. Existing graph-based MCPP algorithms first compute a tree cover on $G$ -- a forest of multiple trees that cover all vertices -- and then employ the Spanning Tree Coverage (STC) paradigm to generate coverage paths on the decomposed graph $D$ of the terrain graph $G$ by circumnavigating the edges of the computed trees, aiming to optimize the makespan (i.e., the maximum coverage path cost among all robots). In this paper, we take a different approach by exploring how to systematically search for good coverage paths directly on $D$. We introduce a new algorithmic framework, called LS-MCPP, which leverages a local search to operate directly on $D$. We propose a novel standalone paradigm, Extended-STC (ESTC), that extends STC to achieve complete coverage for MCPP on any decomposed graphs, even those resulting from incomplete terrain graphs. Furthermore, we demonstrate how to integrate ESTC with three novel types of neighborhood operators into our framework to effectively guide its search process. Our extensive experiments demonstrate the effectiveness of LS-MCPP, consistently improving the initial solution returned by two state-of-the-art baseline algorithms that compute suboptimal tree covers on $G$, with a notable reduction in makespan by up to 35.7\% and 30.3\%, respectively. Moreover, LS-MCPP consistently matches or surpasses the results of optimal tree cover computation, achieving these outcomes with orders of magnitude faster runtime, thereby showcasing its significant benefits for large-scale real-world coverage tasks.
<div id='section'>Paperid: <span id='pid'>385, <a href='https://arxiv.org/pdf/2312.10342.pdf' target='_blank'>https://arxiv.org/pdf/2312.10342.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenguang Liu, Jianjun Chen, Yunfei Chen, Ryan Payton, Michael Riley, Shuang-Hua Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.10342">Self-supervised Adaptive Weighting for Cooperative Perception in V2V Communications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Perception of the driving environment is critical for collision avoidance and route planning to ensure driving safety. Cooperative perception has been widely studied as an effective approach to addressing the shortcomings of single-vehicle perception. However, the practical limitations of vehicle-to-vehicle (V2V) communications have not been adequately investigated. In particular, current cooperative fusion models rely on supervised models and do not address dynamic performance degradation caused by arbitrary channel impairments. In this paper, a self-supervised adaptive weighting model is proposed for intermediate fusion to mitigate the adverse effects of channel distortion. The performance of cooperative perception is investigated in different system settings. Rician fading and imperfect channel state information (CSI) are also considered. Numerical results demonstrate that the proposed adaptive weighting algorithm significantly outperforms the benchmarks without weighting. Visualization examples validate that the proposed weighting algorithm can flexibly adapt to various channel conditions. Moreover, the adaptive weighting algorithm demonstrates good generalization to untrained channels and test datasets from different domains.
<div id='section'>Paperid: <span id='pid'>386, <a href='https://arxiv.org/pdf/2312.04822.pdf' target='_blank'>https://arxiv.org/pdf/2312.04822.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Deyuan Qu, Qi Chen, Tianyu Bai, Hongsheng Lu, Heng Fan, Hao Zhang, Song Fu, Qing Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.04822">SiCP: Simultaneous Individual and Cooperative Perception for 3D Object Detection in Connected and Automated Vehicles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cooperative perception for connected and automated vehicles is traditionally achieved through the fusion of feature maps from two or more vehicles. However, the absence of feature maps shared from other vehicles can lead to a significant decline in 3D object detection performance for cooperative perception models compared to standalone 3D detection models. This drawback impedes the adoption of cooperative perception as vehicle resources are often insufficient to concurrently employ two perception models. To tackle this issue, we present Simultaneous Individual and Cooperative Perception (SiCP), a generic framework that supports a wide range of the state-of-the-art standalone perception backbones and enhances them with a novel Dual-Perception Network (DP-Net) designed to facilitate both individual and cooperative perception. In addition to its lightweight nature with only 0.13M parameters, DP-Net is robust and retains crucial gradient information during feature map fusion. As demonstrated in a comprehensive evaluation on the V2V4Real and OPV2V datasets, thanks to DP-Net, SiCP surpasses state-of-the-art cooperative perception solutions while preserving the performance of standalone perception solutions.
<div id='section'>Paperid: <span id='pid'>387, <a href='https://arxiv.org/pdf/2311.11144.pdf' target='_blank'>https://arxiv.org/pdf/2311.11144.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tyler M. Paine, Michael R. Benjamin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.11144">A Model for Multi-Agent Autonomy That Uses Opinion Dynamics and Multi-Objective Behavior Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper reports a new hierarchical architecture for modeling autonomous multi-robot systems (MRSs): a nonlinear dynamical opinion process is used to model high-level group choice, and multi-objective behavior optimization is used to model individual decisions. Using previously reported theoretical results, we show it is possible to design the behavior of the MRS by the selection of a relatively small set of parameters. The resulting behavior - both collective actions and individual actions - can be understood intuitively. The approach is entirely decentralized and the communication cost scales by the number of group options, not agents. We demonstrated the effectiveness of this approach using a hypothetical `explore-exploit-migrate' scenario in a two hour field demonstration with eight unmanned surface vessels (USVs). The results from our preliminary field experiment show the collective behavior is robust even with time-varying network topology and agent dropouts.
<div id='section'>Paperid: <span id='pid'>388, <a href='https://arxiv.org/pdf/2311.10336.pdf' target='_blank'>https://arxiv.org/pdf/2311.10336.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenguang Liu, Yunfei Chen, Jianjun Chen, Ryan Payton, Michael Riley, Shuang-Hua Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.10336">Cooperative Perception with Learning-Based V2V communications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cooperative perception has been widely used in autonomous driving to alleviate the inherent limitation of single automated vehicle perception. To enable cooperation, vehicle-to-vehicle (V2V) communication plays an indispensable role. This work analyzes the performance of cooperative perception accounting for communications channel impairments. Different fusion methods and channel impairments are evaluated. A new late fusion scheme is proposed to leverage the robustness of intermediate features. In order to compress the data size incurred by cooperation, a convolution neural network-based autoencoder is adopted. Numerical results demonstrate that intermediate fusion is more robust to channel impairments than early fusion and late fusion, when the SNR is greater than 0 dB. Also, the proposed fusion scheme outperforms the conventional late fusion using detection outputs, and autoencoder provides a good compromise between detection accuracy and bandwidth usage.
<div id='section'>Paperid: <span id='pid'>389, <a href='https://arxiv.org/pdf/2311.10194.pdf' target='_blank'>https://arxiv.org/pdf/2311.10194.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nazish Tahir, Ramviyas Parasuraman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.10194">Consensus-based Resource Scheduling for Collaborative Multi-Robot Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose integrating the edge-computing paradigm into the multi-robot collaborative scheduling to maximize resource utilization for complex collaborative tasks, which many robots must perform together. Examples include collaborative map-merging to produce a live global map during exploration instead of traditional approaches that schedule tasks on centralized cloud-based systems to facilitate computing. Our decentralized approach to a consensus-based scheduling strategy benefits a multi-robot-edge collaboration system by adapting to dynamic computation needs and communication-changing statistics as the system tries to optimize resources while maintaining overall performance objectives. Before collaborative task offloading, continuous device, and network profiling are performed at the computing resources, and the distributed scheduling scheme then selects the resource with maximum utility derived using a utility maximization approach. Thorough evaluations with and without edge servers on simulation and real-world multi-robot systems demonstrate that a lower task latency, a large throughput gain, and better frame rate processing may be achieved compared to the conventional edge-based systems.
<div id='section'>Paperid: <span id='pid'>390, <a href='https://arxiv.org/pdf/2309.05632.pdf' target='_blank'>https://arxiv.org/pdf/2309.05632.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mayank Sewlia, Christos K. Verginis, Dimos V. Dimarogonas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.05632">MAPS$^2$: Multi-Robot Autonomous Motion Planning under Signal Temporal Logic Specifications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This article presents MAPS$^2$ : a distributed algorithm that allows multi-robot systems to deliver coupled tasks expressed as Signal Temporal Logic (STL) constraints. Classical control theoretical tools addressing STL constraints either adopt a limited fragment of the STL formula or require approximations of min/max operators, whereas works maximising robustness through optimisation-based methods often suffer from local minima, relaxing any completeness arguments due to the NP-hard nature of the problem. Endowed with probabilistic guarantees, MAPS$^2$ provides an anytime algorithm that iteratively improves the robots' trajectories. The algorithm selectively imposes spatial constraints by taking advantage of the temporal properties of the STL. The algorithm is distributed, in the sense that each robot calculates its trajectory by communicating only with its immediate neighbours as defined via a communication graph. We illustrate the efficiency of MAPS$^2$ by conducting extensive simulation and experimental studies, verifying the generation of STL satisfying trajectories.
<div id='section'>Paperid: <span id='pid'>391, <a href='https://arxiv.org/pdf/2307.06615.pdf' target='_blank'>https://arxiv.org/pdf/2307.06615.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lantao Li, Chen Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.06615">NLOS Dies Twice: Challenges and Solutions of V2X for Cooperative Perception</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent multi-lidar sensor fusion between connected vehicles for cooperative perception has recently been recognized as the best technique for minimizing the blind zone of individual vehicular perception systems and further enhancing the overall safety of autonomous driving systems. This technique relies heavily on the reliability and availability of vehicle-to-everything (V2X) communication. In practical sensor fusion application scenarios, the non-line-of-sight (NLOS) issue causes blind zones for not only the perception system but also V2X direct communication. To counteract underlying communication issues, we introduce an abstract perception matrix matching method for quick sensor fusion matching procedures and mobility-height hybrid relay determination procedures, proactively improving the efficiency and performance of V2X communication to serve the upper layer application fusion requirements. To demonstrate the effectiveness of our solution, we design a new simulation framework to consider autonomous driving, sensor fusion and V2X communication in general, paving the way for end-to-end performance evaluation and further solution derivation.
<div id='section'>Paperid: <span id='pid'>392, <a href='https://arxiv.org/pdf/2306.17609.pdf' target='_blank'>https://arxiv.org/pdf/2306.17609.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingtao Tang, Hang Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.17609">Mixed Integer Programming for Time-Optimal Multi-Robot Coverage Path Planning with Efficient Heuristics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We investigate time-optimal Multi-Robot Coverage Path Planning (MCPP) for both unweighted and weighted terrains, which aims to minimize the coverage time, defined as the maximum travel time of all robots. Specifically, we focus on a reduction from MCPP to Min-Max Rooted Tree Cover (MMRTC). For the first time, we propose a Mixed Integer Programming (MIP) model to optimally solve MMRTC, resulting in an MCPP solution with a coverage time that is provably at most four times the optimal. Moreover, we propose two suboptimal yet effective heuristics that reduce the number of variables in the MIP model, thus improving its efficiency for large-scale MCPP instances. We show that both heuristics result in reduced-size MIP models that remain complete (i.e., guaranteed to find a solution if one exists) for all MMRTC instances. Additionally, we explore the use of model optimization warm-startup to further improve the efficiency of both the original MIP model and the reduced-size MIP models. We validate the effectiveness of our MIP-based MCPP planner through experiments that compare it with two state-of-the-art MCPP planners on various instances, demonstrating a reduction in the coverage time by an average of 27.65% and 23.24% over them, respectively.
<div id='section'>Paperid: <span id='pid'>393, <a href='https://arxiv.org/pdf/2211.05359.pdf' target='_blank'>https://arxiv.org/pdf/2211.05359.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Emon Dey, Mikolaj Walczak, Mohammad Saeid Anwar, Nirmalya Roy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.05359">A Reliable and Low Latency Synchronizing Middleware for Co-simulation of a Heterogeneous Multi-Robot Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Search and rescue, wildfire monitoring, and flood/hurricane impact assessment are mission-critical services for recent IoT networks. Communication synchronization, dependability, and minimal communication jitter are major simulation and system issues for the time-based physics-based ROS simulator, event-based network-based wireless simulator, and complex dynamics of mobile and heterogeneous IoT devices deployed in actual environments. Simulating a heterogeneous multi-robot system before deployment is difficult due to synchronizing physics (robotics) and network simulators. Due to its master-based architecture, most TCP/IP-based synchronization middlewares use ROS1. A real-time ROS2 architecture with masterless packet discovery synchronizes robotics and wireless network simulations. A velocity-aware Transmission Control Protocol (TCP) technique for ground and aerial robots using Data Distribution Service (DDS) publish-subscribe transport minimizes packet loss, synchronization, transmission, and communication jitters. Gazebo and NS-3 simulate and test. Simulator-agnostic middleware. LOS/NLOS and TCP/UDP protocols tested our ROS2-based synchronization middleware for packet loss probability and average latency. A thorough ablation research replaced NS-3 with EMANE, a real-time wireless network simulator, and masterless ROS2 with master-based ROS1. Finally, we tested network synchronization and jitter using one aerial drone (Duckiedrone) and two ground vehicles (TurtleBot3 Burger) on different terrains in masterless (ROS2) and master-enabled (ROS1) clusters. Our middleware shows that a large-scale IoT infrastructure with a diverse set of stationary and robotic devices can achieve low-latency communications (12% and 11% reduction in simulation and real) while meeting mission-critical application reliability (10% and 15% packet loss reduction) and high-fidelity requirements.
<div id='section'>Paperid: <span id='pid'>394, <a href='https://arxiv.org/pdf/2208.02993.pdf' target='_blank'>https://arxiv.org/pdf/2208.02993.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingtao Tang, Yuan Gao, Tin Lun Lam
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2208.02993">Learning to Coordinate for a Worker-Station Multi-robot System in Planar Coverage Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>For massive large-scale tasks, a multi-robot system (MRS) can effectively improve efficiency by utilizing each robot's different capabilities, mobility, and functionality. In this paper, we focus on the multi-robot coverage path planning (mCPP) problem in large-scale planar areas with random dynamic interferers in the environment, where the robots have limited resources. We introduce a worker-station MRS consisting of multiple workers with limited resources for actual work, and one station with enough resources for resource replenishment. We aim to solve the mCPP problem for the worker-station MRS by formulating it as a fully cooperative multi-agent reinforcement learning problem. Then we propose an end-to-end decentralized online planning method, which simultaneously solves coverage planning for workers and rendezvous planning for station. Our method manages to reduce the influence of random dynamic interferers on planning, while the robots can avoid collisions with them. We conduct simulation and real robot experiments, and the comparison results show that our method has competitive performance in solving the mCPP problem for worker-station MRS in metric of task finish time.
<div id='section'>Paperid: <span id='pid'>395, <a href='https://arxiv.org/pdf/2509.20095.pdf' target='_blank'>https://arxiv.org/pdf/2509.20095.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aymeric Vellinger, Nemanja Antonic, Elio Tuci
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20095">From Pheromones to Policies: Reinforcement Learning for Engineered Biological Swarms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Swarm intelligence emerges from decentralised interactions among simple agents, enabling collective problem-solving. This study establishes a theoretical equivalence between pheromone-mediated aggregation in \celeg\ and reinforcement learning (RL), demonstrating how stigmergic signals function as distributed reward mechanisms. We model engineered nematode swarms performing foraging tasks, showing that pheromone dynamics mathematically mirror cross-learning updates, a fundamental RL algorithm. Experimental validation with data from literature confirms that our model accurately replicates empirical \celeg\ foraging patterns under static conditions. In dynamic environments, persistent pheromone trails create positive feedback loops that hinder adaptation by locking swarms into obsolete choices. Through computational experiments in multi-armed bandit scenarios, we reveal that introducing a minority of exploratory agents insensitive to pheromones restores collective plasticity, enabling rapid task switching. This behavioural heterogeneity balances exploration-exploitation trade-offs, implementing swarm-level extinction of outdated strategies. Our results demonstrate that stigmergic systems inherently encode distributed RL processes, where environmental signals act as external memory for collective credit assignment. By bridging synthetic biology with swarm robotics, this work advances programmable living systems capable of resilient decision-making in volatile environments.
<div id='section'>Paperid: <span id='pid'>396, <a href='https://arxiv.org/pdf/2509.20095.pdf' target='_blank'>https://arxiv.org/pdf/2509.20095.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aymeric Vellinger, Nemanja Antonic, Elio Tuci
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20095">From Pheromones to Policies: Reinforcement Learning for Engineered Biological Swarms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Swarm intelligence emerges from decentralised interactions among simple agents, enabling collective problem-solving. This study establishes a theoretical equivalence between pheromone-mediated aggregation in \celeg\ and reinforcement learning (RL), demonstrating how stigmergic signals function as distributed reward mechanisms. We model engineered nematode swarms performing foraging tasks, showing that pheromone dynamics mathematically mirror cross-learning updates, a fundamental RL algorithm. Experimental validation with data from literature confirms that our model accurately replicates empirical \celeg\ foraging patterns under static conditions. In dynamic environments, persistent pheromone trails create positive feedback loops that hinder adaptation by locking swarms into obsolete choices. Through computational experiments in multi-armed bandit scenarios, we reveal that introducing a minority of exploratory agents insensitive to pheromones restores collective plasticity, enabling rapid task switching. This behavioural heterogeneity balances exploration-exploitation trade-offs, implementing swarm-level extinction of outdated strategies. Our results demonstrate that stigmergic systems inherently encode distributed RL processes, where environmental signals act as external memory for collective credit assignment. By bridging synthetic biology with swarm robotics, this work advances programmable living systems capable of resilient decision-making in volatile environments.
<div id='section'>Paperid: <span id='pid'>397, <a href='https://arxiv.org/pdf/2509.06882.pdf' target='_blank'>https://arxiv.org/pdf/2509.06882.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiheng Chen, Wei Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.06882">Dynamic Modeling and Efficient Data-Driven Optimal Control for Micro Autonomous Surface Vehicles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Micro Autonomous Surface Vehicles (MicroASVs) offer significant potential for operations in confined or shallow waters and swarm robotics applications. However, achieving precise and robust control at such small scales remains highly challenging, mainly due to the complexity of modeling nonlinear hydrodynamic forces and the increased sensitivity to self-motion effects and environmental disturbances, including waves and boundary effects in confined spaces. This paper presents a physics-driven dynamics model for an over-actuated MicroASV and introduces a data-driven optimal control framework that leverages a weak formulation-based online model learning method. Our approach continuously refines the physics-driven model in real time, enabling adaptive control that adjusts to changing system parameters. Simulation results demonstrate that the proposed method substantially enhances trajectory tracking accuracy and robustness, even under unknown payloads and external disturbances. These findings highlight the potential of data-driven online learning-based optimal control to improve MicroASV performance, paving the way for more reliable and precise autonomous surface vehicle operations.
<div id='section'>Paperid: <span id='pid'>398, <a href='https://arxiv.org/pdf/2509.06882.pdf' target='_blank'>https://arxiv.org/pdf/2509.06882.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiheng Chen, Wei Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.06882">Dynamic Modeling and Efficient Data-Driven Optimal Control for Micro Autonomous Surface Vehicles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Micro Autonomous Surface Vehicles (MicroASVs) offer significant potential for operations in confined or shallow waters and swarm robotics applications. However, achieving precise and robust control at such small scales remains highly challenging, mainly due to the complexity of modeling nonlinear hydrodynamic forces and the increased sensitivity to self-motion effects and environmental disturbances, including waves and boundary effects in confined spaces. This paper presents a physics-driven dynamics model for an over-actuated MicroASV and introduces a data-driven optimal control framework that leverages a weak formulation-based online model learning method. Our approach continuously refines the physics-driven model in real time, enabling adaptive control that adjusts to changing system parameters. Simulation results demonstrate that the proposed method substantially enhances trajectory tracking accuracy and robustness, even under unknown payloads and external disturbances. These findings highlight the potential of data-driven online learning-based optimal control to improve MicroASV performance, paving the way for more reliable and precise autonomous surface vehicle operations.
<div id='section'>Paperid: <span id='pid'>399, <a href='https://arxiv.org/pdf/2509.02031.pdf' target='_blank'>https://arxiv.org/pdf/2509.02031.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sijiang Li, Rongqing Zhang, Xiang Cheng, Jian Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02031">Synesthesia of Machines (SoM)-Based Task-Driven MIMO System for Image Transmission</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To support cooperative perception (CP) of networked mobile agents in dynamic scenarios, the efficient and robust transmission of sensory data is a critical challenge. Deep learning-based joint source-channel coding (JSCC) has demonstrated promising results for image transmission under adverse channel conditions, outperforming traditional rule-based codecs. While recent works have explored to combine JSCC with the widely adopted multiple-input multiple-output (MIMO) technology, these approaches are still limited to the discrete-time analog transmission (DTAT) model and simple tasks. Given the limited performance of existing MIMO JSCC schemes in supporting complex CP tasks for networked mobile agents with digital MIMO communication systems, this paper presents a Synesthesia of Machines (SoM)-based task-driven MIMO system for image transmission, referred to as SoM-MIMO. By leveraging the structural properties of the feature pyramid for perceptual tasks and the channel properties of the closed-loop MIMO communication system, SoM-MIMO enables efficient and robust digital MIMO transmission of images. Experimental results have shown that compared with two JSCC baseline schemes, our approach achieves average mAP improvements of 6.30 and 10.48 across all SNR levels, while maintaining identical communication overhead.
<div id='section'>Paperid: <span id='pid'>400, <a href='https://arxiv.org/pdf/2509.02031.pdf' target='_blank'>https://arxiv.org/pdf/2509.02031.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sijiang Li, Rongqing Zhang, Xiang Cheng, Jian Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02031">Synesthesia of Machines (SoM)-Based Task-Driven MIMO System for Image Transmission</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To support cooperative perception (CP) of networked mobile agents in dynamic scenarios, the efficient and robust transmission of sensory data is a critical challenge. Deep learning-based joint source-channel coding (JSCC) has demonstrated promising results for image transmission under adverse channel conditions, outperforming traditional rule-based codecs. While recent works have explored to combine JSCC with the widely adopted multiple-input multiple-output (MIMO) technology, these approaches are still limited to the discrete-time analog transmission (DTAT) model and simple tasks. Given the limited performance of existing MIMO JSCC schemes in supporting complex CP tasks for networked mobile agents with digital MIMO communication systems, this paper presents a Synesthesia of Machines (SoM)-based task-driven MIMO system for image transmission, referred to as SoM-MIMO. By leveraging the structural properties of the feature pyramid for perceptual tasks and the channel properties of the closed-loop MIMO communication system, SoM-MIMO enables efficient and robust digital MIMO transmission of images. Experimental results have shown that compared with two JSCC baseline schemes, our approach achieves average mAP improvements of 6.30 and 10.48 across all SNR levels, while maintaining identical communication overhead.
<div id='section'>Paperid: <span id='pid'>401, <a href='https://arxiv.org/pdf/2508.19731.pdf' target='_blank'>https://arxiv.org/pdf/2508.19731.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maryam Kazemi Eskeri, Ville Kyrki, Dominik Baumann, Tomasz Piotr Kucner
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19731">Efficient Human-Aware Task Allocation for Multi-Robot Systems in Shared Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-robot systems are increasingly deployed in applications, such as intralogistics or autonomous delivery, where multiple robots collaborate to complete tasks efficiently. One of the key factors enabling their efficient cooperation is Multi-Robot Task Allocation (MRTA). Algorithms solving this problem optimize task distribution among robots to minimize the overall execution time. In shared environments, apart from the relative distance between the robots and the tasks, the execution time is also significantly impacted by the delay caused by navigating around moving people. However, most existing MRTA approaches are dynamics-agnostic, relying on static maps and neglecting human motion patterns, leading to inefficiencies and delays. In this paper, we introduce \acrfull{method name}. This method leverages Maps of Dynamics (MoDs), spatio-temporal queryable models designed to capture historical human movement patterns, to estimate the impact of humans on the task execution time during deployment. \acrshort{method name} utilizes a stochastic cost function that includes MoDs. Experimental results show that integrating MoDs enhances task allocation performance, resulting in reduced mission completion times by up to $26\%$ compared to the dynamics-agnostic method and up to $19\%$ compared to the baseline. This work underscores the importance of considering human dynamics in MRTA within shared environments and presents an efficient framework for deploying multi-robot systems in environments populated by humans.
<div id='section'>Paperid: <span id='pid'>402, <a href='https://arxiv.org/pdf/2508.18153.pdf' target='_blank'>https://arxiv.org/pdf/2508.18153.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aalok Patwardhan, Andrew J. Davison
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.18153">DANCeRS: A Distributed Algorithm for Negotiating Consensus in Robot Swarms with Gaussian Belief Propagation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robot swarms require cohesive collective behaviour to address diverse challenges, including shape formation and decision-making. Existing approaches often treat consensus in discrete and continuous decision spaces as distinct problems. We present DANCeRS, a unified, distributed algorithm leveraging Gaussian Belief Propagation (GBP) to achieve consensus in both domains. By representing a swarm as a factor graph our method ensures scalability and robustness in dynamic environments, relying on purely peer-to-peer message passing. We demonstrate the effectiveness of our general framework through two applications where agents in a swarm must achieve consensus on global behaviour whilst relying on local communication. In the first, robots must perform path planning and collision avoidance to create shape formations. In the second, we show how the same framework can be used by a group of robots to form a consensus over a set of discrete decisions. Experimental results highlight our method's scalability and efficiency compared to recent approaches to these problems making it a promising solution for multi-robot systems requiring distributed consensus. We encourage the reader to see the supplementary video demo.
<div id='section'>Paperid: <span id='pid'>403, <a href='https://arxiv.org/pdf/2508.15537.pdf' target='_blank'>https://arxiv.org/pdf/2508.15537.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chang Liu, Yang Xu, Tamas Sziranyi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15537">D3FNet: A Differential Attention Fusion Network for Fine-Grained Road Structure Extraction in Remote Perception Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Extracting narrow roads from high-resolution remote sensing imagery remains a significant challenge due to their limited width, fragmented topology, and frequent occlusions. To address these issues, we propose D3FNet, a Dilated Dual-Stream Differential Attention Fusion Network designed for fine-grained road structure segmentation in remote perception systems. Built upon the encoder-decoder backbone of D-LinkNet, D3FNet introduces three key innovations:(1) a Differential Attention Dilation Extraction (DADE) module that enhances subtle road features while suppressing background noise at the bottleneck; (2) a Dual-stream Decoding Fusion Mechanism (DDFM) that integrates original and attention-modulated features to balance spatial precision with semantic context; and (3) a multi-scale dilation strategy (rates 1, 3, 5, 9) that mitigates gridding artifacts and improves continuity in narrow road prediction. Unlike conventional models that overfit to generic road widths, D3FNet specifically targets fine-grained, occluded, and low-contrast road segments. Extensive experiments on the DeepGlobe and CHN6-CUG benchmarks show that D3FNet achieves superior IoU and recall on challenging road regions, outperforming state-of-the-art baselines. Ablation studies further verify the complementary synergy of attention-guided encoding and dual-path decoding. These results confirm D3FNet as a robust solution for fine-grained narrow road extraction in complex remote and cooperative perception scenarios.
<div id='section'>Paperid: <span id='pid'>404, <a href='https://arxiv.org/pdf/2506.20031.pdf' target='_blank'>https://arxiv.org/pdf/2506.20031.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Prithvi Poddar, Ehsan Tarkesh Esfahani, Karthik Dantu, Souma Chowdhury
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.20031">Automated Generation of Diverse Courses of Actions for Multi-Agent Operations using Binary Optimization and Graph Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Operations in disaster response, search \& rescue, and military missions that involve multiple agents demand automated processes to support the planning of the courses of action (COA). Moreover, traverse-affecting changes in the environment (rain, snow, blockades, etc.) may impact the expected performance of a COA, making it desirable to have a pool of COAs that are diverse in task distributions across agents. Further, variations in agent capabilities, which could be human crews and/or autonomous systems, present practical opportunities and computational challenges to the planning process. This paper presents a new theoretical formulation and computational framework to generate such diverse pools of COAs for operations with soft variations in agent-task compatibility. Key to the problem formulation is a graph abstraction of the task space and the pool of COAs itself to quantify its diversity. Formulating the COAs as a centralized multi-robot task allocation problem, a genetic algorithm is used for (order-ignoring) allocations of tasks to each agent that jointly maximize diversity within the COA pool and overall compatibility of the agent-task mappings. A graph neural network is trained using a policy gradient approach to then perform single agent task sequencing in each COA, which maximizes completion rates adaptive to task features. Our tests of the COA generation process in a simulated environment demonstrate significant performance gain over a random walk baseline, small optimality gap in task sequencing, and execution time of about 50 minutes to plan up to 20 COAs for 5 agent/100 task operations.
<div id='section'>Paperid: <span id='pid'>405, <a href='https://arxiv.org/pdf/2506.18178.pdf' target='_blank'>https://arxiv.org/pdf/2506.18178.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Min Deng, Bo Fu, Lingyao Li, Xi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.18178">Integrating LLMs and Digital Twins for Adaptive Multi-Robot Task Allocation in Construction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-robot systems are emerging as a promising solution to the growing demand for productivity, safety, and adaptability across industrial sectors. However, effectively coordinating multiple robots in dynamic and uncertain environments, such as construction sites, remains a challenge, particularly due to unpredictable factors like material delays, unexpected site conditions, and weather-induced disruptions. To address these challenges, this study proposes an adaptive task allocation framework that strategically leverages the synergistic potential of Digital Twins, Integer Programming (IP), and Large Language Models (LLMs). The multi-robot task allocation problem is formally defined and solved using an IP model that accounts for task dependencies, robot heterogeneity, scheduling constraints, and re-planning requirements. A mechanism for narrative-driven schedule adaptation is introduced, in which unstructured natural language inputs are interpreted by an LLM, and optimization constraints are autonomously updated, enabling human-in-the-loop flexibility without manual coding. A digital twin-based system has been developed to enable real-time synchronization between physical operations and their digital representations. This closed-loop feedback framework ensures that the system remains dynamic and responsive to ongoing changes on site. A case study demonstrates both the computational efficiency of the optimization algorithm and the reasoning performance of several LLMs, with top-performing models achieving over 97% accuracy in constraint and parameter extraction. The results confirm the practicality, adaptability, and cross-domain applicability of the proposed methods.
<div id='section'>Paperid: <span id='pid'>406, <a href='https://arxiv.org/pdf/2506.07293.pdf' target='_blank'>https://arxiv.org/pdf/2506.07293.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Seabin Lee, Joonyeol Sim, Changjoo Nam
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.07293">Very Large-scale Multi-Robot Task Allocation in Challenging Environments via Robot Redistribution</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We consider the Multi-Robot Task Allocation (MRTA) problem that aims to optimize an assignment of multiple robots to multiple tasks in challenging environments which are with densely populated obstacles and narrow passages. In such environments, conventional methods optimizing the sum-of-cost are often ineffective because the conflicts between robots incur additional costs (e.g., collision avoidance, waiting). Also, an allocation that does not incorporate the actual robot paths could cause deadlocks, which significantly degrade the collective performance of the robots.
  We propose a scalable MRTA method that considers the paths of the robots to avoid collisions and deadlocks which result in a fast completion of all tasks (i.e., minimizing the \textit{makespan}). To incorporate robot paths into task allocation, the proposed method constructs a roadmap using a Generalized Voronoi Diagram. The method partitions the roadmap into several components to know how to redistribute robots to achieve all tasks with less conflicts between the robots. In the redistribution process, robots are transferred to their final destinations according to a push-pop mechanism with the first-in first-out principle. From the extensive experiments, we show that our method can handle instances with hundreds of robots in dense clutter while competitors are unable to compute a solution within a time limit.
<div id='section'>Paperid: <span id='pid'>407, <a href='https://arxiv.org/pdf/2506.06094.pdf' target='_blank'>https://arxiv.org/pdf/2506.06094.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Elim Kwan, Rehman Qureshi, Liam Fletcher, Colin Laganier, Victoria Nockles, Richard Walters
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.06094">On-board Mission Replanning for Adaptive Cooperative Multi-Robot Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cooperative autonomous robotic systems have significant potential for executing complex multi-task missions across space, air, ground, and maritime domains. But they commonly operate in remote, dynamic and hazardous environments, requiring rapid in-mission adaptation without reliance on fragile or slow communication links to centralised compute. Fast, on-board replanning algorithms are therefore needed to enhance resilience. Reinforcement Learning shows strong promise for efficiently solving mission planning tasks when formulated as Travelling Salesperson Problems (TSPs), but existing methods: 1) are unsuitable for replanning, where agents do not start at a single location; 2) do not allow cooperation between agents; 3) are unable to model tasks with variable durations; or 4) lack practical considerations for on-board deployment. Here we define the Cooperative Mission Replanning Problem as a novel variant of multiple TSP with adaptations to overcome these issues, and develop a new encoder/decoder-based model using Graph Attention Networks and Attention Models to solve it effectively and efficiently. Using a simple example of cooperative drones, we show our replanner consistently (90% of the time) maintains performance within 10% of the state-of-the-art LKH3 heuristic solver, whilst running 85-370 times faster on a Raspberry Pi. This work paves the way for increased resilience in autonomous multi-agent systems.
<div id='section'>Paperid: <span id='pid'>408, <a href='https://arxiv.org/pdf/2505.09511.pdf' target='_blank'>https://arxiv.org/pdf/2505.09511.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianfu Wu, Jiaqi Fu, Wugang Meng, Sungjin Cho, Huanzhe Zhan, Fumin Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.09511">Design of a Formation Control System to Assist Human Operators in Flying a Swarm of Robotic Blimps</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Formation control is essential for swarm robotics, enabling coordinated behavior in complex environments. In this paper, we introduce a novel formation control system for an indoor blimp swarm using a specialized leader-follower approach enhanced with a dynamic leader-switching mechanism. This strategy allows any blimp to take on the leader role, distributing maneuvering demands across the swarm and enhancing overall formation stability. Only the leader blimp is manually controlled by a human operator, while follower blimps use onboard monocular cameras and a laser altimeter for relative position and altitude estimation. A leader-switching scheme is proposed to assist the human operator to maintain stability of the swarm, especially when a sharp turn is performed. Experimental results confirm that the leader-switching mechanism effectively maintains stable formations and adapts to dynamic indoor environments while assisting human operator.
<div id='section'>Paperid: <span id='pid'>409, <a href='https://arxiv.org/pdf/2505.06980.pdf' target='_blank'>https://arxiv.org/pdf/2505.06980.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lei Wan, Prabesh Gupta, Andreas Eich, Marcel Kettelgerdes, Hannan Ejaz Keen, Michael KlÃ¶ppel-Gersdorf, Alexey Vinel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.06980">VALISENS: A Validated Innovative Multi-Sensor System for Cooperative Automated Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Perception is a core capability of automated vehicles and has been significantly advanced through modern sensor technologies and artificial intelligence. However, perception systems still face challenges in complex real-world scenarios. To improve robustness against various external factors, multi-sensor fusion techniques are essential, combining the strengths of different sensor modalities. With recent developments in Vehicle-to-Everything (V2X communication, sensor fusion can now extend beyond a single vehicle to a cooperative multi-agent system involving Connected Automated Vehicle (CAV) and intelligent infrastructure. This paper presents VALISENS, an innovative multi-sensor system distributed across multiple agents. It integrates onboard and roadside LiDARs, radars, thermal cameras, and RGB cameras to enhance situational awareness and support cooperative automated driving. The thermal camera adds critical redundancy for perceiving Vulnerable Road User (VRU), while fusion with roadside sensors mitigates visual occlusions and extends the perception range beyond the limits of individual vehicles. We introduce the corresponding perception module built on this sensor system, which includes object detection, tracking, motion forecasting, and high-level data fusion. The proposed system demonstrates the potential of cooperative perception in real-world test environments and lays the groundwork for future Cooperative Intelligent Transport Systems (C-ITS) applications.
<div id='section'>Paperid: <span id='pid'>410, <a href='https://arxiv.org/pdf/2505.01380.pdf' target='_blank'>https://arxiv.org/pdf/2505.01380.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pengda Mao, Shuli Lv, Chen Min, Zhaolong Shen, Quan Quan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.01380">An Efficient Real-Time Planning Method for Swarm Robotics Based on an Optimal Virtual Tube</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Swarm robotics navigating through unknown obstacle environments is an emerging research area that faces challenges. Performing tasks in such environments requires swarms to achieve autonomous localization, perception, decision-making, control, and planning. The limited computational resources of onboard platforms present significant challenges for planning and control. Reactive planners offer low computational demands and high re-planning frequencies but lack predictive capabilities, often resulting in local minima. Long-horizon planners, on the other hand, can perform multi-step predictions to reduce deadlocks but cost much computation, leading to lower re-planning frequencies. This paper proposes a real-time optimal virtual tube planning method for swarm robotics in unknown environments, which generates approximate solutions for optimal trajectories through affine functions. As a result, the computational complexity of approximate solutions is $O(n_t)$, where $n_t$ is the number of parameters in the trajectory, thereby significantly reducing the overall computational burden. By integrating reactive methods, the proposed method enables low-computation, safe swarm motion in unknown environments. The effectiveness of the proposed method is validated through several simulations and experiments.
<div id='section'>Paperid: <span id='pid'>411, <a href='https://arxiv.org/pdf/2505.00820.pdf' target='_blank'>https://arxiv.org/pdf/2505.00820.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhaoxing Li, Wenbo Wu, Yue Wang, Yanran Xu, William Hunt, Sebastian Stein
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.00820">HMCF: A Human-in-the-loop Multi-Robot Collaboration Framework Based on Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Rapid advancements in artificial intelligence (AI) have enabled robots to performcomplex tasks autonomously with increasing precision. However, multi-robot systems (MRSs) face challenges in generalization, heterogeneity, and safety, especially when scaling to large-scale deployments like disaster response. Traditional approaches often lack generalization, requiring extensive engineering for new tasks and scenarios, and struggle with managing diverse robots. To overcome these limitations, we propose a Human-in-the-loop Multi-Robot Collaboration Framework (HMCF) powered by large language models (LLMs). LLMs enhance adaptability by reasoning over diverse tasks and robot capabilities, while human oversight ensures safety and reliability, intervening only when necessary. Our framework seamlessly integrates human oversight, LLM agents, and heterogeneous robots to optimize task allocation and execution. Each robot is equipped with an LLM agent capable of understanding its capabilities, converting tasks into executable instructions, and reducing hallucinations through task verification and human supervision. Simulation results show that our framework outperforms state-of-the-art task planning methods, achieving higher task success rates with an improvement of 4.76%. Real-world tests demonstrate its robust zero-shot generalization feature and ability to handle diverse tasks and environments with minimal human intervention.
<div id='section'>Paperid: <span id='pid'>412, <a href='https://arxiv.org/pdf/2504.03260.pdf' target='_blank'>https://arxiv.org/pdf/2504.03260.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ze Zhang, Yifan Xue, Nadia Figueroa, Knut Ãkesson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.03260">Gradient Field-Based Dynamic Window Approach for Collision Avoidance in Complex Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>For safe and flexible navigation in multi-robot systems, this paper presents an enhanced and predictive sampling-based trajectory planning approach in complex environments, the Gradient Field-based Dynamic Window Approach (GF-DWA). Building upon the dynamic window approach, the proposed method utilizes gradient information of obstacle distances as a new cost term to anticipate potential collisions. This enhancement enables the robot to improve awareness of obstacles, including those with non-convex shapes. The gradient field is derived from the Gaussian process distance field, which generates both the distance field and gradient field by leveraging Gaussian process regression to model the spatial structure of the environment. Through several obstacle avoidance and fleet collision avoidance scenarios, the proposed GF-DWA is shown to outperform other popular trajectory planning and control methods in terms of safety and flexibility, especially in complex environments with non-convex obstacles.
<div id='section'>Paperid: <span id='pid'>413, <a href='https://arxiv.org/pdf/2503.14656.pdf' target='_blank'>https://arxiv.org/pdf/2503.14656.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Basit Muhammad Imran, Jeeseop Kim, Taizoon Chunawala, Alexander Leonessa, Kaveh Akbari Hamed
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.14656">Safety-Critical and Distributed Nonlinear Predictive Controllers for Teams of Quadrupedal Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a novel hierarchical, safety-critical control framework that integrates distributed nonlinear model predictive controllers (DNMPCs) with control barrier functions (CBFs) to enable cooperative locomotion of multi-agent quadrupedal robots in complex environments. While NMPC-based methods are widely adopted for enforcing safety constraints and navigating multi-robot systems (MRSs) through intricate environments, ensuring the safety of MRSs requires a formal definition grounded in the concept of invariant sets. CBFs, typically implemented via quadratic programs (QPs) at the planning layer, provide formal safety guarantees. However, their zero-control horizon limits their effectiveness for extended trajectory planning in inherently unstable, underactuated, and nonlinear legged robot models. Furthermore, the integration of CBFs into real-time NMPC for sophisticated MRSs, such as quadrupedal robot teams, remains underexplored. This paper develops computationally efficient, distributed NMPC algorithms that incorporate CBF-based collision safety guarantees within a consensus protocol, enabling longer planning horizons for safe cooperative locomotion under disturbances and rough terrain conditions. The optimal trajectories generated by the DNMPCs are tracked using full-order, nonlinear whole-body controllers at the low level. The proposed approach is validated through extensive numerical simulations with up to four Unitree A1 robots and hardware experiments involving two A1 robots subjected to external pushes, rough terrain, and uncertain obstacle information. Comparative analysis demonstrates that the proposed CBF-based DNMPCs achieve a 27.89% higher success rate than conventional NMPCs without CBF constraints.
<div id='section'>Paperid: <span id='pid'>414, <a href='https://arxiv.org/pdf/2503.05848.pdf' target='_blank'>https://arxiv.org/pdf/2503.05848.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wonjong Lee, Joonyeol Sim, Joonkyung Kim, Siwon Jo, Wenhao Luo, Changjoo Nam
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.05848">Merry-Go-Round: Safe Control of Decentralized Multi-Robot Systems with Deadlock Prevention</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a hybrid approach for decentralized multi-robot navigation that ensures both safety and deadlock prevention. Building on a standard control formulation, we add a lightweight deadlock prevention mechanism by forming temporary "roundabouts" (circular reference paths). Each robot relies only on local, peer-to-peer communication and a controller for base collision avoidance; a roundabout is generated or joined on demand to avert deadlocks. Robots in the roundabout travel in one direction until an escape condition is met, allowing them to return to goal-oriented motion. Unlike classical decentralized methods that lack explicit deadlock resolution, our roundabout maneuver ensures system-wide forward progress while preserving safety constraints. Extensive simulations and physical robot experiments show that our method consistently outperforms or matches the success and arrival rates of other decentralized control approaches, particularly in cluttered or high-density scenarios, all with minimal centralized coordination.
<div id='section'>Paperid: <span id='pid'>415, <a href='https://arxiv.org/pdf/2410.08408.pdf' target='_blank'>https://arxiv.org/pdf/2410.08408.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ethan Schneider, Daniel Wu, Devleena Das, Sonia Chernova
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.08408">CE-MRS: Contrastive Explanations for Multi-Robot Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As the complexity of multi-robot systems grows to incorporate a greater number of robots, more complex tasks, and longer time horizons, the solutions to such problems often become too complex to be fully intelligible to human users. In this work, we introduce an approach for generating natural language explanations that justify the validity of the system's solution to the user, or else aid the user in correcting any errors that led to a suboptimal system solution. Toward this goal, we first contribute a generalizable formalism of contrastive explanations for multi-robot systems, and then introduce a holistic approach to generating contrastive explanations for multi-robot scenarios that selectively incorporates data from multi-robot task allocation, scheduling, and motion-planning to explain system behavior. Through user studies with human operators we demonstrate that our integrated contrastive explanation approach leads to significant improvements in user ability to identify and solve system errors, leading to significant improvements in overall multi-robot team performance.
<div id='section'>Paperid: <span id='pid'>416, <a href='https://arxiv.org/pdf/2410.04547.pdf' target='_blank'>https://arxiv.org/pdf/2410.04547.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rayan Bahrami, Hamidreza Jafarnejadsani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.04547">Distributed Detection of Adversarial Attacks for Resilient Cooperation of Multi-Robot Systems with Intermittent Communication</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper concerns the consensus and formation of a network of mobile autonomous agents in adversarial settings where a group of malicious (compromised) agents are subject to deception attacks. In addition, the communication network is arbitrarily time-varying and subject to intermittent connections, possibly imposed by denial-of-service (DoS) attacks. We provide explicit bounds for network connectivity in an integral sense, enabling the characterization of the system's resilience to specific classes of adversarial attacks. We also show that under the condition of connectivity in an integral sense uniformly in time, the system is finite-gain $\mathcal{L}_{p}$ stable and uniformly exponentially fast consensus and formation are achievable, provided malicious agents are detected and isolated from the network. We present a distributed and reconfigurable framework with theoretical guarantees for detecting malicious agents, allowing for the resilient cooperation of the remaining cooperative agents. Simulation studies are provided to illustrate the theoretical findings.
<div id='section'>Paperid: <span id='pid'>417, <a href='https://arxiv.org/pdf/2409.10749.pdf' target='_blank'>https://arxiv.org/pdf/2409.10749.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lei Shi, Qichao Liu, Cheng Zhou, Xiong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.10749">A Fairness-Oriented Control Framework for Safety-Critical Multi-Robot Systems: Alternative Authority Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper proposes a fair control framework for multi-robot systems, which integrates the newly introduced Alternative Authority Control (AAC) and Flexible Control Barrier Function (F-CBF). Control authority refers to a single robot which can plan its trajectory while considering others as moving obstacles, meaning the other robots do not have authority to plan their own paths. The AAC method dynamically distributes the control authority, enabling fair and coordinated movement across the system. This approach significantly improves computational efficiency, scalability, and robustness in complex environments. The proposed F-CBF extends traditional CBFs by incorporating obstacle shape, velocity, and orientation. F-CBF enhances safety by accurate dynamic obstacle avoidance. The framework is validated through simulations in multi-robot scenarios, demonstrating its safety, robustness and computational efficiency.
<div id='section'>Paperid: <span id='pid'>418, <a href='https://arxiv.org/pdf/2409.06952.pdf' target='_blank'>https://arxiv.org/pdf/2409.06952.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Han Liu, Yu Jin, Tianjiang Hu, Kai Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.06952">Flow-Inspired Multi-Robot Real-Time Scheduling Planner</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Collision avoidance and trajectory planning are crucial in multi-robot systems, particularly in environments with numerous obstacles. Although extensive research has been conducted in this field, the challenge of rapid traversal through such environments has not been fully addressed. This paper addresses this problem by proposing a novel real-time scheduling scheme designed to optimize the passage of multi-robot systems through complex, obstacle-rich maps. Inspired from network flow optimization, our scheme decomposes the environment into a network structure, enabling the efficient allocation of robots to paths based on real-time congestion data. The proposed scheduling planner operates on top of existing collision avoidance algorithms, focusing on minimizing traversal time by balancing robot detours and waiting times. Our simulation results demonstrate the efficiency of the proposed scheme. Additionally, we validated its effectiveness through real world flight tests using ten quadrotors. This work contributes a lightweight, effective scheduling planner capable of meeting the real-time demands of multi-robot systems in obstacle-rich environments.
<div id='section'>Paperid: <span id='pid'>419, <a href='https://arxiv.org/pdf/2408.13759.pdf' target='_blank'>https://arxiv.org/pdf/2408.13759.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qi Liu, Jingxiang Guo, Sixu Lin, Shuaikang Ma, Jinxuan Zhu, Yanjie Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.13759">MASQ: Multi-Agent Reinforcement Learning for Single Quadruped Robot Locomotion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper proposes a novel method to improve locomotion learning for a single quadruped robot using multi-agent deep reinforcement learning (MARL). Many existing methods use single-agent reinforcement learning for an individual robot or MARL for the cooperative task in multi-robot systems. Unlike existing methods, this paper proposes using MARL for the locomotion learning of a single quadruped robot. We develop a learning structure called Multi-Agent Reinforcement Learning for Single Quadruped Robot Locomotion (MASQ), considering each leg as an agent to explore the action space of the quadruped robot, sharing a global critic, and learning collaboratively. Experimental results indicate that MASQ not only speeds up learning convergence but also enhances robustness in real-world settings, suggesting that applying MASQ to single robots such as quadrupeds could surpass traditional single-robot reinforcement learning approaches. Our study provides insightful guidance on integrating MARL with single-robot locomotion learning.
<div id='section'>Paperid: <span id='pid'>420, <a href='https://arxiv.org/pdf/2408.05111.pdf' target='_blank'>https://arxiv.org/pdf/2408.05111.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jeppe Heini Mikkelsen, Roberto Galeazzi, Matteo Fumagalli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.05111">Optimal Distributed Multi-Robot Communication-Aware Trajectory Planning using Alternating Direction Method of Multipliers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a distributed, optimal, communication-aware trajectory planning algorithm for multi-robot systems. Building on prior work, it addresses the multi-robot communication-aware trajectory planning problem using a general optimisation framework that imposes linear constraints on changes in robot positions to ensure communication performance and collision avoidance. In this paper, the optimisation problem is solved distributively by separating the communication performance constraint through an economic approach. Here, the current communication budget is distributed equally among the robots, and the robots are allowed to trade parts of their budgets with each other. The separated optimisation problem is then solved using the consensus alternating direction method of multipliers. The method was verified through simulation in an inspection task problem.
<div id='section'>Paperid: <span id='pid'>421, <a href='https://arxiv.org/pdf/2405.02579.pdf' target='_blank'>https://arxiv.org/pdf/2405.02579.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tanja Katharina Kaiser, Heiko Hamann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.02579">Innate Motivation for Robot Swarms by Minimizing Surprise: From Simple Simulations to Real-World Experiments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Applications of large-scale mobile multi-robot systems can be beneficial over monolithic robots because of higher potential for robustness and scalability. Developing controllers for multi-robot systems is challenging because the multitude of interactions is hard to anticipate and difficult to model. Automatic design using machine learning or evolutionary robotics seem to be options to avoid that challenge, but bring the challenge of designing reward or fitness functions. Generic reward and fitness functions seem unlikely to exist and task-specific rewards often have undesired side effects. Approaches of so-called innate motivation try to avoid the specific formulation of rewards and work instead with different drivers, such as curiosity. Our approach to innate motivation is to minimize surprise, which we implement by maximizing the accuracy of the swarm robot's sensor predictions using neuroevolution. A unique advantage of the swarm robot case is that swarm members populate the robot's environment and can trigger more active behaviors in a self-referential loop. We summarize our previous simulation-based results concerning behavioral diversity, robustness, scalability, and engineered self-organization, and put them into context. In several new studies, we analyze the influence of the optimizer's hyperparameters, the scalability of evolved behaviors, and the impact of realistic robot simulations. Finally, we present results using real robots that show how the reality gap can be bridged.
<div id='section'>Paperid: <span id='pid'>422, <a href='https://arxiv.org/pdf/2405.02438.pdf' target='_blank'>https://arxiv.org/pdf/2405.02438.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tanja Katharina Kaiser, Marian Johannes Begemann, Tavia Plattenteich, Lars Schilling, Georg Schildbach, Heiko Hamann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.02438">ROS2swarm - A ROS 2 Package for Swarm Robot Behaviors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Developing reusable software for mobile robots is still challenging. Even more so for swarm robots, despite the desired simplicity of the robot controllers. Prototyping and experimenting are difficult due to the multi-robot setting and often require robot-robot communication. Also, the diversity of swarm robot hardware platforms increases the need for hardware-independent software concepts. The main advantages of the commonly used robot software architecture ROS 2 are modularity and platform independence. We propose a new ROS 2 package, ROS2swarm, for applications of swarm robotics that provides a library of ready-to-use swarm behavioral primitives. We show the successful application of our approach on three different platforms, the TurtleBot3 Burger, the TurtleBot3 Waffle Pi, and the Jackal UGV, and with a set of different behavioral primitives, such as aggregation, dispersion, and collective decision-making. The proposed approach is easy to maintain, extendable, and has good potential for simplifying swarm robotics experiments in future applications.
<div id='section'>Paperid: <span id='pid'>423, <a href='https://arxiv.org/pdf/2404.18331.pdf' target='_blank'>https://arxiv.org/pdf/2404.18331.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanwen Cao, Sriram Shreedharan, Nikolay Atanasov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.18331">Multi-Robot Object SLAM Using Distributed Variational Inference</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-robot simultaneous localization and mapping (SLAM) enables a robot team to achieve coordinated tasks by relying on a common map of the environment. Constructing a map by centralized processing of the robot observations is undesirable because it creates a single point of failure and requires pre-existing infrastructure and significant communication throughput. This paper formulates multi-robot object SLAM as a variational inference problem over a communication graph subject to consensus constraints on the object estimates maintained by different robots. To solve the problem, we develop a distributed mirror descent algorithm with regularization enforcing consensus among the communicating robots. Using Gaussian distributions in the algorithm, we also derive a distributed multi-state constraint Kalman filter (MSCKF) for multi-robot object SLAM. Experiments on real and simulated data show that our method improves the trajectory and object estimates, compared to individual-robot SLAM, while achieving better scaling to large robot teams, compared to centralized multi-robot SLAM.
<div id='section'>Paperid: <span id='pid'>424, <a href='https://arxiv.org/pdf/2404.09200.pdf' target='_blank'>https://arxiv.org/pdf/2404.09200.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pengda Mao, Shuli Lv, Quan Quan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.09200">Tube RRT*: Efficient Homotopic Path Planning for Swarm Robotics Passing-Through Large-Scale Obstacle Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, the concept of homotopic trajectory planning has emerged as a novel solution to navigation in large-scale obstacle environments for swarm robotics, offering a wide ranging of applications. However, it lacks an efficient homotopic path planning method in large-scale obstacle environments. This paper introduces Tube RRT*, an innovative homotopic path planning method that builds upon and improves the Rapidly-exploring Random Tree (RRT) algorithm. Tube RRT* is specifically designed to generate homotopic paths, strategically considering gap volume and path length to mitigate swarm congestion and ensure agile navigation. Through comprehensive simulations and experiments, the effectiveness of Tube RRT* is validated.
<div id='section'>Paperid: <span id='pid'>425, <a href='https://arxiv.org/pdf/2404.01752.pdf' target='_blank'>https://arxiv.org/pdf/2404.01752.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Joonyeol Sim, Joonkyung Kim, Changjoo Nam
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.01752">Safe Interval RRT* for Scalable Multi-Robot Path Planning in Continuous Space</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we consider the problem of Multi-Robot Path Planning (MRPP) in continuous space. The difficulty of the problem arises from the extremely large search space caused by the combinatorial nature of the problem and the continuous state space. We propose a two-level approach where the low level is a sampling-based planner Safe Interval RRT* (SI-RRT*) that finds a collision-free trajectory for individual robots. The high level can use any method that can resolve inter-robot conflicts where we employ two representative methods that are Prioritized Planning (SI-CPP) and Conflict Based Search (SI-CCBS). Experimental results show that SI-RRT* can quickly find a high-quality solution with a few samples. SI-CPP exhibits improved scalability while SI-CCBS produces higher-quality solutions compared to the state-of-the-art planners for continuous space.
<div id='section'>Paperid: <span id='pid'>426, <a href='https://arxiv.org/pdf/2403.17916.pdf' target='_blank'>https://arxiv.org/pdf/2403.17916.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zehao Wang, Yuping Wang, Zhuoyuan Wu, Hengbo Ma, Zhaowei Li, Hang Qiu, Jiachen Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.17916">CMP: Cooperative Motion Prediction with Multi-Agent Communication</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The confluence of the advancement of Autonomous Vehicles (AVs) and the maturity of Vehicle-to-Everything (V2X) communication has enabled the capability of cooperative connected and automated vehicles (CAVs). Building on top of cooperative perception, this paper explores the feasibility and effectiveness of cooperative motion prediction. Our method, CMP, takes LiDAR signals as model input to enhance tracking and prediction capabilities. Unlike previous work that focuses separately on either cooperative perception or motion prediction, our framework, to the best of our knowledge, is the first to address the unified problem where CAVs share information in both perception and prediction modules. Incorporated into our design is the unique capability to tolerate realistic V2X transmission delays, while dealing with bulky perception representations. We also propose a prediction aggregation module, which unifies the predictions obtained by different CAVs and generates the final prediction. Through extensive experiments and ablation studies on the OPV2V and V2V4Real datasets, we demonstrate the effectiveness of our method in cooperative perception, tracking, and motion prediction. In particular, CMP reduces the average prediction error by 12.3% compared with the strongest baseline. Our work marks a significant step forward in the cooperative capabilities of CAVs, showcasing enhanced performance in complex scenarios. More details can be found on the project website: https://cmp-cooperative-prediction.github.io.
<div id='section'>Paperid: <span id='pid'>427, <a href='https://arxiv.org/pdf/2403.13266.pdf' target='_blank'>https://arxiv.org/pdf/2403.13266.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziqi Yang, Roberto Tron
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.13266">Enhancing Security in Multi-Robot Systems through Co-Observation Planning, Reachability Analysis, and Network Flow</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper addresses security challenges in multi-robot systems (MRS) where adversaries may compromise robot control, risking unauthorized access to forbidden areas. We propose a novel multi-robot optimal planning algorithm that integrates mutual observations and introduces reachability constraints for enhanced security. This ensures that, even with adversarial movements, compromised robots cannot breach forbidden regions without missing scheduled co-observations. The reachability constraint uses ellipsoidal over-approximation for efficient intersection checking and gradient computation. To enhance system resilience and tackle feasibility challenges, we also introduce sub-teams. These cohesive units replace individual robot assignments along each route, enabling redundant robots to deviate for co-observations across different trajectories, securing multiple sub-teams without requiring modifications. We formulate the cross-trajectory co-observation plan by solving a network flow coverage problem on the checkpoint graph generated from the original unsecured MRS trajectories, providing the same security guarantees against plan-deviation attacks. We demonstrate the effectiveness and robustness of our proposed algorithm, which significantly strengthens the security of multi-robot systems in the face of adversarial threats.
<div id='section'>Paperid: <span id='pid'>428, <a href='https://arxiv.org/pdf/2403.13266.pdf' target='_blank'>https://arxiv.org/pdf/2403.13266.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziqi Yang, Roberto Tron
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.13266">Enhancing Security in Multi-Robot Systems through Co-Observation Planning, Reachability Analysis, and Network Flow</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper addresses security challenges in multi-robot systems (MRS) where adversaries may compromise robot control, risking unauthorized access to forbidden areas. We propose a novel multi-robot optimal planning algorithm that integrates mutual observations and introduces reachability constraints for enhanced security. This ensures that, even with adversarial movements, compromised robots cannot breach forbidden regions without missing scheduled co-observations. The reachability constraint uses ellipsoidal over-approximation for efficient intersection checking and gradient computation. To enhance system resilience and tackle feasibility challenges, we also introduce sub-teams. These cohesive units replace individual robot assignments along each route, enabling redundant robots to deviate for co-observations across different trajectories, securing multiple sub-teams without requiring modifications. We formulate the cross-trajectory co-observation plan by solving a network flow coverage problem on the checkpoint graph generated from the original unsecured MRS trajectories, providing the same security guarantees against plan-deviation attacks. We demonstrate the effectiveness and robustness of our proposed algorithm, which significantly strengthens the security of multi-robot systems in the face of adversarial threats.
<div id='section'>Paperid: <span id='pid'>429, <a href='https://arxiv.org/pdf/2401.16599.pdf' target='_blank'>https://arxiv.org/pdf/2401.16599.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Joseph Prince Mathew, Cameron Nowzari
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.16599">ReLoki: Infrastructure-free Distributed Relative Localization using On-board UWB Antenna Arrays</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Coordination of multi-robot systems require some form of localization between agents, but most methods today rely on some external infrastructure. Ultra Wide Band (UWB) sensing has gained popularity in relative localization applications, and we see many implementations that use cooperative agents augmenting UWB range measurements with other sensing modalities (e.g., ViO, IMU, VSLAM) for infrastructure-free relative localization. A lesser researched option is using Angle of Arrival (AoA) readings obtained from UWB Antenna pairs to perform relative localization. In this paper we present a UWB platform called ReLoki that can be used for ranging and AoA-based relative localization in~3D. ReLoki enables any message sent from a transmitting agent to be localized by using a Regular Tetrahedral Antenna Array (RTA). As a full scale proof of concept, we deploy ReLoki on a 3-robot system and compare its performance in terms of accuracy and speed with prior methods.
<div id='section'>Paperid: <span id='pid'>430, <a href='https://arxiv.org/pdf/2312.00315.pdf' target='_blank'>https://arxiv.org/pdf/2312.00315.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhuo-Rui Pan, Wei Ren, Xi-Ming Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.00315">Multiple Control Functionals for Interconnected Time-Delay Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Safety is essential for autonomous systems, in particular for interconnected systems in which the interactions among subsystems are involved. Motivated by the recent interest in cyber-physical and interconnected autonomous systems, we address the safe stabilization problem of interconnected systems with time delays. We propose multiple control Lyapunov and barrier functionals for the stabilization and safety control problems, respectively. In order to investigate the safe stabilization control problem, the proposed multiple control functionals are combined together via two methods: the optimization-based method and the sliding mode based method. The resulting controllers can be of either explicit or implicit forms, both of which ensure the safe stabilization objective of the whole system. The derived results are illustrated via a reach-avoid problem of multi-robot systems.
<div id='section'>Paperid: <span id='pid'>431, <a href='https://arxiv.org/pdf/2311.16916.pdf' target='_blank'>https://arxiv.org/pdf/2311.16916.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jana Pavlasek, Joshua Jing Zhi Mah, Ruihan Xu, Odest Chadwicke Jenkins, Fabio Ramos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.16916">Stein Variational Belief Propagation for Multi-Robot Coordination</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Decentralized coordination for multi-robot systems involves planning in challenging, high-dimensional spaces. The planning problem is particularly challenging in the presence of obstacles and different sources of uncertainty such as inaccurate dynamic models and sensor noise. In this paper, we introduce Stein Variational Belief Propagation (SVBP), a novel algorithm for performing inference over nonparametric marginal distributions of nodes in a graph. We apply SVBP to multi-robot coordination by modelling a robot swarm as a graphical model and performing inference for each robot. We demonstrate our algorithm on a simulated multi-robot perception task, and on a multi-robot planning task within a Model-Predictive Control (MPC) framework, on both simulated and real-world mobile robots. Our experiments show that SVBP represents multi-modal distributions better than sampling-based or Gaussian baselines, resulting in improved performance on perception and planning tasks. Furthermore, we show that SVBP's ability to represent diverse trajectories for decentralized multi-robot planning makes it less prone to deadlock scenarios than leading baselines.
<div id='section'>Paperid: <span id='pid'>432, <a href='https://arxiv.org/pdf/2311.02994.pdf' target='_blank'>https://arxiv.org/pdf/2311.02994.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tanja Katharina Kaiser, Tristan Potten, Heiko Hamann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.02994">Evolution of Collective Decision-Making Mechanisms for Collective Perception</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous robot swarms must be able to make fast and accurate collective decisions, but speed and accuracy are known to be conflicting goals. While collective decision-making is widely studied in swarm robotics research, only few works on using methods of evolutionary computation to generate collective decision-making mechanisms exist. These works use task-specific fitness functions rewarding the accomplishment of the respective collective decision-making task. But task-independent rewards, such as for prediction error minimization, may promote the emergence of diverse and innovative solutions. We evolve collective decision-making mechanisms using a task-specific fitness function rewarding correct robot opinions, a task-independent reward for prediction accuracy, and a hybrid fitness function combining the two previous. In our simulations, we use the collective perception scenario, that is, robots must collectively determine which of two environmental features is more frequent. We show that evolution successfully optimizes fitness in all three scenarios, but that only the task-specific fitness function and the hybrid fitness function lead to the emergence of collective decision-making behaviors. In benchmark experiments, we show the competitiveness of the evolved decision-making mechanisms to the voter model and the majority rule and analyze the scalability of the decision-making mechanisms with problem difficulty.
<div id='section'>Paperid: <span id='pid'>433, <a href='https://arxiv.org/pdf/2309.01269.pdf' target='_blank'>https://arxiv.org/pdf/2309.01269.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohammad Naiseh, Mohammad D. Soorati, Sarvapali Ramchurn
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.01269">Outlining the design space of eXplainable swarm (xSwarm): experts perspective</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In swarm robotics, agents interact through local roles to solve complex tasks beyond an individual's ability. Even though swarms are capable of carrying out some operations without the need for human intervention, many safety-critical applications still call for human operators to control and monitor the swarm. There are novel challenges to effective Human-Swarm Interaction (HSI) that are only beginning to be addressed. Explainability is one factor that can facilitate effective and trustworthy HSI and improve the overall performance of Human-Swarm team. Explainability was studied across various Human-AI domains, such as Human-Robot Interaction and Human-Centered ML. However, it is still ambiguous whether explanations studied in Human-AI literature would be beneficial in Human-Swarm research and development. Furthermore, the literature lacks foundational research on the prerequisites for explainability requirements in swarm robotics, i.e., what kind of questions an explainable swarm is expected to answer, and what types of explanations a swarm is expected to generate. By surveying 26 swarm experts, we seek to answer these questions and identify challenges experts faced to generate explanations in Human-Swarm environments. Our work contributes insights into defining a new area of research of eXplainable Swarm (xSwarm) which looks at how explainability can be implemented and developed in swarm systems. This paper opens the discussion on xSwarm and paves the way for more research in the field.
<div id='section'>Paperid: <span id='pid'>434, <a href='https://arxiv.org/pdf/2307.07326.pdf' target='_blank'>https://arxiv.org/pdf/2307.07326.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jeppe Heini Mikkelsen, Matteo Fumagalli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.07326">Distributed Planning for Rigid Robot Formations using Consensus on the Transformation of a Base Configuration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a novel planning method that achieves navigation of multi-robot formations in cluttered environments, while maintaining the formation throughout the robots motion. The method utilises a decentralised approach to find feasible formation parameters that guarantees formation constraints for rigid formations. The method proves to be computationally efficient, making it relevant for reactive planning and control of multi-robot systems formation. The method has been tested in a simulation environment to prove feasibility and run-time efficiency.
<div id='section'>Paperid: <span id='pid'>435, <a href='https://arxiv.org/pdf/2305.01869.pdf' target='_blank'>https://arxiv.org/pdf/2305.01869.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rhett Hull, Ki Myung Brian Lee, Jennifer Wakulicz, Chanyeol Yoo, James McMahon, Bryan Clarke, Stuart Anstee, Jijoong Kim, Robert Fitch
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.01869">Decentralised Active Perception in Continuous Action Spaces for the Coordinated Escort Problem</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We consider the coordinated escort problem, where a decentralised team of supporting robots implicitly assist the mission of higher-value principal robots. The defining challenge is how to evaluate the effect of supporting robots' actions on the principal robots' mission. To capture this effect, we define two novel auxiliary reward functions for supporting robots called satisfaction improvement and satisfaction entropy, which computes the improvement in probability of mission success, or the uncertainty thereof. Given these reward functions, we coordinate the entire team of principal and supporting robots using decentralised cross entropy method (Dec-CEM), a new extension of CEM to multi-agent systems based on the product distribution approximation. In a simulated object avoidance scenario, our planning framework demonstrates up to two-fold improvement in task satisfaction against conventional decoupled information gathering.The significance of our results is to introduce a new family of algorithmic problems that will enable important new practical applications of heterogeneous multi-robot systems.
<div id='section'>Paperid: <span id='pid'>436, <a href='https://arxiv.org/pdf/2012.11444.pdf' target='_blank'>https://arxiv.org/pdf/2012.11444.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>David M. Bossens, Danesh Tarapore
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2012.11444">Rapidly adapting robot swarms with Swarm Map-based Bayesian Optimisation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Rapid performance recovery from unforeseen environmental perturbations remains a grand challenge in swarm robotics. To solve this challenge, we investigate a behaviour adaptation approach, where one searches an archive of controllers for potential recovery solutions. To apply behaviour adaptation in swarm robotic systems, we propose two algorithms: (i) Swarm Map-based Optimisation (SMBO), which selects and evaluates one controller at a time, for a homogeneous swarm, in a centralised fashion; and (ii) Swarm Map-based Optimisation Decentralised (SMBO-Dec), which performs an asynchronous batch-based Bayesian optimisation to simultaneously explore different controllers for groups of robots in the swarm. We set up foraging experiments with a variety of disturbances: injected faults to proximity sensors, ground sensors, and the actuators of individual robots, with 100 unique combinations for each type. We also investigate disturbances in the operating environment of the swarm, where the swarm has to adapt to drastic changes in the number of resources available in the environment, and to one of the robots behaving disruptively towards the rest of the swarm, with 30 unique conditions for each such perturbation. The viability of SMBO and SMBO-Dec is demonstrated, comparing favourably to variants of random search and gradient descent, and various ablations, and improving performance up to 80% compared to the performance at the time of fault injection within at most 30 evaluations.
<div id='section'>Paperid: <span id='pid'>437, <a href='https://arxiv.org/pdf/2003.02341.pdf' target='_blank'>https://arxiv.org/pdf/2003.02341.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>David M. Bossens, Danesh Tarapore
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2003.02341">QED: using Quality-Environment-Diversity to evolve resilient robot swarms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In swarm robotics, any of the robots in a swarm may be affected by different faults, resulting in significant performance declines. To allow fault recovery from randomly injected faults to different robots in a swarm, a model-free approach may be preferable due to the accumulation of faults in models and the difficulty to predict the behaviour of neighbouring robots. One model-free approach to fault recovery involves two phases: during simulation, a quality-diversity algorithm evolves a behaviourally diverse archive of controllers; during the target application, a search for the best controller is initiated after fault injection. In quality-diversity algorithms, the choice of the behavioural descriptor is a key design choice that determines the quality of the evolved archives, and therefore the fault recovery performance. Although the environment is an important determinant of behaviour, the impact of environmental diversity is often ignored in the choice of a suitable behavioural descriptor. This study compares different behavioural descriptors, including two generic descriptors that work on a wide range of tasks, one hand-coded descriptor which fits the domain of interest, and one novel type of descriptor based on environmental diversity, which we call Quality-Environment-Diversity (QED). Results demonstrate that the above-mentioned model-free approach to fault recovery is feasible in the context of swarm robotics, reducing the fault impact by a factor 2-3. Further, the environmental diversity obtained with QED yields a unique behavioural diversity profile that allows it to recover from high-impact faults.
<div id='section'>Paperid: <span id='pid'>438, <a href='https://arxiv.org/pdf/2510.09963.pdf' target='_blank'>https://arxiv.org/pdf/2510.09963.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chaoran Wang, Jingyuan Sun, Yanhui Zhang, Mingyu Zhang, Changju Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.09963">LLM-HBT: Dynamic Behavior Tree Construction for Adaptive Coordination in Heterogeneous Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a novel framework for automatic behavior tree (BT) construction in heterogeneous multi-robot systems, designed to address the challenges of adaptability and robustness in dynamic environments. Traditional robots are limited by fixed functional attributes and cannot efficiently reconfigure their strategies in response to task failures or environmental changes. To overcome this limitation, we leverage large language models (LLMs) to generate and extend BTs dynamically, combining the reasoning and generalization power of LLMs with the modularity and recovery capability of BTs. The proposed framework consists of four interconnected modules task initialization, task assignment, BT update, and failure node detection which operate in a closed loop. Robots tick their BTs during execution, and upon encountering a failure node, they can either extend the tree locally or invoke a centralized virtual coordinator (Alex) to reassign subtasks and synchronize BTs across peers. This design enables long-term cooperative execution in heterogeneous teams. We validate the framework on 60 tasks across three simulated scenarios and in a real-world cafe environment with a robotic arm and a wheeled-legged robot. Results show that our method consistently outperforms baseline approaches in task success rate, robustness, and scalability, demonstrating its effectiveness for multi-robot collaboration in complex scenarios.
<div id='section'>Paperid: <span id='pid'>439, <a href='https://arxiv.org/pdf/2509.23705.pdf' target='_blank'>https://arxiv.org/pdf/2509.23705.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jun Chen, Mingjia Chen, Shinkyu Park
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23705">MDCPP: Multi-robot Dynamic Coverage Path Planning for Workload Adaptation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-robot Coverage Path Planning (MCPP) addresses the problem of computing paths for multiple robots to effectively cover a large area of interest. Conventional approaches to MCPP typically assume that robots move at fixed velocities, which is often unrealistic in real-world applications where robots must adapt their speeds based on the specific coverage tasks assigned to them.Consequently, conventional approaches often lead to imbalanced workload distribution among robots and increased completion time for coverage tasks. To address this, we introduce a novel Multi-robot Dynamic Coverage Path Planning (MDCPP) algorithm for complete coverage in two-dimensional environments. MDCPP dynamically estimates each robot's remaining workload by approximating the target distribution with Gaussian mixture models, and assigns coverage regions using a capacity-constrained Voronoi diagram. We further develop a distributed implementation of MDCPP for range-constrained robotic networks. Simulation results validate the efficacy of MDCPP, showing qualitative improvements and superior performance compared to an existing sweeping algorithm, and a quantifiable impact of communication range on coverage efficiency.
<div id='section'>Paperid: <span id='pid'>440, <a href='https://arxiv.org/pdf/2509.23705.pdf' target='_blank'>https://arxiv.org/pdf/2509.23705.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jun Chen, Mingjia Chen, Shinkyu Park
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23705">MDCPP: Multi-robot Dynamic Coverage Path Planning for Workload Adaptation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-robot Coverage Path Planning (MCPP) addresses the problem of computing paths for multiple robots to effectively cover a large area of interest. Conventional approaches to MCPP typically assume that robots move at fixed velocities, which is often unrealistic in real-world applications where robots must adapt their speeds based on the specific coverage tasks assigned to them.Consequently, conventional approaches often lead to imbalanced workload distribution among robots and increased completion time for coverage tasks. To address this, we introduce a novel Multi-robot Dynamic Coverage Path Planning (MDCPP) algorithm for complete coverage in two-dimensional environments. MDCPP dynamically estimates each robot's remaining workload by approximating the target distribution with Gaussian mixture models, and assigns coverage regions using a capacity-constrained Voronoi diagram. We further develop a distributed implementation of MDCPP for range-constrained robotic networks. Simulation results validate the efficacy of MDCPP, showing qualitative improvements and superior performance compared to an existing sweeping algorithm, and a quantifiable impact of communication range on coverage efficiency.
<div id='section'>Paperid: <span id='pid'>441, <a href='https://arxiv.org/pdf/2509.11025.pdf' target='_blank'>https://arxiv.org/pdf/2509.11025.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peng Chen, Jing Liang, Hui Song, Kang-Jia Qiao, Cai-Tong Yue, Kun-Jie Yu, Ponnuthurai Nagaratnam Suganthan, Witold Pedrycz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11025">Multi-objective task allocation for electric harvesting robots: a hierarchical route reconstruction approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The increasing labor costs in agriculture have accelerated the adoption of multi-robot systems for orchard harvesting. However, efficiently coordinating these systems is challenging due to the complex interplay between makespan and energy consumption, particularly under practical constraints like load-dependent speed variations and battery limitations. This paper defines the multi-objective agricultural multi-electrical-robot task allocation (AMERTA) problem, which systematically incorporates these often-overlooked real-world constraints. To address this problem, we propose a hybrid hierarchical route reconstruction algorithm (HRRA) that integrates several innovative mechanisms, including a hierarchical encoding structure, a dual-phase initialization method, task sequence optimizers, and specialized route reconstruction operators. Extensive experiments on 45 test instances demonstrate HRRA's superior performance against seven state-of-the-art algorithms. Statistical analysis, including the Wilcoxon signed-rank and Friedman tests, empirically validates HRRA's competitiveness and its unique ability to explore previously inaccessible regions of the solution space. In general, this research contributes to the theoretical understanding of multi-robot coordination by offering a novel problem formulation and an effective algorithm, thereby also providing practical insights for agricultural automation.
<div id='section'>Paperid: <span id='pid'>442, <a href='https://arxiv.org/pdf/2509.11025.pdf' target='_blank'>https://arxiv.org/pdf/2509.11025.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peng Chen, Jing Liang, Hui Song, Kang-Jia Qiao, Cai-Tong Yue, Kun-Jie Yu, Ponnuthurai Nagaratnam Suganthan, Witold Pedrycz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11025">Multi-objective task allocation for electric harvesting robots: a hierarchical route reconstruction approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The increasing labor costs in agriculture have accelerated the adoption of multi-robot systems for orchard harvesting. However, efficiently coordinating these systems is challenging due to the complex interplay between makespan and energy consumption, particularly under practical constraints like load-dependent speed variations and battery limitations. This paper defines the multi-objective agricultural multi-electrical-robot task allocation (AMERTA) problem, which systematically incorporates these often-overlooked real-world constraints. To address this problem, we propose a hybrid hierarchical route reconstruction algorithm (HRRA) that integrates several innovative mechanisms, including a hierarchical encoding structure, a dual-phase initialization method, task sequence optimizers, and specialized route reconstruction operators. Extensive experiments on 45 test instances demonstrate HRRA's superior performance against seven state-of-the-art algorithms. Statistical analysis, including the Wilcoxon signed-rank and Friedman tests, empirically validates HRRA's competitiveness and its unique ability to explore previously inaccessible regions of the solution space. In general, this research contributes to the theoretical understanding of multi-robot coordination by offering a novel problem formulation and an effective algorithm, thereby also providing practical insights for agricultural automation.
<div id='section'>Paperid: <span id='pid'>443, <a href='https://arxiv.org/pdf/2508.17173.pdf' target='_blank'>https://arxiv.org/pdf/2508.17173.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chao Ning, Han Wang, Longyan Li, Yang Shi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17173">Collaborative-Online-Learning-Enabled Distributionally Robust Motion Control for Multi-Robot Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper develops a novel COllaborative-Online-Learning (COOL)-enabled motion control framework for multi-robot systems to avoid collision amid randomly moving obstacles whose motion distributions are partially observable through decentralized data streams. To address the notable challenge of data acquisition due to occlusion, a COOL approach based on the Dirichlet process mixture model is proposed to efficiently extract motion distribution information by exchanging among robots selected learning structures. By leveraging the fine-grained local-moment information learned through COOL, a data-stream-driven ambiguity set for obstacle motion is constructed. We then introduce a novel ambiguity set propagation method, which theoretically admits the derivation of the ambiguity sets for obstacle positions over the entire prediction horizon by utilizing obstacle current positions and the ambiguity set for obstacle motion. Additionally, we develop a compression scheme with its safety guarantee to automatically adjust the complexity and granularity of the ambiguity set by aggregating basic ambiguity sets that are close in a measure space, thereby striking an attractive trade-off between control performance and computation time. Then the probabilistic collision-free trajectories are generated through distributionally robust optimization problems. The distributionally robust obstacle avoidance constraints based on the compressed ambiguity set are equivalently reformulated by deriving separating hyperplanes through tractable semi-definite programming. Finally, we establish the probabilistic collision avoidance guarantee and the long-term tracking performance guarantee for the proposed framework. The numerical simulations are used to demonstrate the efficacy and superiority of the proposed approach compared with state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>444, <a href='https://arxiv.org/pdf/2508.08264.pdf' target='_blank'>https://arxiv.org/pdf/2508.08264.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hadush Hailu, Bruk Gebregziabher, Prudhvi Raj
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08264">Forecast-Driven MPC for Decentralized Multi-Robot Collision Avoidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Iterative Forecast Planner (IFP) is a geometric planning approach that offers lightweight computations, scalable, and reactive solutions for multi-robot path planning in decentralized, communication-free settings. However, it struggles in symmetric configurations, where mirrored interactions often lead to collisions and deadlocks. We introduce eIFP-MPC, an optimized and extended version of IFP that improves robustness and path consistency in dense, dynamic environments. The method refines threat prioritization using a time-to-collision heuristic, stabilizes path generation through cost-based via-point selection, and ensures dynamic feasibility by incorporating model predictive control (MPC) into the planning process. These enhancements are tightly integrated into the IFP to preserve its efficiency while improving its adaptability and stability. Extensive simulations across symmetric and high-density scenarios show that eIFP-MPC significantly reduces oscillations, ensures collision-free motion, and improves trajectory efficiency. The results demonstrate that geometric planners can be strengthened through optimization, enabling robust performance at scale in complex multi-agent environments.
<div id='section'>Paperid: <span id='pid'>445, <a href='https://arxiv.org/pdf/2507.11566.pdf' target='_blank'>https://arxiv.org/pdf/2507.11566.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fuda van Diggelen, Tugay Alperen KaragÃ¼zel, Andres Garcia Rincon, A. E. Eiben, Dario Floreano, Eliseo Ferrante
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.11566">Emergent Heterogeneous Swarm Control Through Hebbian Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we introduce Hebbian learning as a novel method for swarm robotics, enabling the automatic emergence of heterogeneity. Hebbian learning presents a biologically inspired form of neural adaptation that solely relies on local information. By doing so, we resolve several major challenges for learning heterogeneous control: 1) Hebbian learning removes the complexity of attributing emergent phenomena to single agents through local learning rules, thus circumventing the micro-macro problem; 2) uniform Hebbian learning rules across all swarm members limit the number of parameters needed, mitigating the curse of dimensionality with scaling swarm sizes; and 3) evolving Hebbian learning rules based on swarm-level behaviour minimises the need for extensive prior knowledge typically required for optimising heterogeneous swarms. This work demonstrates that with Hebbian learning heterogeneity naturally emerges, resulting in swarm-level behavioural switching and in significantly improved swarm capabilities. It also demonstrates how the evolution of Hebbian learning rules can be a valid alternative to Multi Agent Reinforcement Learning in standard benchmarking tasks.
<div id='section'>Paperid: <span id='pid'>446, <a href='https://arxiv.org/pdf/2506.11285.pdf' target='_blank'>https://arxiv.org/pdf/2506.11285.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianhong Wang, Yang Li, Samuel Kaski, Jonathan Lawry
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.11285">Shapley Machine: A Game-Theoretic Framework for N-Agent Ad Hoc Teamwork</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Open multi-agent systems are increasingly important in modeling real-world applications, such as smart grids, swarm robotics, etc. In this paper, we aim to investigate a recently proposed problem for open multi-agent systems, referred to as n-agent ad hoc teamwork (NAHT), where only a number of agents are controlled. Existing methods tend to be based on heuristic design and consequently lack theoretical rigor and ambiguous credit assignment among agents. To address these limitations, we model and solve NAHT through the lens of cooperative game theory. More specifically, we first model an open multi-agent system, characterized by its value, as an instance situated in a space of cooperative games, generated by a set of basis games. We then extend this space, along with the state space, to accommodate dynamic scenarios, thereby characterizing NAHT. Exploiting the justifiable assumption that basis game values correspond to a sequence of n-step returns with different horizons, we represent the state values for NAHT in a form similar to $Î»$-returns. Furthermore, we derive Shapley values to allocate state values to the controlled agents, as credits for their contributions to the ad hoc team. Different from the conventional approach to shaping Shapley values in an explicit form, we shape Shapley values by fulfilling the three axioms uniquely describing them, well defined on the extended game space describing NAHT. To estimate Shapley values in dynamic scenarios, we propose a TD($Î»$)-like algorithm. The resulting reinforcement learning (RL) algorithm is referred to as Shapley Machine. To our best knowledge, this is the first time that the concepts from cooperative game theory are directly related to RL concepts. In experiments, we demonstrate the effectiveness of Shapley Machine and verify reasonableness of our theory.
<div id='section'>Paperid: <span id='pid'>447, <a href='https://arxiv.org/pdf/2505.15036.pdf' target='_blank'>https://arxiv.org/pdf/2505.15036.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kehinde O. Aina, Hosain Bagheri, Daniel I. Goldman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.15036">Fault-Tolerant Multi-Robot Coordination with Limited Sensing within Confined Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As robots are increasingly deployed to collaborate on tasks within shared workspaces and resources, the failure of an individual robot can critically affect the group's performance. This issue is particularly challenging when robots lack global information or direct communication, relying instead on social interaction for coordination and to complete their tasks. In this study, we propose a novel fault-tolerance technique leveraging physical contact interactions in multi-robot systems, specifically under conditions of limited sensing and spatial confinement. We introduce the "Active Contact Response" (ACR) method, where each robot modulates its behavior based on the likelihood of encountering an inoperative (faulty) robot. Active robots are capable of collectively repositioning stationary and faulty peers to reduce obstructions and maintain optimal group functionality. We implement our algorithm in a team of autonomous robots, equipped with contact-sensing and collision-tolerance capabilities, tasked with collectively excavating cohesive model pellets. Experimental results indicate that the ACR method significantly improves the system's recovery time from robot failures, enabling continued collective excavation with minimal performance degradation. Thus, this work demonstrates the potential of leveraging local, social, and physical interactions to enhance fault tolerance and coordination in multi-robot systems operating in constrained and extreme environments.
<div id='section'>Paperid: <span id='pid'>448, <a href='https://arxiv.org/pdf/2505.00842.pdf' target='_blank'>https://arxiv.org/pdf/2505.00842.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mahboubeh Zarei, Robin Chhabra
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.00842">Fault-Tolerant Multi-Modal Localization of Multi-Robots on Matrix Lie Groups</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Consistent localization of cooperative multi-robot systems during navigation presents substantial challenges. This paper proposes a fault-tolerant, multi-modal localization framework for multi-robot systems on matrix Lie groups. We introduce novel stochastic operations to perform composition, differencing, inversion, averaging, and fusion of correlated and non-correlated estimates on Lie groups, enabling pseudo-pose construction for filter updates. The method integrates a combination of proprioceptive and exteroceptive measurements from inertial, velocity, and pose (pseudo-pose) sensors on each robot in an Extended Kalman Filter (EKF) framework. The prediction step is conducted on the Lie group $\mathbb{SE}_2(3) \times \mathbb{R}^3 \times \mathbb{R}^3$, where each robot's pose, velocity, and inertial measurement biases are propagated. The proposed framework uses body velocity, relative pose measurements from fiducial markers, and inter-robot communication to provide scalable EKF update across the network on the Lie group $\mathbb{SE}(3) \times \mathbb{R}^3$. A fault detection module is implemented, allowing the integration of only reliable pseudo-pose measurements from fiducial markers. We demonstrate the effectiveness of the method through experiments with a network of wheeled mobile robots equipped with inertial measurement units, wheel odometry, and ArUco markers. The comparison results highlight the proposed method's real-time performance, superior efficiency, reliability, and scalability in multi-robot localization, making it well-suited for large-scale robotic systems.
<div id='section'>Paperid: <span id='pid'>449, <a href='https://arxiv.org/pdf/2504.08172.pdf' target='_blank'>https://arxiv.org/pdf/2504.08172.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nithish Kumar Saravanan, Varun Jammula, Yezhou Yang, Jeffrey Wishart, Junfeng Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.08172">Enhanced Cooperative Perception Through Asynchronous Vehicle to Infrastructure Framework with Delay Mitigation for Connected and Automated Vehicles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Perception is a key component of Automated vehicles (AVs). However, sensors mounted to the AVs often encounter blind spots due to obstructions from other vehicles, infrastructure, or objects in the surrounding area. While recent advancements in planning and control algorithms help AVs react to sudden object appearances from blind spots at low speeds and less complex scenarios, challenges remain at high speeds and complex intersections. Vehicle to Infrastructure (V2I) technology promises to enhance scene representation for AVs in complex intersections, providing sufficient time and distance to react to adversary vehicles violating traffic rules. Most existing methods for infrastructure-based vehicle detection and tracking rely on LIDAR, RADAR or sensor fusion methods, such as LIDAR-Camera and RADAR-Camera. Although LIDAR and RADAR provide accurate spatial information, the sparsity of point cloud data limits its ability to capture detailed object contours of objects far away, resulting in inaccurate 3D object detection results. Furthermore, the absence of LIDAR or RADAR at every intersection increases the cost of implementing V2I technology. To address these challenges, this paper proposes a V2I framework that utilizes monocular traffic cameras at road intersections to detect 3D objects. The results from the roadside unit (RSU) are then combined with the on-board system using an asynchronous late fusion method to enhance scene representation. Additionally, the proposed framework provides a time delay compensation module to compensate for the processing and transmission delay from the RSU. Lastly, the V2I framework is tested by simulating and validating a scenario similar to the one described in an industry report by Waymo. The results show that the proposed method improves the scene representation and the AV's perception range, giving enough time and space to react to adversary vehicles.
<div id='section'>Paperid: <span id='pid'>450, <a href='https://arxiv.org/pdf/2504.04774.pdf' target='_blank'>https://arxiv.org/pdf/2504.04774.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Khai Yi Chin, Carlo Pinciroli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.04774">BayesCPF: Enabling Collective Perception in Robot Swarms with Degrading Sensors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The collective perception problem -- where a group of robots perceives its surroundings and comes to a consensus on an environmental state -- is a fundamental problem in swarm robotics. Past works studying collective perception use either an entire robot swarm with perfect sensing or a swarm with only a handful of malfunctioning members. A related study proposed an algorithm that does account for an entire swarm of unreliable robots but assumes that the sensor faults are known and remain constant over time. To that end, we build on that study by proposing the Bayes Collective Perception Filter (BayesCPF) that enables robots with continuously degrading sensors to accurately estimate the fill ratio -- the rate at which an environmental feature occurs. Our main contribution is the Extended Kalman Filter within the BayesCPF, which helps swarm robots calibrate for their time-varying sensor degradation. We validate our method across different degradation models, initial conditions, and environments in simulated and physical experiments. Our findings show that, regardless of degradation model assumptions, fill ratio estimation using the BayesCPF is competitive to the case if the true sensor accuracy is known, especially when assumptions regarding the model and initial sensor accuracy levels are preserved.
<div id='section'>Paperid: <span id='pid'>451, <a href='https://arxiv.org/pdf/2503.00659.pdf' target='_blank'>https://arxiv.org/pdf/2503.00659.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Namo Asavisanu, Tina Khezresmaeilzadeh, Rohan Sequeira, Hang Qiu, Fawad Ahmad, Konstantinos Psounis, Ramesh Govindan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.00659">CATS: A framework for Cooperative Autonomy Trust & Security</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With cooperative perception, autonomous vehicles can wirelessly share sensor data and representations to overcome sensor occlusions, improving situational awareness. Securing such data exchanges is crucial for connected autonomous vehicles. Existing, automated reputation-based approaches often suffer from a delay between detection and exclusion of misbehaving vehicles, while majority-based approaches have communication overheads that limits scalability. In this paper, we introduce CATS, a novel automated system that blends together the best traits of reputation-based and majority-based detection mechanisms to secure vehicle-to-everything (V2X) communications for cooperative perception, while preserving the privacy of cooperating vehicles. Our evaluation with city-scale simulations on realistic traffic data shows CATS's effectiveness in rapidly identifying and isolating misbehaving vehicles, with a low false negative rate and overheads, proving its suitability for real world deployments.
<div id='section'>Paperid: <span id='pid'>452, <a href='https://arxiv.org/pdf/2411.17432.pdf' target='_blank'>https://arxiv.org/pdf/2411.17432.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Susu Fang, Hao Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.17432">Communication-Efficient Cooperative SLAMMOT via Determining the Number of Collaboration Vehicles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The SLAMMOT, i.e. simultaneous localization, mapping, and moving object (detection and) tracking, represents an emerging technology for autonomous vehicles in dynamic environments. Such single-vehicle systems still have inherent limitations, such as occlusion issues. Inspired by SLAMMOT and rapidly evolving cooperative technologies, it is natural to explore cooperative simultaneous localization, mapping, moving object (detection and) tracking (C-SLAMMOT) to enhance state estimation for ego-vehicles and moving objects. C-SLAMMOT could significantly upgrade the single-vehicle performance by utilizing and integrating the shared information through communication among the multiple vehicles. This inevitably leads to a fundamental trade-off between performance and communication cost, especially in a scalable manner as the number of collaboration vehicles increases. To address this challenge, we propose a LiDAR-based communication-efficient C-SLAMMOT (CE C-SLAMMOT) method by determining the number of collaboration vehicles. In CE C-SLAMMOT, we adopt descriptor-based methods for enhancing ego-vehicle pose estimation and spatial confidence map-based methods for cooperative object perception, allowing for the continuous and dynamic selection of the corresponding critical collaboration vehicles and interaction content. This approach avoids the waste of precious communication costs by preventing the sharing of information from certain collaborative vehicles that may contribute little or no performance gain, compared to the baseline method of exchanging raw observation information among all vehicles. Comparative experiments in various aspects have confirmed that the proposed method achieves a good trade-off between performance and communication costs, while also outperforms previous state-of-the-art methods in cooperative perception performance.
<div id='section'>Paperid: <span id='pid'>453, <a href='https://arxiv.org/pdf/2410.21546.pdf' target='_blank'>https://arxiv.org/pdf/2410.21546.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Khai Yi Chin, Carlo Pinciroli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.21546">Adaptive Self-Calibration for Minimalistic Collective Perception by Imperfect Robot Swarms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Collective perception is a fundamental problem in swarm robotics, often cast as best-of-$n$ decision-making. Past studies involve robots with perfect sensing or with small numbers of faulty robots. We previously addressed these limitations by proposing an algorithm, here referred to as Minimalistic Collective Perception (MCP) [arxiv:2209.12858], to reach correct decisions despite the entire swarm having severely damaged sensors. However, this algorithm assumes that sensor accuracy is known, which may be infeasible in reality. In this paper, we eliminate this assumption to (i) investigate the decline of estimation performance and (ii) introduce an Adaptive Sensor Degradation Filter (ASDF) to mitigate the decline. We combine the MCP algorithm and a hypothesis test to enable adaptive self-calibration of robots' assumed sensor accuracy. We validate our approach across several parameters of interest. Our findings show that estimation performance by a swarm with correctly known accuracy is superior to that by a swarm unaware of its accuracy. However, the ASDF drastically mitigates the damage, even reaching the performance levels of robots aware a priori of their correct accuracy.
<div id='section'>Paperid: <span id='pid'>454, <a href='https://arxiv.org/pdf/2409.16851.pdf' target='_blank'>https://arxiv.org/pdf/2409.16851.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Leonardo Santos, Caio C. G. Ribeiro, Douglas G. Macharet
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.16851">Communication Backbone Reconfiguration with Connectivity Maintenance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The exchange of information is key in applications that involve multiple agents, such as search and rescue, military operations, and disaster response. In this work, we propose a simple and effective trajectory planning framework that tackles the design, deployment, and reconfiguration of a communication backbone by reframing the problem of networked multi-agent motion planning as a manipulator motion planning problem. Our approach works for backbones of variable configurations both in terms of the number of robots utilized and the distance limit between each robot. While research has been conducted on connection-restricted navigation for multi-robot systems in the last years, the field of manipulators is arguably more developed both in theory and practice. Hence, our methodology facilitates practical applications built on top of widely available motion planning algorithms and frameworks for manipulators.
<div id='section'>Paperid: <span id='pid'>455, <a href='https://arxiv.org/pdf/2409.02863.pdf' target='_blank'>https://arxiv.org/pdf/2409.02863.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Edward Andert, Francis Mendoza, Hans Walter Behrens, Aviral Shrivastava
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.02863">CONClave -- Secure and Robust Cooperative Perception for CAVs Using Authenticated Consensus and Trust Scoring</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Connected Autonomous Vehicles have great potential to improve automobile safety and traffic flow, especially in cooperative applications where perception data is shared between vehicles. However, this cooperation must be secured from malicious intent and unintentional errors that could cause accidents. Previous works typically address singular security or reliability issues for cooperative driving in specific scenarios rather than the set of errors together. In this paper, we propose CONClave, a tightly coupled authentication, consensus, and trust scoring mechanism that provides comprehensive security and reliability for cooperative perception in autonomous vehicles. CONClave benefits from the pipelined nature of the steps such that faults can be detected significantly faster and with less compute. Overall, CONClave shows huge promise in preventing security flaws, detecting even relatively minor sensing faults, and increasing the robustness and accuracy of cooperative perception in CAVs while adding minimal overhead.
<div id='section'>Paperid: <span id='pid'>456, <a href='https://arxiv.org/pdf/2408.11751.pdf' target='_blank'>https://arxiv.org/pdf/2408.11751.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>David Molina Concha, Jiping Li, Haoran Yin, Kyeonghyeon Park, Hyun-Rok Lee, Taesik Lee, Dhruv Sirohi, Chi-Guhn Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.11751">Bayesian Optimization Framework for Efficient Fleet Design in Autonomous Multi-Robot Exploration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study addresses the challenge of fleet design optimization in the context of heterogeneous multi-robot fleets, aiming to obtain feasible designs that balance performance and costs. In the domain of autonomous multi-robot exploration, reinforcement learning agents play a central role, offering adaptability to complex terrains and facilitating collaboration among robots. However, modifying the fleet composition results in changes in the learned behavior, and training multi-robot systems using multi-agent reinforcement learning is expensive. Therefore, an exhaustive evaluation of each potential fleet design is infeasible. To tackle these hurdles, we introduce Bayesian Optimization for Fleet Design (BOFD), a framework leveraging multi-objective Bayesian Optimization to explore fleets on the Pareto front of performance and cost while accounting for uncertainty in the design space. Moreover, we establish a sub-linear bound for cumulative regret, supporting BOFD's robustness and efficacy. Extensive benchmark experiments in synthetic and simulated environments demonstrate the superiority of our framework over state-of-the-art methods, achieving efficient fleet designs with minimal fleet evaluations.
<div id='section'>Paperid: <span id='pid'>457, <a href='https://arxiv.org/pdf/2407.03091.pdf' target='_blank'>https://arxiv.org/pdf/2407.03091.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>LoÃ¯ck Pierre Chovet, Gabriel Manuel Garcia, Abhishek Bera, Antoine Richard, Kazuya Yoshida, Miguel Angel Olivares-Mendez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.03091">Performance Comparison of ROS2 Middlewares for Multi-robot Mesh Networks in Planetary Exploration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in Multi-Robot Systems (MRS) and mesh network technologies pave the way for innovative approaches to explore extreme environments. The Artemis Accords, a series of international agreements, have further catalyzed this progress by fostering cooperation in space exploration, emphasizing the use of cutting-edge technologies. In parallel, the widespread adoption of the Robot Operating System 2 (ROS 2) by companies across various sectors underscores its robustness and versatility. This paper evaluates the performances of available ROS 2 MiddleWare (RMW), such as FastRTPS, CycloneDDS and Zenoh, over a mesh network with a dynamic topology. The final choice of RMW is determined by the one that would fit the most the scenario: an exploration of the extreme extra-terrestrial environment using a MRS. The conducted study in a real environment highlights Zenoh as a potential solution for future applications, showing a reduced delay, reachability, and CPU usage while being competitive on data overhead and RAM usage over a dynamic mesh topology
<div id='section'>Paperid: <span id='pid'>458, <a href='https://arxiv.org/pdf/2407.01308.pdf' target='_blank'>https://arxiv.org/pdf/2407.01308.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vu Phi Tran, Asanka G. Perera, Matthew A. Garratt, Kathryn Kasmarik, Sreenatha G. Anavatti
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.01308">Active Sensing Strategy: Multi-Modal, Multi-Robot Source Localization and Mapping in Real-World Settings with Fixed One-Way Switching</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces a state-machine model for a multi-modal, multi-robot environmental sensing algorithm tailored to dynamic real-world settings. The algorithm uniquely combines two exploration strategies for gas source localization and mapping: (1) an initial exploration phase using multi-robot coverage path planning with variable formations for early gas field indication; and (2) a subsequent active sensing phase employing multi-robot swarms for precise field estimation. The state machine governs the transition between these two phases. During exploration, a coverage path maximizes the visited area while measuring gas concentration and estimating the initial gas field at predefined sample times. In the active sensing phase, mobile robots in a swarm collaborate to select the next measurement point, ensuring coordinated and efficient sensing. System validation involves hardware-in-the-loop experiments and real-time tests with a radio source emulating a gas field. The approach is benchmarked against state-of-the-art single-mode active sensing and gas source localization techniques. Evaluation highlights the multi-modal switching approach's ability to expedite convergence, navigate obstacles in dynamic environments, and significantly enhance gas source location accuracy. The findings show a 43% reduction in turnaround time, a 50% increase in estimation accuracy, and improved robustness of multi-robot environmental sensing in cluttered scenarios without collisions, surpassing the performance of conventional active sensing strategies.
<div id='section'>Paperid: <span id='pid'>459, <a href='https://arxiv.org/pdf/2406.12473.pdf' target='_blank'>https://arxiv.org/pdf/2406.12473.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sara Aldhaheri, Federico Renda, Giulia De Masi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.12473">Underwater Human-Robot and Human-Swarm Interaction: A Review and Perspective</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>There has been a growing interest in extending the capabilities of autonomous underwater vehicles (AUVs) in subsea missions, particularly in integrating underwater human-robot interaction (UHRI) for control. UHRI and its subfield,underwater gesture recognition (UGR), play a significant role in enhancing diver-robot communication for marine research. This review explores the latest developments in UHRI and examines its promising applications for multi-robot systems. With the developments in UGR, opportunities are presented for underwater robots to work alongside human divers to increase their functionality. Human gestures creates a seamless and safe collaborative environment where divers and robots can interact more efficiently. By highlighting the state-of-the-art in this field, we can potentially encourage advancements in underwater multi-robot system (UMRS) blending the natural communication channels of human-robot interaction with the multi-faceted coordination capabilities of underwater swarms,thus enhancing robustness in complex aquatic environments.
<div id='section'>Paperid: <span id='pid'>460, <a href='https://arxiv.org/pdf/2404.19564.pdf' target='_blank'>https://arxiv.org/pdf/2404.19564.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Michael Amir, Alfred M. Bruckstein
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.19564">Time, Travel, and Energy in the Uniform Dispersion Problem</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We investigate the algorithmic problem of uniformly dispersing a swarm of robots in an unknown, gridlike environment. In this setting, our goal is to study the relationships between performance metrics and robot capabilities. We introduce a formal model comparing dispersion algorithms based on makespan, traveled distance, energy consumption, sensing, communication, and memory. Using this framework, we classify uniform dispersion algorithms according to their capability requirements and performance. We prove that while makespan and travel can be minimized in all environments, energy cannot, if the swarm's sensing range is bounded. In contrast, we show that energy can be minimized by ``ant-like'' robots in synchronous settings and asymptotically minimized in asynchronous settings, provided the environment is topologically simply connected, by using our ``Find-Corner Depth-First Search'' (FCDFS) algorithm. Our theoretical and experimental results show that FCDFS significantly outperforms known algorithms. Our findings reveal key limitations in designing swarm robotics systems for unknown environments, emphasizing the role of topology in energy-efficient dispersion.
<div id='section'>Paperid: <span id='pid'>461, <a href='https://arxiv.org/pdf/2404.14167.pdf' target='_blank'>https://arxiv.org/pdf/2404.14167.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ken Hasselmann, Mario Malizia, Rafael Caballero, Fabio Polisano, Shashank Govindaraj, Jakob Stigler, Oleksii Ilchenko, Milan Bajic, Geert De Cubber
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.14167">A multi-robot system for the detection of explosive devices</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In order to clear the world of the threat posed by landmines and other explosive devices, robotic systems can play an important role. However, the development of such field robots that need to operate in hazardous conditions requires the careful consideration of multiple aspects related to the perception, mobility, and collaboration capabilities of the system. In the framework of a European challenge, the Artificial Intelligence for Detection of Explosive Devices - eXtended (AIDEDeX) project proposes to design a heterogeneous multi-robot system with advanced sensor fusion algorithms. This system is specifically designed to detect and classify improvised explosive devices, explosive ordnances, and landmines. This project integrates specialised sensors, including electromagnetic induction, ground penetrating radar, X-Ray backscatter imaging, Raman spectrometers, and multimodal cameras, to achieve comprehensive threat identification and localisation. The proposed system comprises a fleet of unmanned ground vehicles and unmanned aerial vehicles. This article details the operational phases of the AIDEDeX system, from rapid terrain exploration using unmanned aerial vehicles to specialised detection and classification by unmanned ground vehicles equipped with a robotic manipulator. Initially focusing on a centralised approach, the project will also explore the potential of a decentralised control architecture, taking inspiration from swarm robotics to provide a robust, adaptable, and scalable solution for explosive detection.
<div id='section'>Paperid: <span id='pid'>462, <a href='https://arxiv.org/pdf/2404.00442.pdf' target='_blank'>https://arxiv.org/pdf/2404.00442.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Catie Cuan, Kyle Jeffrey, Kim Kleiven, Adrian Li, Emre Fisher, Matt Harrison, Benjie Holson, Allison Okamura, Matt Bennice
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.00442">Interactive Multi-Robot Flocking with Gesture Responsiveness and Musical Accompaniment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>For decades, robotics researchers have pursued various tasks for multi-robot systems, from cooperative manipulation to search and rescue. These tasks are multi-robot extensions of classical robotic tasks and often optimized on dimensions such as speed or efficiency. As robots transition from commercial and research settings into everyday environments, social task aims such as engagement or entertainment become increasingly relevant. This work presents a compelling multi-robot task, in which the main aim is to enthrall and interest. In this task, the goal is for a human to be drawn to move alongside and participate in a dynamic, expressive robot flock. Towards this aim, the research team created algorithms for robot movements and engaging interaction modes such as gestures and sound. The contributions are as follows: (1) a novel group navigation algorithm involving human and robot agents, (2) a gesture responsive algorithm for real-time, human-robot flocking interaction, (3) a weight mode characterization system for modifying flocking behavior, and (4) a method of encoding a choreographer's preferences inside a dynamic, adaptive, learned system. An experiment was performed to understand individual human behavior while interacting with the flock under three conditions: weight modes selected by a human choreographer, a learned model, or subset list. Results from the experiment showed that the perception of the experience was not influenced by the weight mode selection. This work elucidates how differing task aims such as engagement manifest in multi-robot system design and execution, and broadens the domain of multi-robot tasks.
<div id='section'>Paperid: <span id='pid'>463, <a href='https://arxiv.org/pdf/2403.08238.pdf' target='_blank'>https://arxiv.org/pdf/2403.08238.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junfei Li, Simon X. Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.08238">A Novel Feature Learning-based Bio-inspired Neural Network for Real-time Collision-free Rescue of Multi-Robot Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Natural disasters and urban accidents drive the demand for rescue robots to provide safer, faster, and more efficient rescue trajectories. In this paper, a feature learning-based bio-inspired neural network (FLBBINN) is proposed to quickly generate a heuristic rescue path in complex and dynamic environments, as traditional approaches usually cannot provide a satisfactory solution to real-time responses to sudden environmental changes. The neurodynamic model is incorporated into the feature learning method that can use environmental information to improve path planning strategies. Task assignment and collision-free rescue trajectory are generated through robot poses and the dynamic landscape of neural activity. A dual-channel scale filter, a neural activity channel, and a secondary distance fusion are employed to extract and filter feature neurons. After completion of the feature learning process, a neurodynamics-based feature matrix is established to quickly generate the new heuristic rescue paths with parameter-driven topological adaptability. The proposed FLBBINN aims to reduce the computational complexity of the neural network-based approach and enable the feature learning method to achieve real-time responses to environmental changes. Several simulations and experiments have been conducted to evaluate the performance of the proposed FLBBINN. The results show that the proposed FLBBINN would significantly improve the speed, efficiency, and optimality for rescue operations.
<div id='section'>Paperid: <span id='pid'>464, <a href='https://arxiv.org/pdf/2403.05815.pdf' target='_blank'>https://arxiv.org/pdf/2403.05815.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nathaniel Moore Glaser, Rajashree Ravi, Zsolt Kira
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.05815">N-QR: Natural Quick Response Codes for Multi-Robot Instance Correspondence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image correspondence serves as the backbone for many tasks in robotics, such as visual fusion, localization, and mapping. However, existing correspondence methods do not scale to large multi-robot systems, and they struggle when image features are weak, ambiguous, or evolving. In response, we propose Natural Quick Response codes, or N-QR, which enables rapid and reliable correspondence between large-scale teams of heterogeneous robots. Our method works like a QR code, using keypoint-based alignment, rapid encoding, and error correction via ensembles of image patches of natural patterns. We deploy our algorithm in a production-scale robotic farm, where groups of growing plants must be matched across many robots. We demonstrate superior performance compared to several baselines, obtaining a retrieval accuracy of 88.2%. Our method generalizes to a farm with 100 robots, achieving a 12.5x reduction in bandwidth and a 20.5x speedup. We leverage our method to correspond 700k plants and confirm a link between a robotic seeding policy and germination.
<div id='section'>Paperid: <span id='pid'>465, <a href='https://arxiv.org/pdf/2403.03746.pdf' target='_blank'>https://arxiv.org/pdf/2403.03746.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Julian Kaduk, Friederike Weilbeer, Heiko Hamann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.03746">Emotional Tandem Robots: How Different Robot Behaviors Affect Human Perception While Controlling a Mobile Robot</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In human-robot interaction (HRI), we study how humans interact with robots, but also the effects of robot behavior on human perception and well-being. Especially, the influence on humans by tandem robots with one human controlled and one autonomous robot or even semi-autonomous multi-robot systems is not yet fully understood. Here, we focus on a leader-follower scenario and study how emotionally expressive motion patterns of a small, mobile follower robot affect the perception of a human operator controlling the leading robot. We examined three distinct emotional behaviors for the follower compared to a neutral condition: angry, happy and sad. We analyzed how participants maneuvered the leader robot along a set path while experiencing each follower behavior in a randomized order. We identified a significant shift in attention toward the follower with emotionally expressive behaviors compared to the neutral condition. For example, the angry behavior significantly heightened participant stress levels and was considered the least preferred behavior. The happy behavior was the most preferred and associated with increased excitement by the participants. Integrating the proposed behaviors in robots can profoundly influence the human operator's attention, emotional state, and overall experience. These insights are valuable for future HRI tandem robot designs.
<div id='section'>Paperid: <span id='pid'>466, <a href='https://arxiv.org/pdf/2402.06014.pdf' target='_blank'>https://arxiv.org/pdf/2402.06014.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Renan Lima Baima, LoÃ¯ck Chovet, Eduard Hartwich, Abhishek Bera, Johannes Sedlmeir, Gilbert Fridgen, Miguel Angel Olivares-Mendez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.06014">Trustful Coopetitive Infrastructures for the New Space Exploration Era</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the new space economy, space agencies, large enterprises, and start-ups aim to launch space multi-robot systems (MRS) for various in-situ resource utilization (ISRU) purposes, such as mapping, soil evaluation, and utility provisioning. However, these stakeholders' competing economic interests may hinder effective collaboration on a centralized digital platform. To address this issue, neutral and transparent infrastructures could facilitate coordination and value exchange among heterogeneous space MRS. While related work has expressed legitimate concerns about the technical challenges associated with blockchain use in space, we argue that weighing its potential economic benefits against its drawbacks is necessary. This paper presents a novel architectural framework and a comprehensive set of requirements for integrating blockchain technology in MRS, aiming to enhance coordination and data integrity in space exploration missions. We explored distributed ledger technology (DLT) to design a non-proprietary architecture for heterogeneous MRS and validated the prototype in a simulated lunar environment. The analyses of our implementation suggest global ISRU efficiency improvements for map exploration, compared to a corresponding group of individually acting robots, and that fostering a coopetitive environment may provide additional revenue opportunities for stakeholders.
<div id='section'>Paperid: <span id='pid'>467, <a href='https://arxiv.org/pdf/2401.15399.pdf' target='_blank'>https://arxiv.org/pdf/2401.15399.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lianxin Zhang, Yang Jiao, Yihan Huang, Ziyou Wang, Huihuan Qian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.15399">Parallel Self-assembly for Modular USVs with Diverse Docking Mechanism Layouts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Self-assembly enables multi-robot systems to merge diverse capabilities and accomplish tasks beyond the reach of individual robots. Incorporating varied docking mechanisms layouts (DMLs) can enhance robot versatility or reduce costs. However, assembling multiple heterogeneous robots with diverse DMLs is still a research gap. This paper addresses this problem by introducing CuBoat, an omnidirectional unmanned surface vehicle (USV). CuBoat can be equipped with or without docking systems on its four sides to emulate heterogeneous robots. We implement a multi-robot system based on multiple CuBoats. To enhance maneuverability, a linear active disturbance rejection control (LADRC) scheme is proposed. Additionally, we present a generalized parallel self-assembly planning algorithm for efficient assembly among CuBoats with different DMLs. Validation is conducted through simulation within 2 scenarios across 4 distinct maps, demonstrating the performance of the self-assembly planning algorithm. Moreover, trajectory tracking tests confirm the effectiveness of the LADRC controller. Self-assembly experiments on 5 maps with different target structures affirm the algorithm's feasibility and generality. This study advances robotic self-assembly, enabling multi-robot systems to collaboratively tackle complex tasks beyond the capabilities of individual robots.
<div id='section'>Paperid: <span id='pid'>468, <a href='https://arxiv.org/pdf/2401.14325.pdf' target='_blank'>https://arxiv.org/pdf/2401.14325.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dominik RÃ¶Ãle, Jeremias Gerner, Klaus Bogenberger, Daniel Cremers, Stefanie Schmidtner, Torsten SchÃ¶n
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.14325">Unlocking Past Information: Temporal Embeddings in Cooperative Bird's Eye View Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate and comprehensive semantic segmentation of Bird's Eye View (BEV) is essential for ensuring safe and proactive navigation in autonomous driving. Although cooperative perception has exceeded the detection capabilities of single-agent systems, prevalent camera-based algorithms in cooperative perception neglect valuable information derived from historical observations. This limitation becomes critical during sensor failures or communication issues as cooperative perception reverts to single-agent perception, leading to degraded performance and incomplete BEV segmentation maps. This paper introduces TempCoBEV, a temporal module designed to incorporate historical cues into current observations, thereby improving the quality and reliability of BEV map segmentations. We propose an importance-guided attention architecture to effectively integrate temporal information that prioritizes relevant properties for BEV map segmentation. TempCoBEV is an independent temporal module that seamlessly integrates into state-of-the-art camera-based cooperative perception models. We demonstrate through extensive experiments on the OPV2V dataset that TempCoBEV performs better than non-temporal models in predicting current and future BEV map segmentations, particularly in scenarios involving communication failures. We show the efficacy of TempCoBEV and its capability to integrate historical cues into the current BEV map, improving predictions under optimal communication conditions by up to 2% and under communication failures by up to 19%. The code will be published on GitHub.
<div id='section'>Paperid: <span id='pid'>469, <a href='https://arxiv.org/pdf/2308.12698.pdf' target='_blank'>https://arxiv.org/pdf/2308.12698.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinjie Li, Liang Han, Haoyang Yu, Zhaotian Wang, Pengzhi Yang, Ziwei Yan, Zhang Ren
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.12698">Potato: A Data-Oriented Programming 3D Simulator for Large-Scale Heterogeneous Swarm Robotics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large-scale simulation with realistic nonlinear dynamic models is crucial for algorithms development for swarm robotics. However, existing platforms are mainly developed based on Object-Oriented Programming (OOP) and either use simple kinematic models to pursue a large number of simulating nodes or implement realistic dynamic models with limited simulating nodes. In this paper, we develop a simulator based on Data-Oriented Programming (DOP) that utilizes GPU parallel computing to achieve large-scale swarm robotic simulations. Specifically, we use a multi-process approach to simulate heterogeneous agents and leverage PyTorch with GPU to simulate homogeneous agents with a large number. We test our approach using a nonlinear quadrotor model and demonstrate that this DOP approach can maintain almost the same computational speed when quadrotors are less than 5,000. We also provide two examples to present the functionality of the platform.
<div id='section'>Paperid: <span id='pid'>470, <a href='https://arxiv.org/pdf/2308.12624.pdf' target='_blank'>https://arxiv.org/pdf/2308.12624.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianan Li, Liang Li, Shiyu Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.12624">Predator-prey survival pressure is sufficient to evolve swarming behaviors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The comprehension of how local interactions arise in global collective behavior is of utmost importance in both biological and physical research. Traditional agent-based models often rely on static rules that fail to capture the dynamic strategies of the biological world. Reinforcement learning has been proposed as a solution, but most previous methods adopt handcrafted reward functions that implicitly or explicitly encourage the emergence of swarming behaviors. In this study, we propose a minimal predator-prey coevolution framework based on mixed cooperative-competitive multiagent reinforcement learning, and adopt a reward function that is solely based on the fundamental survival pressure, that is, prey receive a reward of $-1$ if caught by predators while predators receive a reward of $+1$. Surprisingly, our analysis of this approach reveals an unexpectedly rich diversity of emergent behaviors for both prey and predators, including flocking and swirling behaviors for prey, as well as dispersion tactics, confusion, and marginal predation phenomena for predators. Overall, our study provides novel insights into the collective behavior of organisms and highlights the potential applications in swarm robotics.
<div id='section'>Paperid: <span id='pid'>471, <a href='https://arxiv.org/pdf/2307.06000.pdf' target='_blank'>https://arxiv.org/pdf/2307.06000.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pian Yu, Gianmarco Fedeli, Dimos V. Dimarogonas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.06000">Reactive and human-in-the-loop planning and control of multi-robot systems under LTL specifications in dynamic environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper investigates the planning and control problems for multi-robot systems under linear temporal logic (LTL) specifications. In contrast to most of existing literature, which presumes a static and known environment, our study focuses on dynamic environments that can have unknown moving obstacles like humans walking through. Depending on whether local communication is allowed between robots, we consider two different online re-planning approaches. When local communication is allowed, we propose a local trajectory generation algorithm for each robot to resolve conflicts that are detected on-line. In the other case, i.e., no communication is allowed, we develop a model predictive controller to reactively avoid potential collisions. In both cases, task satisfaction is guaranteed whenever it is feasible. In addition, we consider the human-in-the-loop scenario where humans may additionally take control of one or multiple robots. We design a mixed initiative controller for each robot to prevent unsafe human behaviors while guarantee the LTL satisfaction. Using our previous developed ROS software package, several experiments are conducted to demonstrate the effectiveness and the applicability of the proposed strategies.
<div id='section'>Paperid: <span id='pid'>472, <a href='https://arxiv.org/pdf/2307.03103.pdf' target='_blank'>https://arxiv.org/pdf/2307.03103.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Behzad Akbari, Zikai Wang, Haibin Zhu, Lucas Wan, Ryan Adderson, Ya-Jun Pan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.03103">Role Engine Implementation for a Continuous and Collaborative Multi-Robot System</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In situations involving teams of diverse robots, assigning appropriate roles to each robot and evaluating their performance is crucial. These roles define the specific characteristics of a robot within a given context. The stream actions exhibited by a robot based on its assigned role are referred to as the process role. Our research addresses the depiction of process roles using a multivariate probabilistic function. The main aim of this study is to develop a role engine for collaborative multi-robot systems and optimize the behavior of the robots. The role engine is designed to assign suitable roles to each robot, generate approximately optimal process roles, update them on time, and identify instances of robot malfunction or trigger replanning when necessary. The environment considered is dynamic, involving obstacles and other agents. The role engine operates hybrid, with central initiation and decentralized action, and assigns unlabeled roles to agents. We employ the Gaussian Process (GP) inference method to optimize process roles based on local constraints and constraints related to other agents. Furthermore, we propose an innovative approach that utilizes the environment's skeleton to address initialization and feasibility evaluation challenges. We successfully demonstrated the proposed approach's feasibility, and efficiency through simulation studies and real-world experiments involving diverse mobile robots.
<div id='section'>Paperid: <span id='pid'>473, <a href='https://arxiv.org/pdf/2306.16501.pdf' target='_blank'>https://arxiv.org/pdf/2306.16501.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Abhinav Dahiya, Yifan Cai, Oliver Schneider, Stephen L. Smith
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.16501">On the Impact of Interruptions During Multi-Robot Supervision Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human supervisors in multi-robot systems are primarily responsible for monitoring robots, but can also be assigned with secondary tasks. These tasks can act as interruptions and can be categorized as either intrinsic, i.e., being directly related to the monitoring task, or extrinsic, i.e., being unrelated. In this paper, we investigate the impact of these two types of interruptions through a user study ($N=39$), where participants monitor a number of remote mobile robots while intermittently being interrupted by either a robot fault correction task (intrinsic) or a messaging task (extrinsic). We find that task performance of participants does not change significantly with the interruptions but depends greatly on the number of robots. However, interruptions result in an increase in perceived workload, and extrinsic interruptions have a more negative effect on workload across all NASA-TLX scales. Participants also reported switching between extrinsic interruptions and the primary task to be more difficult compared to the intrinsic interruption case. Statistical significance of these results is confirmed using ANOVA and one-sample t-test. These findings suggest that when deciding task assignment in such supervision systems, one should limit interruptions from secondary tasks, especially extrinsic ones, in order to limit user workload.
<div id='section'>Paperid: <span id='pid'>474, <a href='https://arxiv.org/pdf/2305.15729.pdf' target='_blank'>https://arxiv.org/pdf/2305.15729.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junfeng Chen, Zili Tang, Meng Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.15729">Accelerated K-Serial Stable Coalition for Dynamic Capture and Resource Defense</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Coalition is an important mean of multi-robot systems to collaborate on common tasks. An adaptive coalition strategy is essential for the online performance in dynamic and unknown environments. In this work, the problem of territory defense by large-scale heterogeneous robotic teams is considered. The tasks include exploration, capture of dynamic targets, and perimeter defense over valuable resources. Since each robot can choose among many tasks, it remains a challenging problem to coordinate jointly these robots such that the overall utility is maximized. This work proposes a generic coalition strategy called K-serial stable coalition algorithm. Different from centralized approaches, it is distributed and complete, meaning that only local communication is required and a K-serial Stable solution is ensured. Furthermore, to accelerate adaptation to dynamic targets and resource distribution that are only perceived online, a heterogeneous graph attention network based heuristic is learned to select more appropriate parameters and promising initial solutions during local optimization. Compared with manual heuristics or end-to-end predictors, it is shown to both improve online adaptability and retain the quality guarantee. The proposed methods are validated via large-scale simulations with 170 robots and hardware experiments of 13 robots, against several strong baselines such as GreedyNE and FastMaxSum.
<div id='section'>Paperid: <span id='pid'>475, <a href='https://arxiv.org/pdf/2304.11407.pdf' target='_blank'>https://arxiv.org/pdf/2304.11407.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pengda Mao, Rao Fu, Quan Quan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.11407">Optimal Virtual Tube Planning and Control for Swarm Robotics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a novel method for efficiently solving a trajectory planning problem for swarm robotics in cluttered environments. Recent research has demonstrated high success rates in real-time local trajectory planning for swarm robotics in cluttered environments, but optimizing trajectories for each robot is still computationally expensive, with a computational complexity from $O\left(k\left(n_t,\varepsilon \right)n_t^2\right)$ to $ O\left(k\left(n_t,\varepsilon \right)n_t^3\right)$ where $n_t$ is the number of parameters in the parameterized trajectory, $\varepsilon$ is precision and $k\left(n_t,\varepsilon \right)$ is the number of iterations with respect to $n_t$ and $\varepsilon$. Furthermore, the swarm is difficult to move as a group. To address this issue, we define and then construct the optimal virtual tube, which includes infinite optimal trajectories. Under certain conditions, any optimal trajectory in the optimal virtual tube can be expressed as a convex combination of a finite number of optimal trajectories, with a computational complexity of $O\left(n_t\right)$. Afterward, a hierarchical approach including a planning method of the optimal virtual tube with minimizing energy and distributed model predictive control is proposed. In simulations and experiments, the proposed approach is validated and its effectiveness over other methods is demonstrated through comparison.
<div id='section'>Paperid: <span id='pid'>476, <a href='https://arxiv.org/pdf/2304.02075.pdf' target='_blank'>https://arxiv.org/pdf/2304.02075.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nikhil Angad Bakshi, Tejus Gupta, Ramina Ghods, Jeff Schneider
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.02075">GUTS: Generalized Uncertainty-Aware Thompson Sampling for Multi-Agent Active Search</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robotic solutions for quick disaster response are essential to ensure minimal loss of life, especially when the search area is too dangerous or too vast for human rescuers. We model this problem as an asynchronous multi-agent active-search task where each robot aims to efficiently seek objects of interest (OOIs) in an unknown environment. This formulation addresses the requirement that search missions should focus on quick recovery of OOIs rather than full coverage of the search region. Previous approaches fail to accurately model sensing uncertainty, account for occlusions due to foliage or terrain, or consider the requirement for heterogeneous search teams and robustness to hardware and communication failures. We present the Generalized Uncertainty-aware Thompson Sampling (GUTS) algorithm, which addresses these issues and is suitable for deployment on heterogeneous multi-robot systems for active search in large unstructured environments. We show through simulation experiments that GUTS consistently outperforms existing methods such as parallelized Thompson Sampling and exhaustive search, recovering all OOIs in 80% of all runs. In contrast, existing approaches recover all OOIs in less than 40% of all runs. We conduct field tests using our multi-robot system in an unstructured environment with a search area of approximately 75,000 sq. m. Our system demonstrates robustness to various failure modes, achieving full recovery of OOIs (where feasible) in every field run, and significantly outperforming our baseline.
<div id='section'>Paperid: <span id='pid'>477, <a href='https://arxiv.org/pdf/2303.11279.pdf' target='_blank'>https://arxiv.org/pdf/2303.11279.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiu Ming Chung, Hazem Youssef, Moritz Roidl
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.11279">Distributed Timed Elastic Band (DTEB) Planner: Trajectory Sharing and Collision Prediction for Multi-Robot Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous navigation of mobile robots is a well studied problem in robotics. However, the navigation task becomes challenging when multi-robot systems have to cooperatively navigate dynamic environments with deadlock-prone layouts. We present a Distributed Timed Elastic Band (DTEB) Planner that combines Prioritized Planning with the online TEB trajectory Planner, in order to extend the capabilities of the latter to multi-robot systems. The proposed planner is able to reactively avoid imminent collisions as well as predictively resolve potential deadlocks among a team of robots, while navigating in a complex environment. The results of our simulation demonstrate the reliable performance and the versatility of the planner in different environment settings. The code and tests for our approach are available online.
<div id='section'>Paperid: <span id='pid'>478, <a href='https://arxiv.org/pdf/2209.14951.pdf' target='_blank'>https://arxiv.org/pdf/2209.14951.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Leonardo Pedroso, Pedro Batista
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.14951">Distributed decentralized receding horizon control for very large-scale networks with application to satellite mega-constellations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The implementation feasibility of control algorithms over very large-scale networks calls for hard constraints regarding communication, computational, and memory requirements. In this paper, the decentralized receding horizon control problem for very large-scale networks of dynamically decoupled systems with a common, possibly time-varying, control objective is addressed. Each system is assumed to be modeled by linear time-varying dynamics, which can be leveraged to approximate nonlinear systems about successive points of operation. A distributed and decentralized receding horizon control solution is put forward, which: i) takes communication delays into account; ii) allows local communication exclusively; and iii) whose computational and memory requirements in each computational unit do not scale with the dimension of the network. The scalability of the proposed solution enables emerging very large-scale applications of swarm robotics and networked control. This approach is applied to the orbit control problem of low Earth orbit mega-constellations, featuring high-fidelity numerical simulations for the Starlink mega-constellation.
<div id='section'>Paperid: <span id='pid'>479, <a href='https://arxiv.org/pdf/2209.12858.pdf' target='_blank'>https://arxiv.org/pdf/2209.12858.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Khai Yi Chin, Yara Khaluf, Carlo Pinciroli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.12858">Minimalistic Collective Perception with Imperfect Sensors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Collective perception is a foundational problem in swarm robotics, in which the swarm must reach consensus on a coherent representation of the environment. An important variant of collective perception casts it as a best-of-$n$ decision-making process, in which the swarm must identify the most likely representation out of a set of alternatives. Past work on this variant primarily focused on characterizing how different algorithms navigate the speed-vs-accuracy tradeoff in a scenario where the swarm must decide on the most frequent environmental feature. Crucially, past work on best-of-$n$ decision-making assumes the robot sensors to be perfect (noise- and fault-less), limiting the real-world applicability of these algorithms. In this paper, we derive from first principles an optimal, probabilistic framework for minimalistic swarm robots equipped with flawed sensors. Then, we validate our approach in a scenario where the swarm collectively decides the frequency of a certain environmental feature. We study the speed and accuracy of the decision-making process with respect to several parameters of interest. Our approach can provide timely and accurate frequency estimates even in presence of severe sensory noise.
<div id='section'>Paperid: <span id='pid'>480, <a href='https://arxiv.org/pdf/2207.08930.pdf' target='_blank'>https://arxiv.org/pdf/2207.08930.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fawad Ahmad, Christina Suyong Shin, Weiwu Pang, Branden Leong, Pradipta Ghosh, Ramesh Govindan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2207.08930">Cooperative Infrastructure Perception</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent works have considered two qualitatively different approaches to overcome line-of-sight limitations of 3D sensors used for perception: cooperative perception and infrastructure-augmented perception. In this paper, motivated by increasing deployments of infrastructure LiDARs, we explore a third approach, cooperative infrastructure perception. This approach generates perception outputs by fusing outputs of multiple infrastructure sensors, but, to be useful, must do so quickly and accurately. We describe the design, implementation and evaluation of Cooperative Infrastructure Perception (CIP), which uses a combination of novel algorithms and systems optimizations. It produces perception outputs within 100 ms using modest computing resources and with accuracy comparable to the state-of-the-art. CIP, when used to augment vehicle perception, can improve safety. When used in conjunction with offloaded planning, CIP can increase traffic throughput at intersections.
<div id='section'>Paperid: <span id='pid'>481, <a href='https://arxiv.org/pdf/2205.00955.pdf' target='_blank'>https://arxiv.org/pdf/2205.00955.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaolong Wang, Alp Sahin, Subhrajit Bhattacharya
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2205.00955">Coordination-free Multi-robot Path Planning for Congestion Reduction Using Topological Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We consider the problem of multi-robot path planning in a complex, cluttered environment with the aim of reducing overall congestion in the environment, while avoiding any inter-robot communication or coordination. Such limitations may exist due to lack of communication or due to privacy restrictions (for example, autonomous vehicles may not want to share their locations or intents with other vehicles or even to a central server). The key insight that allows us to solve this problem is to stochastically distribute the robots across different routes in the environment by assigning them paths in different topologically distinct classes, so as to lower congestion and the overall travel time for all robots in the environment. We outline the computation of topologically distinct paths in a spatio-temporal configuration space and propose methods for the stochastic assignment of paths to the robots. A fast replanning algorithm and a potential field based controller allow robots to avoid collision with nearby agents while following the assigned path. Our simulation and experiment results show a significant advantage over shortest path following under such a coordination-free setup.
<div id='section'>Paperid: <span id='pid'>482, <a href='https://arxiv.org/pdf/2011.13219.pdf' target='_blank'>https://arxiv.org/pdf/2011.13219.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qingbiao Li, Weizhe Lin, Zhe Liu, Amanda Prorok
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2011.13219">Message-Aware Graph Attention Networks for Large-Scale Multi-Robot Path Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The domains of transport and logistics are increasingly relying on autonomous mobile robots for the handling and distribution of passengers or resources. At large system scales, finding decentralized path planning and coordination solutions is key to efficient system performance. Recently, Graph Neural Networks (GNNs) have become popular due to their ability to learn communication policies in decentralized multi-agent systems. Yet, vanilla GNNs rely on simplistic message aggregation mechanisms that prevent agents from prioritizing important information. To tackle this challenge, in this paper, we extend our previous work that utilizes GNNs in multi-agent path planning by incorporating a novel mechanism to allow for message-dependent attention. Our Message-Aware Graph Attention neTwork (MAGAT) is based on a key-query-like mechanism that determines the relative importance of features in the messages received from various neighboring robots. We show that MAGAT is able to achieve a performance close to that of a coupled centralized expert algorithm. Further, ablation studies and comparisons to several benchmark models show that our attention mechanism is very effective across different robot densities and performs stably in different constraints in communication bandwidth. Experiments demonstrate that our model is able to generalize well in previously unseen problem instances, and that it achieves a 47\% improvement over the benchmark success rate, even in very large-scale instances that are $\times$100 larger than the training instances.
<div id='section'>Paperid: <span id='pid'>483, <a href='https://arxiv.org/pdf/2009.05710.pdf' target='_blank'>https://arxiv.org/pdf/2009.05710.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shengkai Li, Bahnisikha Dutta, Sarah Cannon, Joshua J. Daymude, Ram Avinery, Enes Aydin, AndrÃ©a W. Richa, Daniel I. Goldman, Dana Randall
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2009.05710">Programming Active Cohesive Granular Matter with Mechanically Induced Phase Changes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Active matter physics and swarm robotics have provided powerful tools for the study and control of ensembles driven by internal sources. At the macroscale, controlling swarms typically utilizes significant memory, processing power, and coordination unavailable at the microscale, e.g., for colloidal robots, which could be useful for fighting disease, fabricating intelligent textiles, and designing nanocomputers. To develop principles that that can leverage physics of interactions and thus can be utilized across scales, we take a two-pronged approach: a theoretical abstraction of self-organizing particle systems and an experimental robot system of active cohesive granular matter that intentionally lacks digital electronic computation and communication, using minimal (or no) sensing and control, to test theoretical predictions. We consider the problems of aggregation, dispersion, and collective transport. As predicted by the theory, as a parameter representing interparticle attraction increases, the robots transition from a dispersed phase to an aggregated one, forming a dense, compact collective. When aggregated, the collective can transport non-robot "impurities" in their environment, thus performing an emergent task driven by the physics underlying the transition. These results point to a fruitful interplay between algorithm design and active matter robophysics that can result in new nonequilibrium physics and principles for programming collectives without the need for complex algorithms or capabilities.
<div id='section'>Paperid: <span id='pid'>484, <a href='https://arxiv.org/pdf/2509.25097.pdf' target='_blank'>https://arxiv.org/pdf/2509.25097.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jesús Roche, Eduardo Sebastián, Eduardo Montijano
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25097">Curriculum Imitation Learning of Distributed Multi-Robot Policies</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning control policies for multi-robot systems (MRS) remains a major challenge due to long-term coordination and the difficulty of obtaining realistic training data. In this work, we address both limitations within an imitation learning framework. First, we shift the typical role of Curriculum Learning in MRS, from scalability with the number of robots, to focus on improving long-term coordination. We propose a curriculum strategy that gradually increases the length of expert trajectories during training, stabilizing learning and enhancing the accuracy of long-term behaviors. Second, we introduce a method to approximate the egocentric perception of each robot using only third-person global state demonstrations. Our approach transforms idealized trajectories into locally available observations by filtering neighbors, converting reference frames, and simulating onboard sensor variability. Both contributions are integrated into a physics-informed technique to produce scalable, distributed policies from observations. We conduct experiments across two tasks with varying team sizes and noise levels. Results show that our curriculum improves long-term accuracy, while our perceptual estimation method yields policies that are robust to realistic uncertainty. Together, these strategies enable the learning of robust, distributed controllers from global demonstrations, even in the absence of expert actions or onboard measurements.
<div id='section'>Paperid: <span id='pid'>485, <a href='https://arxiv.org/pdf/2509.25097.pdf' target='_blank'>https://arxiv.org/pdf/2509.25097.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jesús Roche, Eduardo Sebastián, Eduardo Montijano
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25097">Curriculum Imitation Learning of Distributed Multi-Robot Policies</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning control policies for multi-robot systems (MRS) remains a major challenge due to long-term coordination and the difficulty of obtaining realistic training data. In this work, we address both limitations within an imitation learning framework. First, we shift the typical role of Curriculum Learning in MRS, from scalability with the number of robots, to focus on improving long-term coordination. We propose a curriculum strategy that gradually increases the length of expert trajectories during training, stabilizing learning and enhancing the accuracy of long-term behaviors. Second, we introduce a method to approximate the egocentric perception of each robot using only third-person global state demonstrations. Our approach transforms idealized trajectories into locally available observations by filtering neighbors, converting reference frames, and simulating onboard sensor variability. Both contributions are integrated into a physics-informed technique to produce scalable, distributed policies from observations. We conduct experiments across two tasks with varying team sizes and noise levels. Results show that our curriculum improves long-term accuracy, while our perceptual estimation method yields policies that are robust to realistic uncertainty. Together, these strategies enable the learning of robust, distributed controllers from global demonstrations, even in the absence of expert actions or onboard measurements.
<div id='section'>Paperid: <span id='pid'>486, <a href='https://arxiv.org/pdf/2509.22469.pdf' target='_blank'>https://arxiv.org/pdf/2509.22469.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ben Rossano, Jaein Lim, Jonathan P. How
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22469">Uncertainty-Aware Multi-Robot Task Allocation With Strongly Coupled Inter-Robot Rewards</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper proposes a task allocation algorithm for teams of heterogeneous robots in environments with uncertain task requirements. We model these requirements as probability distributions over capabilities and use this model to allocate tasks such that robots with complementary skills naturally position near uncertain tasks, proactively mitigating task failures without wasting resources. We introduce a market-based approach that optimizes the joint team objective while explicitly capturing coupled rewards between robots, offering a polynomial-time solution in decentralized settings with strict communication assumptions. Comparative experiments against benchmark algorithms demonstrate the effectiveness of our approach and highlight the challenges of incorporating coupled rewards in a decentralized formulation.
<div id='section'>Paperid: <span id='pid'>487, <a href='https://arxiv.org/pdf/2509.22469.pdf' target='_blank'>https://arxiv.org/pdf/2509.22469.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ben Rossano, Jaein Lim, Jonathan P. How
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22469">Uncertainty-Aware Multi-Robot Task Allocation With Strongly Coupled Inter-Robot Rewards</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper proposes a task allocation algorithm for teams of heterogeneous robots in environments with uncertain task requirements. We model these requirements as probability distributions over capabilities and use this model to allocate tasks such that robots with complementary skills naturally position near uncertain tasks, proactively mitigating task failures without wasting resources. We introduce a market-based approach that optimizes the joint team objective while explicitly capturing coupled rewards between robots, offering a polynomial-time solution in decentralized settings with strict communication assumptions. Comparative experiments against benchmark algorithms demonstrate the effectiveness of our approach and highlight the challenges of incorporating coupled rewards in a decentralized formulation.
<div id='section'>Paperid: <span id='pid'>488, <a href='https://arxiv.org/pdf/2509.16267.pdf' target='_blank'>https://arxiv.org/pdf/2509.16267.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Victor V. Puche, Kashish Verma, Matteo Fumagalli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.16267">Underground Multi-robot Systems at Work: a revolution in mining</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The growing global demand for critical raw materials (CRMs) has highlighted the need to access difficult and hazardous environments such as abandoned underground mines. These sites pose significant challenges for conventional machinery and human operators due to confined spaces, structural instability, and lack of infrastructure. To address this, we propose a modular multi-robot system designed for autonomous operation in such environments, enabling sequential mineral extraction tasks. Unlike existing work that focuses primarily on mapping and inspection through global behavior or central control, our approach incorporates physical interaction capabilities using specialized robots coordinated through local high-level behavior control. Our proposed system utilizes Hierarchical Finite State Machine (HFSM) behaviors to structure complex task execution across heterogeneous robotic platforms. Each robot has its own HFSM behavior to perform sequential autonomy while maintaining overall system coordination, achieved by triggering behavior execution through inter-robot communication. This architecture effectively integrates software and hardware components to support collaborative, task-driven multi-robot operation in confined underground environments.
<div id='section'>Paperid: <span id='pid'>489, <a href='https://arxiv.org/pdf/2509.16267.pdf' target='_blank'>https://arxiv.org/pdf/2509.16267.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Victor V. Puche, Kashish Verma, Matteo Fumagalli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.16267">Underground Multi-robot Systems at Work: a revolution in mining</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The growing global demand for critical raw materials (CRMs) has highlighted the need to access difficult and hazardous environments such as abandoned underground mines. These sites pose significant challenges for conventional machinery and human operators due to confined spaces, structural instability, and lack of infrastructure. To address this, we propose a modular multi-robot system designed for autonomous operation in such environments, enabling sequential mineral extraction tasks. Unlike existing work that focuses primarily on mapping and inspection through global behavior or central control, our approach incorporates physical interaction capabilities using specialized robots coordinated through local high-level behavior control. Our proposed system utilizes Hierarchical Finite State Machine (HFSM) behaviors to structure complex task execution across heterogeneous robotic platforms. Each robot has its own HFSM behavior to perform sequential autonomy while maintaining overall system coordination, achieved by triggering behavior execution through inter-robot communication. This architecture effectively integrates software and hardware components to support collaborative, task-driven multi-robot operation in confined underground environments.
<div id='section'>Paperid: <span id='pid'>490, <a href='https://arxiv.org/pdf/2508.07720.pdf' target='_blank'>https://arxiv.org/pdf/2508.07720.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Themistoklis Charalambous, Nikolaos Pappas, Nikolaos Nomikos, Risto Wichman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.07720">Toward Goal-Oriented Communication in Multi-Agent Systems: An overview</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As multi-agent systems (MAS) become increasingly prevalent in autonomous systems, distributed control, and edge intelligence, efficient communication under resource constraints has emerged as a critical challenge. Traditional communication paradigms often emphasize message fidelity or bandwidth optimization, overlooking the task relevance of the exchanged information. In contrast, goal-oriented communication prioritizes the importance of information with respect to the agents' shared objectives. This review provides a comprehensive survey of goal-oriented communication in MAS, bridging perspectives from information theory, communication theory, and machine learning. We examine foundational concepts alongside learning-based approaches and emergent protocols. Special attention is given to coordination under communication constraints, as well as applications in domains such as swarm robotics, federated learning, and edge computing. The paper concludes with a discussion of open challenges and future research directions at the intersection of communication theory, machine learning, and multi-agent decision making.
<div id='section'>Paperid: <span id='pid'>491, <a href='https://arxiv.org/pdf/2508.02858.pdf' target='_blank'>https://arxiv.org/pdf/2508.02858.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianheng Zhu, Yiheng Feng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02858">MIDAR: Mimicking LiDAR Detection for Traffic Applications with a Lightweight Plug-and-Play Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As autonomous driving (AD) technology advances, increasing research has focused on leveraging cooperative perception (CP) data collected from multiple AVs to enhance traffic applications. Due to the impracticality of large-scale real-world AV deployments, simulation has become the primary approach in most studies. While game-engine-based simulators like CARLA generate high-fidelity raw sensor data (e.g., LiDAR point clouds) which can be used to produce realistic detection outputs, they face scalability challenges in multi-AV scenarios. In contrast, microscopic traffic simulators such as SUMO scale efficiently but lack perception modeling capabilities. To bridge this gap, we propose MIDAR, a LiDAR detection mimicking model that approximates realistic LiDAR detections using vehicle-level features readily available from microscopic traffic simulators. Specifically, MIDAR predicts true positives (TPs) and false negatives (FNs) from ideal LiDAR detection results based on the spatial layouts and dimensions of surrounding vehicles. A Refined Multi-hop Line-of-Sight (RM-LoS) graph is constructed to encode the occlusion relationships among vehicles, upon which MIDAR employs a GRU-enhanced APPNP architecture to propagate features from the ego AV and occluding vehicles to the prediction target. MIDAR achieves an AUC of 0.909 in approximating the detection results generated by CenterPoint, a mainstream 3D LiDAR detection model, on the nuScenes AD dataset. Two CP-based traffic applications further validate the necessity of such realistic detection modeling, particularly for tasks requiring accurate individual vehicle observations (e.g., position, speed, lane index). As demonstrated in the applications, MIDAR can be seamlessly integrated into traffic simulators and trajectory datasets and will be open-sourced upon publication.
<div id='section'>Paperid: <span id='pid'>492, <a href='https://arxiv.org/pdf/2508.01736.pdf' target='_blank'>https://arxiv.org/pdf/2508.01736.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tyrone Justin Sta Maria, Faith Griffin, Jordan Aiko Deja
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01736">Set the Stage: Enabling Storytelling with Multiple Robots through Roleplaying Metaphors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Gestures are an expressive input modality for controlling multiple robots, but their use is often limited by rigid mappings and recognition constraints. To move beyond these limitations, we propose roleplaying metaphors as a scaffold for designing richer interactions. By introducing three roles: Director, Puppeteer, and Wizard, we demonstrate how narrative framing can guide the creation of diverse gesture sets and interaction styles. These roles enable a variety of scenarios, showing how roleplay can unlock new possibilities for multi-robot systems. Our approach emphasizes creativity, expressiveness, and intuitiveness as key elements for future human-robot interaction design.
<div id='section'>Paperid: <span id='pid'>493, <a href='https://arxiv.org/pdf/2508.01062.pdf' target='_blank'>https://arxiv.org/pdf/2508.01062.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenyi Wang, Ruoyu Song, Raymond Muller, Jean-Philippe Monteuuis, Z. Berkay Celik, Jonathan Petit, Ryan Gerdes, Ming Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01062">CP-FREEZER: Latency Attacks against Vehicular Cooperative Perception</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cooperative perception (CP) enhances situational awareness of connected and autonomous vehicles by exchanging and combining messages from multiple agents. While prior work has explored adversarial integrity attacks that degrade perceptual accuracy, little is known about CP's robustness against attacks on timeliness (or availability), a safety-critical requirement for autonomous driving. In this paper, we present CP-FREEZER, the first latency attack that maximizes the computation delay of CP algorithms by injecting adversarial perturbation via V2V messages. Our attack resolves several unique challenges, including the non-differentiability of point cloud preprocessing, asynchronous knowledge of the victim's input due to transmission delays, and uses a novel loss function that effectively maximizes the execution time of the CP pipeline. Extensive experiments show that CP-FREEZER increases end-to-end CP latency by over $90\times$, pushing per-frame processing time beyond 3 seconds with a 100% success rate on our real-world vehicle testbed. Our findings reveal a critical threat to the availability of CP systems, highlighting the urgent need for robust defenses.
<div id='section'>Paperid: <span id='pid'>494, <a href='https://arxiv.org/pdf/2508.00467.pdf' target='_blank'>https://arxiv.org/pdf/2508.00467.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Samratul Fuady, Danesh Tarapore, Mohammad D. Soorati
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.00467">SubCDM: Collective Decision-Making with a Swarm Subset</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Collective decision-making is a key function of autonomous robot swarms, enabling them to reach a consensus on actions based on environmental features. Existing strategies require the participation of all robots in the decision-making process, which is resource-intensive and prevents the swarm from allocating the robots to any other tasks. We propose Subset-Based Collective Decision-Making (SubCDM), which enables decisions using only a swarm subset. The construction of the subset is dynamic and decentralized, relying solely on local information. Our method allows the swarm to adaptively determine the size of the subset for accurate decision-making, depending on the difficulty of reaching a consensus. Simulation results using one hundred robots show that our approach achieves accuracy comparable to using the entire swarm while reducing the number of robots required to perform collective decision-making, making it a resource-efficient solution for collective decision-making in swarm robotics.
<div id='section'>Paperid: <span id='pid'>495, <a href='https://arxiv.org/pdf/2507.06750.pdf' target='_blank'>https://arxiv.org/pdf/2507.06750.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tohid Kargar Tasooji, Ramviyas Parasuraman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06750">Distributed Fault-Tolerant Multi-Robot Cooperative Localization in Adversarial Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In multi-robot systems (MRS), cooperative localization is a crucial task for enhancing system robustness and scalability, especially in GPS-denied or communication-limited environments. However, adversarial attacks, such as sensor manipulation, and communication jamming, pose significant challenges to the performance of traditional localization methods. In this paper, we propose a novel distributed fault-tolerant cooperative localization framework to enhance resilience against sensor and communication disruptions in adversarial environments. We introduce an adaptive event-triggered communication strategy that dynamically adjusts communication thresholds based on real-time sensing and communication quality. This strategy ensures optimal performance even in the presence of sensor degradation or communication failure. Furthermore, we conduct a rigorous analysis of the convergence and stability properties of the proposed algorithm, demonstrating its resilience against bounded adversarial zones and maintaining accurate state estimation. Robotarium-based experiment results show that our proposed algorithm significantly outperforms traditional methods in terms of localization accuracy and communication efficiency, particularly in adversarial settings. Our approach offers improved scalability, reliability, and fault tolerance for MRS, making it suitable for large-scale deployments in real-world, challenging environments.
<div id='section'>Paperid: <span id='pid'>496, <a href='https://arxiv.org/pdf/2506.22223.pdf' target='_blank'>https://arxiv.org/pdf/2506.22223.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Felipe Valle Quiroz, Johan Elfing, Joel PÃ¥lsson, Elena Haller, Oscar Amador Molina
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.22223">V2X Intention Sharing for Cooperative Electrically Power-Assisted Cycles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces a novel intention-sharing mechanism for Electrically Power-Assisted Cycles (EPACs) within V2X communication frameworks, enhancing the ETSI VRU Awareness Message (VAM) protocol. The method replaces discrete predicted trajectory points with a compact elliptical geographical area representation derived via quadratic polynomial fitting and Least Squares Method (LSM). This approach encodes trajectory predictions with fixed-size data payloads, independent of the number of forecasted points, enabling higher-frequency transmissions and improved network reliability. Simulation results demonstrate superior inter-packet gap (IPG) performance compared to standard ETSI VAMs, particularly under constrained communication conditions. A physical experiment validates the feasibility of real-time deployment on embedded systems. The method supports scalable, low-latency intention sharing, contributing to cooperative perception and enhanced safety for vulnerable road users in connected and automated mobility ecosystems. Finally, we discuss the viability of LSM and open the door to other methods for prediction.
<div id='section'>Paperid: <span id='pid'>497, <a href='https://arxiv.org/pdf/2506.06612.pdf' target='_blank'>https://arxiv.org/pdf/2506.06612.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Akshaya Agrawal, Evan Palmer, Zachary Kingston, Geoffrey A. Hollinger
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.06612">Underwater Multi-Robot Simulation and Motion Planning in Angler</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deploying multi-robot systems in underwater environments is expensive and lengthy; testing algorithms and software in simulation improves development by decoupling software and hardware. However, this requires a simulation framework that closely resembles the real-world. Angler is an open-source framework that simulates low-level communication protocols for an onboard autopilot, such as ArduSub, providing a framework that is close to reality, but unfortunately lacking support for simulating multiple robots. We present an extension to Angler that supports multi-robot simulation and motion planning. Our extension has a modular architecture that creates non-conflicting communication channels between Gazebo, ArduSub Software-in-the-Loop (SITL), and MAVROS to operate multiple robots simultaneously in the same environment. Our multi-robot motion planning module interfaces with cascaded controllers via a JointTrajectory controller in ROS~2. We also provide an integration with the Open Motion Planning Library (OMPL), a collision avoidance module, and tools for procedural environment generation. Our work enables the development and benchmarking of underwater multi-robot motion planning in dynamic environments.
<div id='section'>Paperid: <span id='pid'>498, <a href='https://arxiv.org/pdf/2506.04881.pdf' target='_blank'>https://arxiv.org/pdf/2506.04881.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ioana Hustiu, Roozbeh Abolpour, Cristian Mahulea, Marius Kloetzer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.04881">Efficient Path Planning and Task Allocation Algorithm for Boolean Specifications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a novel path-planning and task assignment algorithm for multi-robot systems that should fulfill a global Boolean specification. The proposed method is based on Integer Linear Programming (ILP) formulations, which are combined with structural insights from Petri nets to improve scalability and computational efficiency. By proving that the \emph{constraint matrix} is totally unimodular (TU) for certain classes of problems, the ILP formulation can be relaxed into a Linear Programming (LP) problem without losing the integrality of the solution. This relaxation eliminates complex combinatorial techniques, significantly reducing computational overhead and thus ensuring scalability for large-scale systems. Using the approach proposed in this paper, we can solve path-planning problems for teams made up to 500 robots. The method guarantees computational tractability, handles collision avoidance and reduces computational demands through iterative LP optimization techniques. Case studies demonstrate the efficiency of the algorithm in generating scalable, collision-free paths for large robot teams navigating in complex environments. While the conservative nature of collision avoidance introduces additional constraints, and thus, computational requirements, the solution remains practical and impactful for diverse applications. The algorithm is particularly applicable to real-world scenarios, including warehouse logistics where autonomous robots must efficiently coordinate tasks or search-and-rescue operations in various environments. This work contributes both theoretically and practically to scalable multi-robot path planning and task allocation, offering an efficient framework for coordinating autonomous agents in shared environments.
<div id='section'>Paperid: <span id='pid'>499, <a href='https://arxiv.org/pdf/2505.10073.pdf' target='_blank'>https://arxiv.org/pdf/2505.10073.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rathin Chandra Shit, Sharmila Subudhi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.10073">Multi-Robot Task Allocation for Homogeneous Tasks with Collision Avoidance via Spatial Clustering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, a novel framework is presented that achieves a combined solution based on Multi-Robot Task Allocation (MRTA) and collision avoidance with respect to homogeneous measurement tasks taking place in industrial environments. The spatial clustering we propose offers to simultaneously solve the task allocation problem and deal with collision risks by cutting the workspace into distinguishable operational zones for each robot. To divide task sites and to schedule robot routes within corresponding clusters, we use K-means clustering and the 2-Opt algorithm. The presented framework shows satisfactory performance, where up to 93\% time reduction (1.24s against 17.62s) with a solution quality improvement of up to 7\% compared to the best performing method is demonstrated. Our method also completely eliminates collision points that persist in comparative methods in a most significant sense. Theoretical analysis agrees with the claim that spatial partitioning unifies the apparently disjoint tasks allocation and collision avoidance problems under conditions of many identical tasks to be distributed over sparse geographical areas. Ultimately, the findings in this work are of substantial importance for real world applications where both computational efficiency and operation free from collisions is of paramount importance.
<div id='section'>Paperid: <span id='pid'>500, <a href='https://arxiv.org/pdf/2505.08419.pdf' target='_blank'>https://arxiv.org/pdf/2505.08419.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ashish Verma, Avinash Gautam, Tanishq Duhan, V. S. Shekhawat, Sudeept Mohan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.08419">HMR-ODTA: Online Diverse Task Allocation for a Team of Heterogeneous Mobile Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Coordinating time-sensitive deliveries in environments like hospitals poses a complex challenge, particularly when managing multiple online pickup and delivery requests within strict time windows using a team of heterogeneous robots. Traditional approaches fail to address dynamic rescheduling or diverse service requirements, typically restricting robots to single-task types. This paper tackles the Multi-Pickup and Delivery Problem with Time Windows (MPDPTW), where autonomous mobile robots are capable of handling varied service requests. The objective is to minimize late delivery penalties while maximizing task completion rates. To achieve this, we propose a novel framework leveraging a heterogeneous robot team and an efficient dynamic scheduling algorithm that supports dynamic task rescheduling. Users submit requests with specific time constraints, and our decentralized algorithm, Heterogeneous Mobile Robots Online Diverse Task Allocation (HMR-ODTA), optimizes task assignments to ensure timely service while addressing delays or task rejections. Extensive simulations validate the algorithm's effectiveness. For smaller task sets (40-160 tasks), penalties were reduced by nearly 63%, while for larger sets (160-280 tasks), penalties decreased by approximately 50%. These results highlight the algorithm's effectiveness in improving task scheduling and coordination in multi-robot systems, offering a robust solution for enhancing delivery performance in structured, time-critical environments.
<div id='section'>Paperid: <span id='pid'>501, <a href='https://arxiv.org/pdf/2503.13813.pdf' target='_blank'>https://arxiv.org/pdf/2503.13813.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingming Peng, Zhendong Chen, Jie Yang, Jin Huang, Zhengqi Shi, Qihao Liu, Xinyu Li, Liang Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.13813">Automatic MILP Model Construction for Multi-Robot Task Allocation and Scheduling Based on Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the accelerated development of Industry 4.0, intelligent manufacturing systems increasingly require efficient task allocation and scheduling in multi-robot systems. However, existing methods rely on domain expertise and face challenges in adapting to dynamic production constraints. Additionally, enterprises have high privacy requirements for production scheduling data, which prevents the use of cloud-based large language models (LLMs) for solution development. To address these challenges, there is an urgent need for an automated modeling solution that meets data privacy requirements. This study proposes a knowledge-augmented mixed integer linear programming (MILP) automated formulation framework, integrating local LLMs with domain-specific knowledge bases to generate executable code from natural language descriptions automatically. The framework employs a knowledge-guided DeepSeek-R1-Distill-Qwen-32B model to extract complex spatiotemporal constraints (82% average accuracy) and leverages a supervised fine-tuned Qwen2.5-Coder-7B-Instruct model for efficient MILP code generation (90% average accuracy). Experimental results demonstrate that the framework successfully achieves automatic modeling in the aircraft skin manufacturing case while ensuring data privacy and computational efficiency. This research provides a low-barrier and highly reliable technical path for modeling in complex industrial scenarios.
<div id='section'>Paperid: <span id='pid'>502, <a href='https://arxiv.org/pdf/2503.06869.pdf' target='_blank'>https://arxiv.org/pdf/2503.06869.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kai Li, Zhao Ma, Liang Li, Shiyu Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.06869">Collective Behavior Clone with Visual Attention via Neural Interaction Graph Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose a framework, collective behavioral cloning (CBC), to learn the underlying interaction mechanism and control policy of a swarm system. Given the trajectory data of a swarm system, we propose a graph variational autoencoder (GVAE) to learn the local interaction graph. Based on the interaction graph and swarm trajectory, we use behavioral cloning to learn the control policy of the swarm system. To demonstrate the practicality of CBC, we deploy it on a real-world decentralized vision-based robot swarm system. A visual attention network is trained based on the learned interaction graph for online neighbor selection. Experimental results show that our method outperforms previous approaches in predicting both the interaction graph and swarm actions with higher accuracy. This work offers a promising approach for understanding interaction mechanisms and swarm dynamics in future swarm robotics research. Code and data are available.
<div id='section'>Paperid: <span id='pid'>503, <a href='https://arxiv.org/pdf/2502.16079.pdf' target='_blank'>https://arxiv.org/pdf/2502.16079.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aritra Pal, Anandsingh Chauhan, Mayank Baranwal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.16079">Together We Rise: Optimizing Real-Time Multi-Robot Task Allocation using Coordinated Heterogeneous Plays</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Efficient task allocation among multiple robots is crucial for optimizing productivity in modern warehouses, particularly in response to the increasing demands of online order fulfillment. This paper addresses the real-time multi-robot task allocation (MRTA) problem in dynamic warehouse environments, where tasks emerge with specified start and end locations. The objective is to minimize both the total travel distance of robots and delays in task completion, while also considering practical constraints such as battery management and collision avoidance. We introduce MRTAgent, a dual-agent Reinforcement Learning (RL) framework inspired by self-play, designed to optimize task assignments and robot selection to ensure timely task execution. For safe navigation, a modified linear quadratic controller (LQR) approach is employed. To the best of our knowledge, MRTAgent is the first framework to address all critical aspects of practical MRTA problems while supporting continuous robot movements.
<div id='section'>Paperid: <span id='pid'>504, <a href='https://arxiv.org/pdf/2502.10218.pdf' target='_blank'>https://arxiv.org/pdf/2502.10218.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pascal Goldschmid, Aamir Ahmad
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.10218">A Multi-Simulation Approach with Model Predictive Control for Anafi Drones</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Simulation frameworks are essential for the safe development of robotic applications. However, different components of a robotic system are often best simulated in different environments, making full integration challenging. This is particularly true for partially-open or closed-source simulators, which commonly suffer from two limitations: (i) lack of runtime control over scene actors via interfaces like ROS, and (ii) restricted access to real-time state data (e.g., pose, velocity) of scene objects. In the first part of this work, we address these issues by integrating aerial drones simulated in Parrot's Sphinx environment (used for Anafi drones) into the Gazebo simulator. Our approach uses a mirrored drone instance embedded within Gazebo environments to bridge the two simulators. One key application is aerial target tracking, a common task in multi-robot systems. However, Parrot's default PID-based controller lacks the agility needed for tracking fast-moving targets. To overcome this, in the second part of this work we develop a model predictive controller (MPC) that leverages cumulative error states to improve tracking accuracy. Our MPC significantly outperforms the built-in PID controller in dynamic scenarios, increasing the effectiveness of the overall system. We validate our integrated framework by incorporating the Anafi drone into an existing Gazebo-based airship simulation and rigorously test the MPC against a custom PID baseline in both simulated and real-world experiments.
<div id='section'>Paperid: <span id='pid'>505, <a href='https://arxiv.org/pdf/2501.01200.pdf' target='_blank'>https://arxiv.org/pdf/2501.01200.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tengfei Lyu, Md Noor-A-Rahim, Dirk Pesch, Aisling O'Driscoll
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.01200">A Survey and Tutorial of Redundancy Mitigation for Vehicular Cooperative Perception: Standards, Strategies and Open Issues</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper provides an in-depth review and discussion of the state of the art in redundancy mitigation for the vehicular Collective Perception Service (CPS). We focus on the evolutionary differences between the redundancy mitigation rules proposed in 2019 in ETSI TR 103 562 versus the 2023 technical specification ETSI TS 103 324, which uses a Value of Information (VoI) based mitigation approach. We also critically analyse the academic literature that has sought to quantify the communication challenges posed by the CPS and present a unique taxonomy of the redundancy mitigation approaches proposed using three distinct classifications: object inclusion filtering, data format optimisation, and frequency management. Finally, this paper identifies open research challenges that must be adequately investigated to satisfactorily deploy CPS redundancy mitigation measures. Our critical and comprehensive evaluation serves as a point of reference for those undertaking research in this area.
<div id='section'>Paperid: <span id='pid'>506, <a href='https://arxiv.org/pdf/2412.20397.pdf' target='_blank'>https://arxiv.org/pdf/2412.20397.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lucas C. D. Bezerra, AtaÃ­de M. G. dos Santos, Shinkyu Park
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.20397">Learning Policies for Dynamic Coalition Formation in Multi-Robot Task Allocation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a decentralized, learning-based framework for dynamic coalition formation in Multi-Robot Task Allocation (MRTA). Our approach extends MAPPO by integrating spatial action maps, robot motion planning, intention sharing, and task allocation revision to enable effective and adaptive coalition formation. Extensive simulation studies confirm the effectiveness of our model, enabling each robot to rely solely on local information to learn timely revisions of task selections and form coalitions with other robots to complete collaborative tasks. The results also highlight the proposed framework's ability to handle large robot populations and adapt to scenarios with diverse task sets.
<div id='section'>Paperid: <span id='pid'>507, <a href='https://arxiv.org/pdf/2412.14793.pdf' target='_blank'>https://arxiv.org/pdf/2412.14793.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Atharva Sagale, Tohid Kargar Tasooji, Ramviyas Parasuraman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.14793">DCL-Sparse: Distributed Range-only Cooperative Localization of Multi-Robots in Noisy and Sparse Sensing Graphs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a novel approach to range-based cooperative localization for robot swarms in GPS-denied environments, addressing the limitations of current methods in noisy and sparse settings. We propose a robust multi-layered localization framework that combines shadow edge localization techniques with the strategic deployment of UAVs. This approach not only addresses the challenges associated with nonrigid and poorly connected graphs but also enhances the convergence rate of the localization process. We introduce two key concepts: the S1-Edge approach in our distributed protocol to address the rigidity problem of sparse graphs and the concept of a powerful UAV node to increase the sensing and localization capability of the multi-robot system. Our approach leverages the advantages of the distributed localization methods, enhancing scalability and adaptability in large robot networks. We establish theoretical conditions for the new S1-Edge that ensure solutions exist even in the presence of noise, thereby validating the effectiveness of shadow edge localization. Extensive simulation experiments confirm the superior performance of our method compared to state-of-the-art techniques, resulting in up to 95\% reduction in localization error, demonstrating substantial improvements in localization accuracy and robustness to sparse graphs. This work provides a decisive advancement in the field of multi-robot localization, offering a powerful tool for high-performance and reliable operations in challenging environments.
<div id='section'>Paperid: <span id='pid'>508, <a href='https://arxiv.org/pdf/2411.02524.pdf' target='_blank'>https://arxiv.org/pdf/2411.02524.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sai Krishna Ghanta, Ramviyas Parasuraman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.02524">SPACE: 3D Spatial Co-operation and Exploration Framework for Robust Mapping and Coverage with Multi-Robot Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In indoor environments, multi-robot visual (RGB-D) mapping and exploration hold immense potential for application in domains such as domestic service and logistics, where deploying multiple robots in the same environment can significantly enhance efficiency. However, there are two primary challenges: (1) the "ghosting trail" effect, which occurs due to overlapping views of robots impacting the accuracy and quality of point cloud reconstruction, and (2) the oversight of visual reconstructions in selecting the most effective frontiers for exploration. Given these challenges are interrelated, we address them together by proposing a new semi-distributed framework (SPACE) for spatial cooperation in indoor environments that enables enhanced coverage and 3D mapping. SPACE leverages geometric techniques, including "mutual awareness" and a "dynamic robot filter," to overcome spatial mapping constraints. Additionally, we introduce a novel spatial frontier detection system and map merger, integrated with an adaptive frontier assigner for optimal coverage balancing the exploration and reconstruction objectives. In extensive ROS-Gazebo simulations, SPACE demonstrated superior performance over state-of-the-art approaches in both exploration and mapping metrics.
<div id='section'>Paperid: <span id='pid'>509, <a href='https://arxiv.org/pdf/2411.02230.pdf' target='_blank'>https://arxiv.org/pdf/2411.02230.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aiman Munir, Ayan Dutta, Ramviyas Parasuraman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.02230">Energy-Aware Coverage Planning for Heterogeneous Multi-Robot System</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a distributed control law for a heterogeneous multi-robot coverage problem, where the robots could have different energy characteristics, such as capacity and depletion rates, due to their varying sizes, speeds, capabilities, and payloads. Existing energy-aware coverage control laws consider capacity differences but assume the battery depletion rate to be the same for all robots. In realistic scenarios, however, some robots can consume energy much faster than other robots; for instance, UAVs hover at different altitudes, and these changes could be dynamically updated based on their assigned tasks. Robots' energy capacities and depletion rates need to be considered to maximize the performance of a multi-robot system. To this end, we propose a new energy-aware controller based on Lloyd's algorithm to adapt the weights of the robots based on their energy dynamics and divide the area of interest among the robots accordingly. The controller is theoretically analyzed and extensively evaluated through simulations and real-world demonstrations in multiple realistic scenarios and compared with three baseline control laws to validate its performance and efficacy.
<div id='section'>Paperid: <span id='pid'>510, <a href='https://arxiv.org/pdf/2411.02062.pdf' target='_blank'>https://arxiv.org/pdf/2411.02062.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alvaro Calvo, Jesus Capitan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.02062">Heterogeneous Multi-robot Task Allocation for Long-Endurance Missions in Dynamic Scenarios</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a framework for Multi-Robot Task Allocation (MRTA) in heterogeneous teams performing long-endurance missions in dynamic scenarios. Given the limited battery of robots, especially for aerial vehicles, we allow for robot recharges and the possibility of fragmenting and/or relaying certain tasks. We also address tasks that must be performed by a coalition of robots in a coordinated manner. Given these features, we introduce a new class of heterogeneous MRTA problems which we analyze theoretically and optimally formulate as a Mixed-Integer Linear Program. We then contribute a heuristic algorithm to compute approximate solutions and integrate it into a mission planning and execution architecture capable of reacting to unexpected events by repairing or recomputing plans online. Our experimental results show the relevance of our newly formulated problem in a realistic use case for inspection with aerial robots. We assess the performance of our heuristic solver in comparison with other variants and with exact optimal solutions in small-scale scenarios. In addition, we evaluate the ability of our replanning framework to repair plans online.
<div id='section'>Paperid: <span id='pid'>511, <a href='https://arxiv.org/pdf/2410.23128.pdf' target='_blank'>https://arxiv.org/pdf/2410.23128.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Di Ni, Hungtang Ko, Radhika Nagpal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.23128">Leader-Follower 3D Formation for Underwater Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The schooling behavior of fish is hypothesized to confer many survival benefits, including foraging success, safety from predators, and energy savings through hydrodynamic interactions when swimming in formation. Underwater robot collectives may be able to achieve similar benefits in future applications, e.g. using formation control to achieve efficient spatial sampling for environmental monitoring. Although many theoretical algorithms exist for multi-robot formation control, they have not been tested in the underwater domain due to the fundamental challenges in underwater communication. Here we introduce a leader-follower strategy for underwater formation control that allows us to realize complex 3D formations, using purely vision-based perception and a reactive control algorithm that is low computation. We use a physical platform, BlueSwarm, to demonstrate for the first time an experimental realization of inline, side-by-side, and staggered swimming 3D formations. More complex formations are studied in a physics-based simulator, providing new insights into the convergence and stability of formations given underwater inertial/drag conditions. Our findings lay the groundwork for future applications of underwater robot swarms in aquatic environments with minimal communication.
<div id='section'>Paperid: <span id='pid'>512, <a href='https://arxiv.org/pdf/2410.00582.pdf' target='_blank'>https://arxiv.org/pdf/2410.00582.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pengxi Zeng, Alberto Presta, Jonah Reinis, Dinesh Bharadia, Hang Qiu, Pamela Cosman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.00582">Can We Remove the Ground? Obstacle-aware Point Cloud Compression for Remote Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Efficient point cloud (PC) compression is crucial for streaming applications, such as augmented reality and cooperative perception. Classic PC compression techniques encode all the points in a frame. Tailoring compression towards perception tasks at the receiver side, we ask the question, "Can we remove the ground points during transmission without sacrificing the detection performance?" Our study reveals a strong dependency on the ground from state-of-the-art (SOTA) 3D object detection models, especially on those points below and around the object. In this work, we propose a lightweight obstacle-aware Pillar-based Ground Removal (PGR) algorithm. PGR filters out ground points that do not provide context to object recognition, significantly improving compression ratio without sacrificing the receiver side perception performance. Not using heavy object detection or semantic segmentation models, PGR is light-weight, highly parallelizable, and effective. Our evaluations on KITTI and Waymo Open Dataset show that SOTA detection models work equally well with PGR removing 20-30% of the points, with a speeding of 86 FPS.
<div id='section'>Paperid: <span id='pid'>513, <a href='https://arxiv.org/pdf/2410.00122.pdf' target='_blank'>https://arxiv.org/pdf/2410.00122.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zachary Fuge, Benjamin Beiter, Alexander Leonessa
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.00122">Additively Manufactured Open-Source Quadruped Robots for Multi-Robot SLAM Applications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work presents the design and development of the quadruped robot Squeaky to be used as a research and learning platform for single and multi-SLAM robotics, computer vision, and reinforcement learning. Affordable robots are becoming necessary when expanding from single to multi-robot applications, as the cost can increase exponentially as fleet size increases. SLAM is essential for a robot to perceive and localize within its environment to perform applications such as cave exploration, disaster assistance, and remote inspection. For improved efficiency, a fleet of robots can be employed to combine maps for multi-robot SLAM. Squeaky is an affordable quadrupedal robot, designed to have easily adaptable hardware and software, capable of creating a merged map under a shared network from multiple robots, and available open-source for the benefit of the research community.
<div id='section'>Paperid: <span id='pid'>514, <a href='https://arxiv.org/pdf/2409.16577.pdf' target='_blank'>https://arxiv.org/pdf/2409.16577.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chao Huang, Wenshuo Zang, Carlo Pinciroli, Zhi Jane Li, Taposh Banerjee, Lili Su, Rui Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.16577">Reactive Multi-Robot Navigation in Outdoor Environments Through Uncertainty-Aware Active Learning of Human Preference Landscape</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Compared with single robots, Multi-Robot Systems (MRS) can perform missions more efficiently due to the presence of multiple members with diverse capabilities. However, deploying an MRS in wide real-world environments is still challenging due to uncertain and various obstacles (e.g., building clusters and trees). With a limited understanding of environmental uncertainty on performance, an MRS cannot flexibly adjust its behaviors (e.g., teaming, load sharing, trajectory planning) to ensure both environment adaptation and task accomplishments. In this work, a novel joint preference landscape learning and behavior adjusting framework (PLBA) is designed. PLBA efficiently integrates real-time human guidance to MRS coordination and utilizes Sparse Variational Gaussian Processes with Varying Output Noise to quickly assess human preferences by leveraging spatial correlations between environment characteristics. An optimization-based behavior-adjusting method then safely adapts MRS behaviors to environments. To validate PLBA's effectiveness in MRS behavior adaption, a flood disaster search and rescue task was designed. 20 human users provided 1764 feedback based on human preferences obtained from MRS behaviors related to "task quality", "task progress", "robot safety". The prediction accuracy and adaptation speed results show the effectiveness of PLBA in preference learning and MRS behavior adaption.
<div id='section'>Paperid: <span id='pid'>515, <a href='https://arxiv.org/pdf/2408.11822.pdf' target='_blank'>https://arxiv.org/pdf/2408.11822.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bin Wu, C Steve Suh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.11822">State-of-the-art in Robot Learning for Multi-Robot Collaboration: A Comprehensive Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the continuous breakthroughs in core technology, the dawn of large-scale integration of robotic systems into daily human life is on the horizon. Multi-robot systems (MRS) built on this foundation are undergoing drastic evolution. The fusion of artificial intelligence technology with robot hardware is seeing broad application possibilities for MRS. This article surveys the state-of-the-art of robot learning in the context of Multi-Robot Cooperation (MRC) of recent. Commonly adopted robot learning methods (or frameworks) that are inspired by humans and animals are reviewed and their advantages and disadvantages are discussed along with the associated technical challenges. The potential trends of robot learning and MRS integration exploiting the merging of these methods with real-world applications is also discussed at length. Specifically statistical methods are used to quantitatively corroborate the ideas elaborated in the article.
<div id='section'>Paperid: <span id='pid'>516, <a href='https://arxiv.org/pdf/2408.11339.pdf' target='_blank'>https://arxiv.org/pdf/2408.11339.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bin Wu, C Steve Suh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.11339">Deep Reinforcement Learning for Decentralized Multi-Robot Control: A DQN Approach to Robustness and Information Integration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The superiority of Multi-Robot Systems (MRS) in various complex environments is unquestionable. However, in complex situations such as search and rescue, environmental monitoring, and automated production, robots are often required to work collaboratively without a central control unit. This necessitates an efficient and robust decentralized control mechanism to process local information and guide the robots' behavior. In this work, we propose a new decentralized controller design method that utilizes the Deep Q-Network (DQN) algorithm from deep reinforcement learning, aimed at improving the integration of local information and robustness of multi-robot systems. The designed controller allows each robot to make decisions independently based on its local observations while enhancing the overall system's collaborative efficiency and adaptability to dynamic environments through a shared learning mechanism. Through testing in simulated environments, we have demonstrated the effectiveness of this controller in improving task execution efficiency, strengthening system fault tolerance, and enhancing adaptability to the environment. Furthermore, we explored the impact of DQN parameter tuning on system performance, providing insights for further optimization of the controller design. Our research not only showcases the potential application of the DQN algorithm in the decentralized control of multi-robot systems but also offers a new perspective on how to enhance the overall performance and robustness of the system through the integration of local information.
<div id='section'>Paperid: <span id='pid'>517, <a href='https://arxiv.org/pdf/2407.04840.pdf' target='_blank'>https://arxiv.org/pdf/2407.04840.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weihang Tan, Timothy Anglea, Yongqiang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.04840">Analysis of Dead Reckoning Accuracy in Swarm Robotics System</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The objective of this paper is to determine the position of a single mobile robot in a swarm using dead reckoning techniques. We investigate the accuracy of navigation by using this process. The paper begins with the research background and social importance. Then, the specific experimental setup and analysis of experimental results are presented. Finally, the results are detailed and some potential improvements are provided.
<div id='section'>Paperid: <span id='pid'>518, <a href='https://arxiv.org/pdf/2406.13707.pdf' target='_blank'>https://arxiv.org/pdf/2406.13707.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vishrut Bohara, Siavash Farzan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.13707">Safety-Critical Formation Control of Non-Holonomic Multi-Robot Systems in Communication-Limited Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces a decentralized estimator-based safety-critical controller designed for formation control of non-holonomic mobile robots operating in communication-constrained environments. The proposed framework integrates a robust state estimator capable of accurately reconstructing neighboring agents' velocity vectors and orientations under varying dynamic conditions, with a decentralized formation tracking controller that leverages Control Barrier Functions (CBFs) to guarantee collision avoidance and inter-agent safety. We present a closed-form control law that ensures both stability and string stability, effectively attenuating disturbances propagating from leader to followers. The theoretical foundations of the estimator and controller are established using Lyapunov stability analysis, which confirms global asymptotic stability under constant velocities and global uniformly ultimate boundedness under time-varying conditions. Extensive numerical simulations and realistic Gazebo-based experiments validate the effectiveness, robustness, and practical applicability of the proposed method, demonstrating precise formation tracking, stringent safety maintenance, and disturbance resilience without relying on inter-robot communication.
<div id='section'>Paperid: <span id='pid'>519, <a href='https://arxiv.org/pdf/2405.20587.pdf' target='_blank'>https://arxiv.org/pdf/2405.20587.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Amr M. Zaki, Sara A. Elsayed, Khalid Elgazzar, Hossam S. Hassanein
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.20587">Quality-Aware Task Offloading for Cooperative Perception in Vehicular Edge Computing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Task offloading in Vehicular Edge Computing (VEC) can advance cooperative perception (CP) to improve traffic awareness in Autonomous Vehicles. In this paper, we propose the Quality-aware Cooperative Perception Task Offloading (QCPTO) scheme. Q-CPTO is the first task offloading scheme that enhances traffic awareness by prioritizing the quality rather than the quantity of cooperative perception. Q-CPTO improves the quality of CP by curtailing perception redundancy and increasing the Value of Information (VOI) procured by each user. We use Kalman filters (KFs) for VOI assessment, predicting the next movement of each vehicle to estimate its region of interest. The estimated VOI is then integrated into the task offloading problem. We formulate the task offloading problem as an Integer Linear Program (ILP) that maximizes the VOI of users and reduces perception redundancy by leveraging the spatially diverse fields of view (FOVs) of vehicles, while adhering to strict latency requirements. We also propose the Q-CPTO-Heuristic (Q-CPTOH) scheme to solve the task offloading problem in a time-efficient manner. Extensive evaluations show that Q-CPTO significantly outperforms prominent task offloading schemes by up to 14% and 20% in terms of response delay and traffic awareness, respectively. Furthermore, Q-CPTO-H closely approaches the optimal solution, with marginal gaps of up to 1.4% and 2.1% in terms of traffic awareness and the number of collaborating users, respectively, while reducing the runtime by up to 84%.
<div id='section'>Paperid: <span id='pid'>520, <a href='https://arxiv.org/pdf/2404.11817.pdf' target='_blank'>https://arxiv.org/pdf/2404.11817.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuma Shida, Tomohiko Jimbo, Tadashi Odashima, Takamitsu Matsubara
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.11817">Reinforcement Learning of Multi-robot Task Allocation for Multi-object Transportation with Infeasible Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object transport using multi-robot systems has the potential for diverse practical applications such as delivery services owing to its efficient individual and scalable cooperative transport. However, allocating transportation tasks of objects with unknown weights remains challenging. Moreover, the presence of infeasible tasks (untransportable objects) can lead to robot stoppage (deadlock). This paper proposes a framework for dynamic task allocation that involves storing task experiences for each task in a scalable manner with respect to the number of robots. First, these experiences are broadcasted from the cloud server to the entire robot system. Subsequently, each robot learns the exclusion levels for each task based on those task experiences, enabling it to exclude infeasible tasks and reset its task priorities. Finally, individual transportation, cooperative transportation, and the temporary exclusion of tasks considered infeasible are achieved. The scalability and versatility of the proposed method were confirmed through numerical experiments with an increased number of robots and objects, including unlearned weight objects. The effectiveness of the temporary deadlock avoidance was also confirmed by introducing additional robots within an episode. The proposed method enables the implementation of task allocation strategies that are feasible for different numbers of robots and various transport tasks without prior consideration of feasibility.
<div id='section'>Paperid: <span id='pid'>521, <a href='https://arxiv.org/pdf/2404.02010.pdf' target='_blank'>https://arxiv.org/pdf/2404.02010.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nicky Zimmerman, Alessandro Giusti, JÃ©rÃ´me Guzzi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.02010">Resource-Aware Collaborative Monte Carlo Localization with Distribution Compression</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Global localization is essential in enabling robot autonomy, and collaborative localization is key for multi-robot systems. In this paper, we address the task of collaborative global localization under computational and communication constraints. We propose a method which reduces the amount of information exchanged and the computational cost. We also analyze, implement and open-source seminal approaches, which we believe to be a valuable contribution to the community. We exploit techniques for distribution compression in near-linear time, with error guarantees. We evaluate our approach and the implemented baselines on multiple challenging scenarios, simulated and real-world. Our approach can run online on an onboard computer. We release an open-source C++/ROS2 implementation of our approach, as well as the baselines
<div id='section'>Paperid: <span id='pid'>522, <a href='https://arxiv.org/pdf/2403.13348.pdf' target='_blank'>https://arxiv.org/pdf/2403.13348.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weiying Wang, Victor Cai, Stephanie Gil
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.13348">MULAN-WC: Multi-Robot Localization Uncertainty-aware Active NeRF with Wireless Coordination</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents MULAN-WC, a novel multi-robot 3D reconstruction framework that leverages wireless signal-based coordination between robots and Neural Radiance Fields (NeRF). Our approach addresses key challenges in multi-robot 3D reconstruction, including inter-robot pose estimation, localization uncertainty quantification, and active best-next-view selection. We introduce a method for using wireless Angle-of-Arrival (AoA) and ranging measurements to estimate relative poses between robots, as well as quantifying and incorporating the uncertainty embedded in the wireless localization of these pose estimates into the NeRF training loss to mitigate the impact of inaccurate camera poses. Furthermore, we propose an active view selection approach that accounts for robot pose uncertainty when determining the next-best views to improve the 3D reconstruction, enabling faster convergence through intelligent view selection. Extensive experiments on both synthetic and real-world datasets demonstrate the effectiveness of our framework in theory and in practice. Leveraging wireless coordination and localization uncertainty-aware training, MULAN-WC can achieve high-quality 3d reconstruction which is close to applying the ground truth camera poses. Furthermore, the quantification of the information gain from a novel view enables consistent rendering quality improvement with incrementally captured images by commending the robot the novel view position. Our hardware experiments showcase the practicality of deploying MULAN-WC to real robotic systems.
<div id='section'>Paperid: <span id='pid'>523, <a href='https://arxiv.org/pdf/2403.10460.pdf' target='_blank'>https://arxiv.org/pdf/2403.10460.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ratijit Mitra, Indranil Saha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.10460">Online Concurrent Multi-Robot Coverage Path Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, centralized receding horizon online multi-robot coverage path planning algorithms have shown remarkable scalability in thoroughly exploring large, complex, unknown workspaces with many robots. In a horizon, the path planning and the path execution interleave, meaning when the path planning occurs for robots with no paths, the robots with outstanding paths do not execute, and subsequently, when the robots with new or outstanding paths execute to reach respective goals, path planning does not occur for those robots yet to get new paths, leading to wastage of both the robotic and the computation resources. As a remedy, we propose a centralized algorithm that is not horizon-based. It plans paths at any time for a subset of robots with no paths, i.e., who have reached their previously assigned goals, while the rest execute their outstanding paths, thereby enabling concurrent planning and execution. We formally prove that the proposed algorithm ensures complete coverage of an unknown workspace and analyze its time complexity. To demonstrate scalability, we evaluate our algorithm to cover eight large $2$D grid benchmark workspaces with up to 512 aerial and ground robots, respectively. A comparison with a state-of-the-art horizon-based algorithm shows its superiority in completing the coverage with up to 1.6x speedup. For validation, we perform ROS + Gazebo simulations in six 2D grid benchmark workspaces with 10 quadcopters and TurtleBots, respectively. We also successfully conducted one outdoor experiment with three quadcopters and one indoor with two TurtleBots.
<div id='section'>Paperid: <span id='pid'>524, <a href='https://arxiv.org/pdf/2402.10505.pdf' target='_blank'>https://arxiv.org/pdf/2402.10505.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zirui Liao, Jian Shi, Yuwei Zhang, Shaoping Wang, Zhiyong Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.10505">A Survey of Resilient Coordination for Cyber-Physical Systems Against Malicious Attacks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cyber-physical systems (CPSs) facilitate the integration of physical entities and cyber infrastructures through the utilization of pervasive computational resources and communication units, leading to improved efficiency, automation, and practical viability in both academia and industry. Due to its openness and distributed characteristics, a critical issue prevalent in CPSs is to guarantee resilience in presence of malicious attacks. This paper conducts a comprehensive survey of recent advances on resilient coordination for CPSs. Different from existing survey papers, we focus on the node injection attack and propose a novel taxonomy according to the multi-layered framework of CPS. Furthermore, miscellaneous resilient coordination problems are discussed in this survey. Specifically, some preliminaries and the fundamental problem settings are given at the beginning. Subsequently, based on a multi-layered framework of CPSs, promising results of resilient consensus are classified and reviewed from three perspectives: physical structure, communication mechanism, and network topology. Next, two typical application scenarios, i.e., multi-robot systems and smart grids are exemplified to extend resilient consensus to other coordination tasks. Particularly, we examine resilient containment and resilient distributed optimization problems, both of which demonstrate the applicability of resilient coordination approaches. Finally, potential avenues are highlighted for future research.
<div id='section'>Paperid: <span id='pid'>525, <a href='https://arxiv.org/pdf/2402.08566.pdf' target='_blank'>https://arxiv.org/pdf/2402.08566.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Syed S. Ahmed, Mohammed A. Shalaby, Charles C. Cossette, Jerome Le Ny, James R. Forbes
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.08566">Gaussian-Sum Filter for Range-based 3D Relative Pose Estimation in the Presence of Ambiguities</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-robot systems must have the ability to accurately estimate relative states between robots in order to perform collaborative tasks, possibly with no external aiding. Three-dimensional relative pose estimation using range measurements oftentimes suffers from a finite number of non-unique solutions, or ambiguities. This paper: 1) identifies and accurately estimates all possible ambiguities in 2D; 2) treats them as components of a Gaussian mixture model; and 3) presents a computationally-efficient estimator, in the form of a Gaussian-sum filter (GSF), to realize range-based relative pose estimation in an infrastructure-free, 3D, setup. This estimator is evaluated in simulation and experiment and is shown to avoid divergence to local minima induced by the ambiguous poses. Furthermore, the proposed GSF outperforms an extended Kalman filter, demonstrates similar performance to the computationally-demanding particle filter, and is shown to be consistent.
<div id='section'>Paperid: <span id='pid'>526, <a href='https://arxiv.org/pdf/2401.15313.pdf' target='_blank'>https://arxiv.org/pdf/2401.15313.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kihoon Shin, Hyunjae Sim, Seungwon Nam, Yonghee Kim, Jae Hu, Kwang-Ki K. Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.15313">Multi-Robot Relative Pose Estimation in SE(2) with Observability Analysis: A Comparison of Extended Kalman Filtering and Robust Pose Graph Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this study, we address multi-robot localization issues, with a specific focus on cooperative localization and observability analysis of relative pose estimation. Cooperative localization involves enhancing each robot's information through a communication network and message passing. If odometry data from a target robot can be transmitted to the ego robot, observability of their relative pose estimation can be achieved through range-only or bearing-only measurements, provided both robots have non-zero linear velocities. In cases where odometry data from a target robot are not directly transmitted but estimated by the ego robot, both range and bearing measurements are necessary to ensure observability of relative pose estimation. For ROS/Gazebo simulations, we explore four sensing and communication structures. We compare extended Kalman filtering (EKF) and pose graph optimization (PGO) estimation using different robust loss functions (filtering and smoothing with varying batch sizes of sliding windows) in terms of estimation accuracy. In hardware experiments, two Turtlebot3 equipped with UWB modules are used for real-world inter-robot relative pose estimation, applying both EKF and PGO and comparing their performance.
<div id='section'>Paperid: <span id='pid'>527, <a href='https://arxiv.org/pdf/2401.11634.pdf' target='_blank'>https://arxiv.org/pdf/2401.11634.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hussein Ali Jaafar, Cheng-Hao Kao, Sajad Saeedi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.11634">MR.CAP: Multi-Robot Joint Control and Planning for Object Transport</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the recent influx in demand for multi-robot systems throughout industry and academia, there is an increasing need for faster, robust, and generalizable path planning algorithms. Similarly, given the inherent connection between control algorithms and multi-robot path planners, there is in turn an increased demand for fast, efficient, and robust controllers. We propose a scalable joint path planning and control algorithm for multi-robot systems with constrained behaviours based on factor graph optimization. We demonstrate our algorithm on a series of hardware and simulated experiments. Our algorithm is consistently able to recover from disturbances and avoid obstacles while outperforming state-of-the-art methods in optimization time, path deviation, and inter-robot errors. See the code and supplementary video for experiments.
<div id='section'>Paperid: <span id='pid'>528, <a href='https://arxiv.org/pdf/2312.17731.pdf' target='_blank'>https://arxiv.org/pdf/2312.17731.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andrew Fishberg, Brian Quiter, Jonathan P. How
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.17731">MURP: Multi-Agent Ultra-Wideband Relative Pose Estimation with Constrained Communications in 3D Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Inter-agent relative localization is critical for many multi-robot systems operating in the absence of external positioning infrastructure or prior environmental knowledge. We propose a novel inter-agent relative 3D pose estimation system where each participating agent is equipped with several ultra-wideband (UWB) ranging tags. Prior work typically supplements noisy UWB range measurements with additional continuously transmitted data (e.g., odometry) leading to potential scaling issues with increased team size and/or decreased communication network capability. By equipping each agent with multiple UWB antennas, our approach addresses these concerns by using only locally collected UWB range measurements, a priori state constraints, and event-based detections of when said constraints are violated. The addition of our learned mean ranging bias correction improves our approach by an additional 19% positional error, and gives us an overall experimental mean absolute position and heading errors of 0.24m and 9.5 degrees respectively. When compared to other state-of-the-art approaches, our work demonstrates improved performance over similar systems, while remaining competitive with methods that have significantly higher communication costs. Additionally, we make our datasets available.
<div id='section'>Paperid: <span id='pid'>529, <a href='https://arxiv.org/pdf/2312.12639.pdf' target='_blank'>https://arxiv.org/pdf/2312.12639.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zachary R. Madin, Jonathan Lawry, Edmund R. Hunt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.12639">Collective Anomaly Perception During Multi-Robot Patrol: Constrained Interactions Can Promote Accurate Consensus</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>An important real-world application of multi-robot systems is multi-robot patrolling (MRP), where robots must carry out the activity of going through an area at regular intervals. Motivations for MRP include the detection of anomalies that may represent security threats. While MRP algorithms show some maturity in development, a key potential advantage has been unexamined: the ability to exploit collective perception of detected anomalies to prioritize the location ordering of security checks. This is because noisy individual-level detection of an anomaly may be compensated for by group-level consensus formation regarding whether an anomaly is likely to be truly present. Here, we examine the performance of unmodified idleness-based patrolling algorithms when given the additional objective of reaching an environmental perception consensus via local pairwise communication and a quorum threshold. We find that generally, MRP algorithms that promote physical mixing of robots, as measured by a higher connectivity of their emergent communication network, reach consensus more quickly. However, when there is noise present in anomaly detection, a more moderate (constrained) level of connectivity is preferable because it reduces the spread of false positive detections, as measured by a group-level F-score. These findings can inform user choice of MRP algorithm and future algorithm development.
<div id='section'>Paperid: <span id='pid'>530, <a href='https://arxiv.org/pdf/2312.05428.pdf' target='_blank'>https://arxiv.org/pdf/2312.05428.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanran Wang, Michael J. Banks, Igor Mezic, Takashi Hikihara
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.05428">Trajectory Estimation in Unknown Nonlinear Manifold Using Koopman Operator Theory</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Formation coordination is a critical aspect of swarm robotics, which involves coordinating the motion and behavior of a group of robots to achieve a specific objective. In formation coordination, the robots must maintain a specific spatial arrangement while in motion. In this paper, we present a leader-follower column formation coordination problem in an unknown, two-dimensional nonlinear manifold, where we redefining it as a trajectory estimation problem. Leveraging Koopman operator theory and Extended Dynamic Mode Decomposition, we estimate the measurement vectors for the follower agent and guide its nonlinear trajectories.
<div id='section'>Paperid: <span id='pid'>531, <a href='https://arxiv.org/pdf/2312.01358.pdf' target='_blank'>https://arxiv.org/pdf/2312.01358.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Eduard Heiss, Andrey Kozyr, Oleg Morozov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.01358">Formations organization in robotic swarm using the thermal motion equivalent method</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Due to its decentralised, distributed and scalable nature, swarm robotics has great potential for applications ranging from agriculture to environmental monitoring and logistics. Various swarm control methods and algorithms are currently known, such as virtual leader, vector and potential field, and others. Such methods often show good results in specific conditions and tasks. The variety of tasks solved by the swarm requires the development of a universal control algorithm. In this paper, we propose an evolution of a thermal motion equivalent method (TMEM) inspired by the behavioural similarity of thermodynamic interactions between molecules. Previous research has shown the high efficiency of such a method for terrain monitoring tasks. This work addresses the problem of swarm formation of geometric structures, as required for logistics and formation movement tasks. It is shown that the formation of swarm geometric structures using the TMEM is possible with a special nonlinear interaction function of the agents. A piecewise linear interaction function is proposed that allows the formation of a stable group of agents. The results of the paper are validated by numerical modelling of the swarm dynamics. A linear quadrocopter model is considered as an agent. The fairness of the choice of the interaction function is shown.
<div id='section'>Paperid: <span id='pid'>532, <a href='https://arxiv.org/pdf/2311.12580.pdf' target='_blank'>https://arxiv.org/pdf/2311.12580.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Young-Hee Lee, Chen Zhu, Thomas Wiedemann, Emanuel Staudinger, Siwei Zhang, Christoph GÃ¼nther
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.12580">CoVOR-SLAM: Cooperative SLAM using Visual Odometry and Ranges for Multi-Robot Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A swarm of robots has advantages over a single robot, since it can explore larger areas much faster and is more robust to single-point failures. Accurate relative positioning is necessary to successfully carry out a collaborative mission without collisions. When Visual Simultaneous Localization and Mapping (VSLAM) is used to estimate the poses of each robot, inter-agent loop closing is widely applied to reduce the relative positioning errors. This technique can mitigate errors using the feature points commonly observed by different robots. However, it requires significant computing and communication capabilities to detect inter-agent loops, and to process the data transmitted by multiple agents. In this paper, we propose Collaborative SLAM using Visual Odometry and Range measurements (CoVOR-SLAM) to overcome this challenge. In the framework of CoVOR-SLAM, robots only need to exchange pose estimates, covariances (uncertainty) of the estimates, and range measurements between robots. Since CoVOR-SLAM does not require to associate visual features and map points observed by different agents, the computational and communication loads are significantly reduced. The required range measurements can be obtained using pilot signals of the communication system, without requiring complex additional infrastructure. We tested CoVOR-SLAM using real images as well as real ultra-wideband-based ranges obtained with two rovers. In addition, CoVOR-SLAM is evaluated with a larger scale multi-agent setup exploiting public image datasets and ranges generated using a realistic simulation. The results show that CoVOR-SLAM can accurately estimate the robots' poses, requiring much less computational power and communication capabilities than the inter-agent loop closing technique.
<div id='section'>Paperid: <span id='pid'>533, <a href='https://arxiv.org/pdf/2310.06160.pdf' target='_blank'>https://arxiv.org/pdf/2310.06160.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Muhammad Farhan Ahmed, Matteo Maragliano, Vincent FrÃ©mont, Carmine Tommaso Recchiuto
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.06160">Efficient Multi-robot Active SLAM</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous exploration in unknown environments remains a fundamental challenge in robotics, particularly for applications such as search and rescue, industrial inspection, and planetary exploration. Multi-robot active SLAM presents a promising solution by enabling collaborative mapping and exploration while actively reducing uncertainty. However, existing approaches often suffer from high computational costs and inefficient frontier management, making them computationally expensive for real-time applications. In this paper, we introduce an efficient multi-robot active SLAM framework that incorporates a frontier-sharing strategy to enhance robot distribution in unexplored environments. Our approach integrates a utility function that considers both pose graph uncertainty and path entropy, achieving an optimal balance between exploration coverage and computational efficiency. By filtering and prioritizing goal frontiers, our method significantly reduces computational overhead while preserving high mapping accuracy. The proposed framework has been implemented in ROS and validated through simulations and real-world experiments. Results demonstrate superior exploration performance and mapping quality compared to state-of-the-art approaches.
<div id='section'>Paperid: <span id='pid'>534, <a href='https://arxiv.org/pdf/2310.06008.pdf' target='_blank'>https://arxiv.org/pdf/2310.06008.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Donghao Qiao, Farhana Zulkernine
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.06008">CoBEVFusion: Cooperative Perception with LiDAR-Camera Bird's-Eye View Fusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous Vehicles (AVs) use multiple sensors to gather information about their surroundings. By sharing sensor data between Connected Autonomous Vehicles (CAVs), the safety and reliability of these vehicles can be improved through a concept known as cooperative perception. However, recent approaches in cooperative perception only share single sensor information such as cameras or LiDAR. In this research, we explore the fusion of multiple sensor data sources and present a framework, called CoBEVFusion, that fuses LiDAR and camera data to create a Bird's-Eye View (BEV) representation. The CAVs process the multi-modal data locally and utilize a Dual Window-based Cross-Attention (DWCA) module to fuse the LiDAR and camera features into a unified BEV representation. The fused BEV feature maps are shared among the CAVs, and a 3D Convolutional Neural Network is applied to aggregate the features from the CAVs. Our CoBEVFusion framework was evaluated on the cooperative perception dataset OPV2V for two perception tasks: BEV semantic segmentation and 3D object detection. The results show that our DWCA LiDAR-camera fusion model outperforms perception models with single-modal data and state-of-the-art BEV fusion models. Our overall cooperative perception architecture, CoBEVFusion, also achieves comparable performance with other cooperative perception models.
<div id='section'>Paperid: <span id='pid'>535, <a href='https://arxiv.org/pdf/2306.01179.pdf' target='_blank'>https://arxiv.org/pdf/2306.01179.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Michael Crosscombe, Jonathan Lawry
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.01179">The Benefits of Interaction Constraints in Distributed Autonomous Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The design of distributed autonomous systems often omits consideration of the underlying network dynamics. Recent works in multi-agent systems and swarm robotics alike have highlighted the impact that the interactions between agents have on the collective behaviours exhibited by the system. In this paper, we seek to highlight the role that the underlying interaction network plays in determining the performance of the collective behaviour of a system, comparing its impact with that of the physical network. We contextualise this by defining a collective learning problem in which agents must reach a consensus about their environment in the presence of noisy information. We show that the physical connectivity of the agents plays a less important role than when an interaction network of limited connectivity is imposed on the system to constrain agent communication. Constraining agent interactions in this way drastically improves the performance of the system in a collective learning context. Additionally, we provide further evidence for the idea that `less is more' when it comes to propagating information in distributed autonomous systems for the purpose of collective learning.
<div id='section'>Paperid: <span id='pid'>536, <a href='https://arxiv.org/pdf/2305.15288.pdf' target='_blank'>https://arxiv.org/pdf/2305.15288.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sukriti Singh, Anusha Srikanthan, Vivek Mallampati, Harish Ravichandar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.15288">Concurrent Constrained Optimization of Unknown Rewards for Multi-Robot Task Allocation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Task allocation can enable effective coordination of multi-robot teams to accomplish tasks that are intractable for individual robots. However, existing approaches to task allocation often assume that task requirements or reward functions are known and explicitly specified by the user. In this work, we consider the challenge of forming effective coalitions for a given heterogeneous multi-robot team when task reward functions are unknown. To this end, we first formulate a new class of problems, dubbed COncurrent Constrained Online optimization of Allocation (COCOA). The COCOA problem requires online optimization of coalitions such that the unknown rewards of all the tasks are simultaneously maximized using a given multi-robot team with constrained resources. To address the COCOA problem, we introduce an online optimization algorithm, named Concurrent Multi-Task Adaptive Bandits (CMTAB), that leverages and builds upon continuum-armed bandit algorithms. Experiments involving detailed numerical simulations and a simulated emergency response task reveal that CMTAB can effectively trade-off exploration and exploitation to simultaneously and efficiently optimize the unknown task rewards while respecting the team's resource constraints.
<div id='section'>Paperid: <span id='pid'>537, <a href='https://arxiv.org/pdf/2303.00047.pdf' target='_blank'>https://arxiv.org/pdf/2303.00047.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ratijit Mitra, Indranil Saha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.00047">Online On-Demand Multi-Robot Coverage Path Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present an online centralized path planning algorithm to cover a large, complex, unknown workspace with multiple homogeneous mobile robots. Our algorithm is horizon-based, synchronous, and on-demand. The recently proposed horizon-based synchronous algorithms compute all the robots' paths in each horizon, significantly increasing the computation burden in large workspaces with many robots. As a remedy, we propose an algorithm that computes the paths for a subset of robots that have traversed previously computed paths entirely (thus on-demand) and reuses the remaining paths for the other robots. We formally prove that the algorithm guarantees complete coverage of the unknown workspace. Experimental results on several standard benchmark workspaces show that our algorithm scales to hundreds of robots in large complex workspaces and consistently beats a state-of-the-art online centralized multi-robot coverage path planning algorithm in terms of the time needed to achieve complete coverage. For its validation, we perform ROS+Gazebo simulations in five 2D grid benchmark workspaces with 10 Quadcopters and 10 TurtleBots, respectively. Also, to demonstrate its practical feasibility, we conduct one indoor experiment with two real TurtleBot2 robots and one outdoor experiment with three real Quadcopters.
<div id='section'>Paperid: <span id='pid'>538, <a href='https://arxiv.org/pdf/2301.07147.pdf' target='_blank'>https://arxiv.org/pdf/2301.07147.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Manthan Patel, Marco Karrer, Philipp BÃ¤nninger, Margarita Chli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.07147">COVINS-G: A Generic Back-end for Collaborative Visual-Inertial SLAM</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Collaborative SLAM is at the core of perception in multi-robot systems as it enables the co-localization of the team of robots in a common reference frame, which is of vital importance for any coordination amongst them. The paradigm of a centralized architecture is well established, with the robots (i.e. agents) running Visual-Inertial Odometry (VIO) onboard while communicating relevant data, such as e.g. Keyframes (KFs), to a central back-end (i.e. server), which then merges and optimizes the joint maps of the agents. While these frameworks have proven to be successful, their capability and performance are highly dependent on the choice of the VIO front-end, thus limiting their flexibility. In this work, we present COVINS-G, a generalized back-end building upon the COVINS framework, enabling the compatibility of the server-back-end with any arbitrary VIO front-end, including, for example, off-the-shelf cameras with odometry capabilities, such as the Realsense T265. The COVINS-G back-end deploys a multi-camera relative pose estimation algorithm for computing the loop-closure constraints allowing the system to work purely on 2D image data. In the experimental evaluation, we show on-par accuracy with state-of-the-art multi-session and collaborative SLAM systems, while demonstrating the flexibility and generality of our approach by employing different front-ends onboard collaborating agents within the same mission. The COVINS-G codebase along with a generalized front-end wrapper to allow any existing VIO front-end to be readily used in combination with the proposed collaborative back-end is open-sourced. Video: https://youtu.be/FoJfXCfaYDw
<div id='section'>Paperid: <span id='pid'>539, <a href='https://arxiv.org/pdf/2209.12091.pdf' target='_blank'>https://arxiv.org/pdf/2209.12091.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mariliza Tzes, Nikolaos Bousias, Evangelos Chatzipantazis, George J. Pappas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.12091">Graph Neural Networks for Multi-Robot Active Information Acquisition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper addresses the Multi-Robot Active Information Acquisition (AIA) problem, where a team of mobile robots, communicating through an underlying graph, estimates a hidden state expressing a phenomenon of interest. Applications like target tracking, coverage and SLAM can be expressed in this framework. Existing approaches, though, are either not scalable, unable to handle dynamic phenomena or not robust to changes in the communication graph. To counter these shortcomings, we propose an Information-aware Graph Block Network (I-GBNet), an AIA adaptation of Graph Neural Networks, that aggregates information over the graph representation and provides sequential-decision making in a distributed manner. The I-GBNet, trained via imitation learning with a centralized sampling-based expert solver, exhibits permutation equivariance and time invariance, while harnessing the superior scalability, robustness and generalizability to previously unseen environments and robot configurations. Experiments on significantly larger graphs and dimensionality of the hidden state and more complex environments than those seen in training validate the properties of the proposed architecture and its efficacy in the application of localization and tracking of dynamic targets.
<div id='section'>Paperid: <span id='pid'>540, <a href='https://arxiv.org/pdf/2208.00116.pdf' target='_blank'>https://arxiv.org/pdf/2208.00116.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Donghao Qiao, Farhana Zulkernine
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2208.00116">Adaptive Feature Fusion for Cooperative Perception using LiDAR Point Clouds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cooperative perception allows a Connected Autonomous Vehicle (CAV) to interact with the other CAVs in the vicinity to enhance perception of surrounding objects to increase safety and reliability. It can compensate for the limitations of the conventional vehicular perception such as blind spots, low resolution, and weather effects. An effective feature fusion model for the intermediate fusion methods of cooperative perception can improve feature selection and information aggregation to further enhance the perception accuracy. We propose adaptive feature fusion models with trainable feature selection modules. One of our proposed models Spatial-wise Adaptive feature Fusion (S-AdaFusion) outperforms all other State-of-the-Arts (SOTAs) on two subsets of the OPV2V dataset: Default CARLA Towns for vehicle detection and the Culver City for domain adaptation. In addition, previous studies have only tested cooperative perception for vehicle detection. A pedestrian, however, is much more likely to be seriously injured in a traffic accident. We evaluate the performance of cooperative perception for both vehicle and pedestrian detection using the CODD dataset. Our architecture achieves higher Average Precision (AP) than other existing models for both vehicle and pedestrian detection on the CODD dataset. The experiments demonstrate that cooperative perception also improves the pedestrian detection accuracy compared to the conventional single vehicle perception process.
<div id='section'>Paperid: <span id='pid'>541, <a href='https://arxiv.org/pdf/1912.07796.pdf' target='_blank'>https://arxiv.org/pdf/1912.07796.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nathan K Long, Karl Sammut, Daniel Sgarioto, Matthew Garratt, Hussein Abbass
</span></div><div id="title">Title: <a href="https://arxiv.org/html/1912.07796">A Comprehensive Review of Shepherding as a Bio-inspired Swarm-Robotics Guidance Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The simultaneous control of multiple coordinated robotic agents represents an elaborate problem. If solved, however, the interaction between the agents can lead to solutions to sophisticated problems. The concept of swarming, inspired by nature, can be described as the emergence of complex system-level behaviors from the interactions of relatively elementary agents. Due to the effectiveness of solutions found in nature, bio-inspired swarming-based control techniques are receiving a lot of attention in robotics. One method, known as swarm shepherding, is founded on the sheep herding behavior exhibited by sheepdogs, where a swarm of relatively simple agents are governed by a shepherd (or shepherds) which is responsible for high-level guidance and planning. Many studies have been conducted on shepherding as a control technique, ranging from the replication of sheep herding via simulation, to the control of uninhabited vehicles and robots for a variety of applications. We present a comprehensive review of the literature on swarm shepherding to reveal the advantages and potential of the approach to be applied to a plethora of robotic systems in the future.
<div id='section'>Paperid: <span id='pid'>542, <a href='https://arxiv.org/pdf/2508.12456.pdf' target='_blank'>https://arxiv.org/pdf/2508.12456.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hadas C. Kuzmenko, David Ehevich, Oren Gal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.12456">Autonomous Oil Spill Response Through Liquid Neural Trajectory Modeling and Coordinated Marine Robotics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Marine oil spills pose grave environmental and economic risks, threatening marine ecosystems, coastlines, and dependent industries. Predicting and managing oil spill trajectories is highly complex, due to the interplay of physical, chemical, and environmental factors such as wind, currents, and temperature, which makes timely and effective response challenging. Accurate real-time trajectory forecasting and coordinated mitigation are vital for minimizing the impact of these disasters. This study introduces an integrated framework combining a multi-agent swarm robotics system built on the MOOS-IvP platform with Liquid Time-Constant Neural Networks (LTCNs). The proposed system fuses adaptive machine learning with autonomous marine robotics, enabling real-time prediction, dynamic tracking, and rapid response to evolving oil spills. By leveraging LTCNs--well-suited for modeling complex, time-dependent processes--the framework achieves real-time, high-accuracy forecasts of spill movement. Swarm intelligence enables decentralized, scalable, and resilient decision-making among robot agents, enhancing collective monitoring and containment efforts. Our approach was validated using data from the Deepwater Horizon spill, where the LTC-RK4 model achieved 0.96 spatial accuracy, surpassing LSTM approaches by 23%. The integration of advanced neural modeling with autonomous, coordinated robotics demonstrates substantial improvements in prediction precision, flexibility, and operational scalability. Ultimately, this research advances the state-of-the-art for sustainable, autonomous oil spill management and environmental protection by enhancing both trajectory prediction and response coordination.
<div id='section'>Paperid: <span id='pid'>543, <a href='https://arxiv.org/pdf/2507.21553.pdf' target='_blank'>https://arxiv.org/pdf/2507.21553.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Federica Di Lauro, Domenico G. Sorrenti, Miguel Angel Sotelo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21553">Multi-robot LiDAR SLAM: a practical case study in underground tunnel environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-robot SLAM aims at localizing and building a map with multiple robots, interacting with each other. In the work described in this article, we analyze the pipeline of a decentralized LiDAR SLAM system to study the current limitations of the state of the art, and we discover a significant source of failures, i.e., that the loop detection is the source of too many false positives. We therefore develop and propose a new heuristic to overcome these limitations. The environment taken as reference in this work is the highly challenging case of underground tunnels. We also highlight potential new research areas still under-explored.
<div id='section'>Paperid: <span id='pid'>544, <a href='https://arxiv.org/pdf/2507.13969.pdf' target='_blank'>https://arxiv.org/pdf/2507.13969.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maria Eduarda Silva de Macedo, Ana Paula Chiarelli de Souza, Roberto Silvio Ubertino Rosso, Yuri Kaszubowski Lopes
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.13969">A Minimalist Controller for Autonomously Self-Aggregating Robotic Swarms: Enabling Compact Formations in Multitasking Scenarios</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The deployment of simple emergent behaviors in swarm robotics has been well-rehearsed in the literature. A recent study has shown how self-aggregation is possible in a multitask approach -- where multiple self-aggregation task instances occur concurrently in the same environment. The multitask approach poses new challenges, in special, how the dynamic of each group impacts the performance of others. So far, the multitask self-aggregation of groups of robots suffers from generating a circular formation -- that is not fully compact -- or is not fully autonomous. In this paper, we present a multitask self-aggregation where groups of homogeneous robots sort themselves into different compact clusters, relying solely on a line-of-sight sensor. Our multitask self-aggregation behavior was able to scale well and achieve a compact formation. We report scalability results from a series of simulation trials with different configurations in the number of groups and the number of robots per group. We were able to improve the multitask self-aggregation behavior performance in terms of the compactness of the clusters, keeping the proportion of clustered robots found in other studies.
<div id='section'>Paperid: <span id='pid'>545, <a href='https://arxiv.org/pdf/2507.04917.pdf' target='_blank'>https://arxiv.org/pdf/2507.04917.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Thayanne FranÃ§a da Silva, JosÃ© Everardo Bessa Maia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.04917">Leadership Detection via Time-Lagged Correlation-Based Network Inference</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding leadership dynamics in collective behavior is a key challenge in animal ecology, swarm robotics, and intelligent transportation. Traditional information-theoretic approaches, including Transfer Entropy (TE) and Time-Lagged Mutual Information (TLMI), have been widely used to infer leader-follower relationships but face critical limitations in noisy or short-duration datasets due to their reliance on robust probability estimations. This study proposes a method based on dynamic network inference using time-lagged correlations across multiple kinematic variables: velocity, acceleration, and direction. Our approach constructs directed influence graphs over time, enabling the identification of leadership patterns without the need for large volumes of data or parameter-sensitive discretization. We validate our method through two multi-agent simulations in NetLogo: a modified Vicsek model with informed leaders and a predator-prey model featuring coordinated and independent wolf groups. Experimental results demonstrate that the network-based method outperforms TE and TLMI in scenarios with limited spatiotemporal observations, ranking true leaders at the top of influence metrics more consistently than TE and TLMI.
<div id='section'>Paperid: <span id='pid'>546, <a href='https://arxiv.org/pdf/2506.13453.pdf' target='_blank'>https://arxiv.org/pdf/2506.13453.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>YR Darr, MA Niazi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.13453">Towards a Formal Specification for Self-organized Shape Formation in Swarm Robotics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The self-organization of robots for the formation of structures and shapes is a stimulating application of the swarm robotic system. It involves a large number of autonomous robots of heterogeneous behavior, coordination among them, and their interaction with the dynamic environment. This process of complex structure formation is considered a complex system, which needs to be modeled by using any modeling approach. Although the formal specification approach along with other formal methods has been used to model the behavior of robots in a swarm. However, to the best of our knowledge, the formal specification approach has not been used to model the self-organization process in swarm robotic systems for shape formation. In this paper, we use a formal specification approach to model the shape formation task of swarm robots. We use Z (Zed) language of formal specification, which is a state-based language, to model the states of the entities of the systems. We demonstrate the effectiveness of Z for the self-organized shape formation. The presented formal specification model gives the outlines for designing and implementing the swarm robotic system for the formation of complex shapes and structures. It also provides the foundation for modeling the complex shape formation process for swarm robotics using a multi-agent system in a simulation-based environment. Keywords: Swarm robotics, Self-organization, Formal specification, Complex systems
<div id='section'>Paperid: <span id='pid'>547, <a href='https://arxiv.org/pdf/2505.03920.pdf' target='_blank'>https://arxiv.org/pdf/2505.03920.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jose Fernando Contreras-Monsalvo, Victor Dossetti, Blanca Susana Soto-Cruz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.03920">Omnidirectional vision sensors based on catadioptric systems with discrete infrared photoreceptors for swarm robotics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we fabricated and studied two designs for omnidirectional vision sensors for swarm robotics, based on catadioptric systems consisting of a mirror with rotational symmetry, eight discrete infrared photodiodes and a single LED, in order to provide localization and navigation abilities for mobile robotic agents. We considered two arrangements for the photodiodes: one in which they point upward into the mirror, and one in which they point outward, perpendicular to the mirror. To determine which design offers a better field of view on the plane, as well as detection of distance and orientation between two agents, we developed a test rail with three degrees of freedom to experimentally and systematically measure the signal registered by the photodiodes of a given sensor (in a single readout) from the light emitted by another as functions of the distance and orientation. Afterwards, we processed and analyzed the experimental data to develop mathematical models for the mean response of a photodiode in each design. Finally, by numerically inverting the models, we compared the two designs in terms of their accuracy. Our results show that the design with the photodiodes pointing upward resolves better the distance, while the other resolves better the orientation of the emitting agent, both providing an omnidirectional field of view.
<div id='section'>Paperid: <span id='pid'>548, <a href='https://arxiv.org/pdf/2504.20947.pdf' target='_blank'>https://arxiv.org/pdf/2504.20947.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Norah K. Alghamdi, Shinkyu Park
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.20947">Opinion-Driven Decision-Making for Multi-Robot Navigation through Narrow Corridors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose an opinion-driven navigation framework for multi-robot traversal through a narrow corridor. Our approach leverages a multi-agent decision-making model known as the Nonlinear Opinion Dynamics (NOD) to address the narrow corridor passage problem, formulated as a multi-robot navigation game. By integrating the NOD model with a multi-robot path planning algorithm, we demonstrate that the framework effectively reduces the likelihood of deadlocks during corridor traversal. To ensure scalability with an increasing number of robots, we introduce a game reduction technique that enables efficient coordination in larger groups. Extensive simulation studies are conducted to validate the effectiveness of the proposed approach.
<div id='section'>Paperid: <span id='pid'>549, <a href='https://arxiv.org/pdf/2504.20071.pdf' target='_blank'>https://arxiv.org/pdf/2504.20071.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pranav Kedia, Madhav Rao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.20071">GenGrid: A Generalised Distributed Experimental Environmental Grid for Swarm Robotics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>GenGrid is a novel comprehensive open-source, distributed platform intended for conducting extensive swarm robotic experiments. The modular platform is designed to run swarm robotics experiments that are compatible with different types of mobile robots ranging from Colias, Kilobot, and E puck. The platform offers programmable control over the experimental setup and its parameters and acts as a tool to collect swarm robot data, including localization, sensory feedback, messaging, and interaction. GenGrid is designed as a modular grid of attachable computing nodes that offers bidirectional communication between the robotic agent and grid nodes and within grids. The paper describes the hardware and software architecture design of the GenGrid system. Further, it discusses some common experimental studies covering multi-robot and swarm robotics to showcase the platform's use. GenGrid of 25 homogeneous cells with identical sensing and communication characteristics with a footprint of 37.5 cm X 37.5 cm, exhibits multiple capabilities with minimal resources. The open-source hardware platform is handy for running swarm experiments, including robot hopping based on multiple gradients, collective transport, shepherding, continuous pheromone deposition, and subsequent evaporation. The low-cost, modular, and open-source platform is significant in the swarm robotics research community, which is currently driven by commercial platforms that allow minimal modifications.
<div id='section'>Paperid: <span id='pid'>550, <a href='https://arxiv.org/pdf/2504.01940.pdf' target='_blank'>https://arxiv.org/pdf/2504.01940.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Juan Bravo-Arrabal, Ricardo VÃ¡zquez-MartÃ­n, J. J. FernÃ¡ndez-Lozano, Alfonso GarcÃ­a-Cerezo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.01940">Strengthening Multi-Robot Systems for SAR: Co-Designing Robotics and Communication Towards 6G</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents field-tested use cases from Search and Rescue (SAR) missions, highlighting the co-design of mobile robots and communication systems to support Edge-Cloud architectures based on 5G Standalone (SA). The main goal is to contribute to the effective cooperation of multiple robots and first responders. Our field experience includes the development of Hybrid Wireless Sensor Networks (H-WSNs) for risk and victim detection, smartphones integrated into the Robot Operating System (ROS) as Edge devices for mission requests and path planning, real-time Simultaneous Localization and Mapping (SLAM) via Multi-Access Edge Computing (MEC), and implementation of Uncrewed Ground Vehicles (UGVs) for victim evacuation in different navigation modes. These experiments, conducted in collaboration with actual first responders, underscore the need for intelligent network resource management, balancing low-latency and high-bandwidth demands. Network slicing is key to ensuring critical emergency services are performed despite challenging communication conditions. The paper identifies architectural needs, lessons learned, and challenges to be addressed by 6G technologies to enhance emergency response capabilities.
<div id='section'>Paperid: <span id='pid'>551, <a href='https://arxiv.org/pdf/2504.01174.pdf' target='_blank'>https://arxiv.org/pdf/2504.01174.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sheikh A. Tahmid, Gennaro Notomista
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.01174">Value Iteration for Learning Concurrently Executable Robotic Control Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Many modern robotic systems such as multi-robot systems and manipulators exhibit redundancy, a property owing to which they are capable of executing multiple tasks. This work proposes a novel method, based on the Reinforcement Learning (RL) paradigm, to train redundant robots to be able to execute multiple tasks concurrently. Our approach differs from typical multi-objective RL methods insofar as the learned tasks can be combined and executed in possibly time-varying prioritized stacks. We do so by first defining a notion of task independence between learned value functions. We then use our definition of task independence to propose a cost functional that encourages a policy, based on an approximated value function, to accomplish its control objective while minimally interfering with the execution of higher priority tasks. This allows us to train a set of control policies that can be executed simultaneously. We also introduce a version of fitted value iteration to learn to approximate our proposed cost functional efficiently. We demonstrate our approach on several scenarios and robotic systems.
<div id='section'>Paperid: <span id='pid'>552, <a href='https://arxiv.org/pdf/2502.17039.pdf' target='_blank'>https://arxiv.org/pdf/2502.17039.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinxin Feng, Haoran Sun, Haifeng Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.17039">LCV2I: Communication-Efficient and High-Performance Collaborative Perception Framework with Low-Resolution LiDAR</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vehicle-to-Infrastructure (V2I) collaborative perception leverages data collected by infrastructure's sensors to enhance vehicle perceptual capabilities. LiDAR, as a commonly used sensor in cooperative perception, is widely equipped in intelligent vehicles and infrastructure. However, its superior performance comes with a correspondingly high cost. To achieve low-cost V2I, reducing the cost of LiDAR is crucial. Therefore, we study adopting low-resolution LiDAR on the vehicle to minimize cost as much as possible. However, simply reducing the resolution of vehicle's LiDAR results in sparse point clouds, making distant small objects even more blurred. Additionally, traditional communication methods have relatively low bandwidth utilization efficiency. These factors pose challenges for us. To balance cost and perceptual accuracy, we propose a new collaborative perception framework, namely LCV2I. LCV2I uses data collected from cameras and low-resolution LiDAR as input. It also employs feature offset correction modules and regional feature enhancement algorithms to improve feature representation. Finally, we use regional difference map and regional score map to assess the value of collaboration content, thereby improving communication bandwidth efficiency. In summary, our approach achieves high perceptual performance while substantially reducing the demand for high-resolution sensors on the vehicle. To evaluate this algorithm, we conduct 3D object detection in the real-world scenario of DAIR-V2X, demonstrating that the performance of LCV2I consistently surpasses currently existing algorithms.
<div id='section'>Paperid: <span id='pid'>553, <a href='https://arxiv.org/pdf/2502.10062.pdf' target='_blank'>https://arxiv.org/pdf/2502.10062.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoshan Lin, Roberto Tron
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.10062">Adaptive Bi-Level Multi-Robot Task Allocation and Learning under Uncertainty with Temporal Logic Constraints</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work addresses the problem of multi-robot coordination under unknown robot transition models, ensuring that tasks specified by Time Window Temporal Logic are satisfied with user-defined probability thresholds. We present a bi-level framework that integrates (i) high-level task allocation, where tasks are assigned based on the robots' estimated task completion probabilities and expected rewards, and (ii) low-level distributed policy learning and execution, where robots independently optimize auxiliary rewards while fulfilling their assigned tasks. To handle uncertainty in robot dynamics, our approach leverages real-time task execution data to iteratively refine expected task completion probabilities and rewards, enabling adaptive task allocation without explicit robot transition models. We theoretically validate the proposed algorithm, demonstrating that the task assignments meet the desired probability thresholds with high confidence. Finally, we demonstrate the effectiveness of our framework through comprehensive simulations.
<div id='section'>Paperid: <span id='pid'>554, <a href='https://arxiv.org/pdf/2502.07595.pdf' target='_blank'>https://arxiv.org/pdf/2502.07595.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Federico Pratissoli, Mattia Mantovani, Amanda Prorok, Lorenzo Sabattini
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.07595">Distributed Coverage Control for Time-Varying Spatial Processes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-robot systems are essential for environmental monitoring, particularly for tracking spatial phenomena like pollution, soil minerals, and water salinity, and more. This study addresses the challenge of deploying a multi-robot team for optimal coverage in environments where the density distribution, describing areas of interest, is unknown and changes over time. We propose a fully distributed control strategy that uses Gaussian Processes (GPs) to model the spatial field and balance the trade-off between learning the field and optimally covering it. Unlike existing approaches, we address a more realistic scenario by handling time-varying spatial fields, where the exploration-exploitation trade-off is dynamically adjusted over time. Each robot operates locally, using only its own collected data and the information shared by the neighboring robots. To address the computational limits of GPs, the algorithm efficiently manages the volume of data by selecting only the most relevant samples for the process estimation. The performance of the proposed algorithm is evaluated through several simulations and experiments, incorporating real-world data phenomena to validate its effectiveness.
<div id='section'>Paperid: <span id='pid'>555, <a href='https://arxiv.org/pdf/2501.04442.pdf' target='_blank'>https://arxiv.org/pdf/2501.04442.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Seyed Amir Tafrishi, Mikhail Svinin, Kenji Tahara
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.04442">A Survey on Path Planning Problem of Rolling Contacts: Approaches, Applications and Future Challenges</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper explores an eclectic range of path-planning methodologies engineered for rolling surfaces. Our focus is on the kinematic intricacies of rolling contact systems, which are investigated through a motion planning lens. Beyond summarizing the approaches to single-contact rotational surfaces, we explore the challenging domain of spin-rolling multi-contact systems. Our work proposes solutions for the higher-dimensional problem of multiple rotating objects in contact. Venturing beyond kinematics, these methodologies find application across a spectrum of domains, including rolling robots, reconfigurable swarm robotics, micro/nano manipulation, and nonprehensile manipulations. Through meticulously examining established planning strategies, we unveil their practical implementations in various real-world scenarios, from intricate dexterous manipulation tasks to the nimble manoeuvring of rolling robots and even shape planning of multi-contact swarms of particles. This study introduces the persistent challenges and unexplored frontiers of robotics, intricately linked to both path planning and mechanism design. As we illuminate existing solutions, we also set the stage for future breakthroughs in this dynamic and rapidly evolving field by highlighting the critical importance of addressing rolling contact problems.
<div id='section'>Paperid: <span id='pid'>556, <a href='https://arxiv.org/pdf/2412.07485.pdf' target='_blank'>https://arxiv.org/pdf/2412.07485.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sumit Paul, Danh Lephuoc, Manfred Hauswirth
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.07485">Performance Evaluation of ROS2-DDS middleware implementations facilitating Cooperative Driving in Autonomous Vehicle</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the autonomous vehicle and self-driving paradigm, cooperative perception or exchanging sensor information among vehicles over wireless communication has added a new dimension. Generally, an autonomous vehicle is a special type of robot that requires real-time, highly reliable sensor inputs due to functional safety. Autonomous vehicles are equipped with a considerable number of sensors to provide different required sensor data to make the driving decision and share with other surrounding vehicles. The inclusion of Data Distribution Service(DDS) as a communication middleware in ROS2 has proved its potential capability to be a reliable real-time distributed system. DDS comes with a scoping mechanism known as domain. Whenever a ROS2 process is initiated, it creates a DDS participant. It is important to note that there is a limit to the number of participants allowed in a single domain.
  The efficient handling of numerous in-vehicle sensors and their messages demands the use of multiple ROS2 nodes in a single vehicle. Additionally, in the cooperative perception paradigm, a significant number of ROS2 nodes can be required when a vehicle functions as a single ROS2 node. These ROS2 nodes cannot be part of a single domain due to DDS participant limitation; thus, different domain communication is unavoidable. Moreover, there are different vendor-specific implementations of DDS, and each vendor has their configurations, which is an inevitable communication catalyst between the ROS2 nodes. The communication between vehicles or robots or ROS2 nodes depends directly on the vendor-specific configuration, data type, data size, and the DDS implementation used as middleware; in our study, we evaluate and investigate the limitations, capabilities, and prospects of the different domain communication for various vendor-specific DDS implementations for diverse sensor data type.
<div id='section'>Paperid: <span id='pid'>557, <a href='https://arxiv.org/pdf/2412.03387.pdf' target='_blank'>https://arxiv.org/pdf/2412.03387.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xin Ye, Karl Handwerker, SÃ¶ren Hohmann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.03387">Adaptive Model Predictive Control for Differential-Algebraic Systems towards a Higher Path Accuracy for Physically Coupled Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The physical coupling between robots has the potential to improve the capabilities of multi-robot systems in challenging manufacturing processes. However, the path tracking accuracy of physically coupled robots is not studied adequately, especially considering the uncertain kinematic parameters, the mechanical elasticity, and the built-in controllers of off-the-shelf robots. This paper addresses these issues with a novel differential-algebraic system model which is verified against measurement data from real execution. The uncertain kinematic parameters are estimated online to adapt the model. Consequently, an adaptive model predictive controller is designed as a coordinator between the robots. The controller achieves a path tracking error reduction of 88.6% compared to the state-of-the-art benchmark in the simulation.
<div id='section'>Paperid: <span id='pid'>558, <a href='https://arxiv.org/pdf/2410.19718.pdf' target='_blank'>https://arxiv.org/pdf/2410.19718.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guilherme S. Y. Giardini, John F. Hardy, Carlo R. da Cunha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.19718">Evolving Neural Networks Reveal Emergent Collective Behavior from Minimal Agent Interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding the mechanisms behind emergent behaviors in multi-agent systems is critical for advancing fields such as swarm robotics and artificial intelligence. In this study, we investigate how neural networks evolve to control agents' behavior in a dynamic environment, focusing on the relationship between the network's complexity and collective behavior patterns. By performing quantitative and qualitative analyses, we demonstrate that the degree of network non-linearity correlates with the complexity of emergent behaviors. Simpler behaviors, such as lane formation and laminar flow, are characterized by more linear network operations, while complex behaviors like swarming and flocking show highly non-linear neural processing. Moreover, specific environmental parameters, such as moderate noise, broader field of view, and lower agent density, promote the evolution of non-linear networks that drive richer, more intricate collective behaviors. These results highlight the importance of tuning evolutionary conditions to induce desired behaviors in multi-agent systems, offering new pathways for optimizing coordination in autonomous swarms. Our findings contribute to a deeper understanding of how neural mechanisms influence collective dynamics, with implications for the design of intelligent, self-organizing systems.
<div id='section'>Paperid: <span id='pid'>559, <a href='https://arxiv.org/pdf/2410.19373.pdf' target='_blank'>https://arxiv.org/pdf/2410.19373.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gengyuan Cai, Luosong Guo, Xiangmao Chang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.19373">An Enhanced Hierarchical Planning Framework for Multi-Robot Autonomous Exploration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The autonomous exploration of environments by multi-robot systems is a critical task with broad applications in rescue missions, exploration endeavors, and beyond. Current approaches often rely on either greedy frontier selection or end-to-end deep reinforcement learning (DRL) methods, yet these methods are frequently hampered by limitations such as short-sightedness, overlooking long-term implications, and convergence difficulties stemming from the intricate high-dimensional learning space. To address these challenges, this paper introduces an innovative integration strategy that combines the low-dimensional action space efficiency of frontier-based methods with the far-sightedness and optimality of DRL-based approaches. We propose a three-tiered planning framework that first identifies frontiers in free space, creating a sparse map representation that lightens data transmission burdens and reduces the DRL action space's dimensionality. Subsequently, we develop a multi-graph neural network (mGNN) that incorporates states of potential targets and robots, leveraging policy-based reinforcement learning to compute affinities, thereby superseding traditional heuristic utility values. Lastly, we implement local routing planning through subsequence search, which avoids exhaustive sequence traversal. Extensive validation across diverse scenarios and comprehensive simulation results demonstrate the effectiveness of our proposed method. Compared to baseline approaches, our framework achieves environmental exploration with fewer time steps and a notable reduction of over 30% in data transmission, showcasing its superiority in terms of efficiency and performance.
<div id='section'>Paperid: <span id='pid'>560, <a href='https://arxiv.org/pdf/2410.16295.pdf' target='_blank'>https://arxiv.org/pdf/2410.16295.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shiba Biswal, Karthik Elamvazhuthi, Rishi Sonthalia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.16295">Universal Approximation of Mean-Field Models via Transformers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper investigates the use of transformers to approximate the mean-field dynamics of interacting particle systems exhibiting collective behavior. Such systems are fundamental in modeling phenomena across physics, biology, and engineering, including opinion formation, biological networks, and swarm robotics. The key characteristic of these systems is that the particles are indistinguishable, leading to permutation-equivariant dynamics. First, we empirically demonstrate that transformers are well-suited for approximating a variety of mean field models, including the Cucker-Smale model for flocking and milling, and the mean-field system for training two-layer neural networks. We validate our numerical experiments via mathematical theory. Specifically, we prove that if a finite-dimensional transformer effectively approximates the finite-dimensional vector field governing the particle system, then the $L_2$ distance between the \textit{expected transformer} and the infinite-dimensional mean-field vector field can be uniformly bounded by a function of the number of particles observed during training. Leveraging this result, we establish theoretical bounds on the distance between the true mean-field dynamics and those obtained using the transformer.
<div id='section'>Paperid: <span id='pid'>561, <a href='https://arxiv.org/pdf/2410.05017.pdf' target='_blank'>https://arxiv.org/pdf/2410.05017.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ang He, Xi-mei Wu, Xiao-bin Guo, Li-bin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.05017">Enhanced Multi-Robot SLAM System with Cross-Validation Matching and Exponential Threshold Keyframe Selection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The evolving field of mobile robotics has indeed increased the demand for simultaneous localization and mapping (SLAM) systems. To augment the localization accuracy and mapping efficacy of SLAM, we refined the core module of the SLAM system. Within the feature matching phase, we introduced cross-validation matching to filter out mismatches. In the keyframe selection strategy, an exponential threshold function is constructed to quantify the keyframe selection process. Compared with a single robot, the multi-robot collaborative SLAM (CSLAM) system substantially improves task execution efficiency and robustness. By employing a centralized structure, we formulate a multi-robot SLAM system and design a coarse-to-fine matching approach for multi-map point cloud registration. Our system, built upon ORB-SLAM3, underwent extensive evaluation utilizing the TUM RGB-D, EuRoC MAV, and TUM_VI datasets. The experimental results demonstrate a significant improvement in the positioning accuracy and mapping quality of our enhanced algorithm compared to those of ORB-SLAM3, with a 12.90% reduction in the absolute trajectory error.
<div id='section'>Paperid: <span id='pid'>562, <a href='https://arxiv.org/pdf/2407.08767.pdf' target='_blank'>https://arxiv.org/pdf/2407.08767.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Poojith U Rao, Florian Speelman, Balwinder Sodhi, Sachin Kinge
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.08767">A Quantum Computing Approach for Multi-robot Coverage Path Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper tackles the multi-vehicle Coverage Path Planning (CPP) problem, crucial for applications like search and rescue or environmental monitoring. Due to its NP-hard nature, finding optimal solutions becomes infeasible with larger problem sizes. This motivates the development of heuristic approaches that enhance efficiency even marginally. We propose a novel approach for exploring paths in a 2D grid, specifically designed for easy integration with the Quantum Alternating Operator Ansatz (QAOA), a powerful quantum heuristic. Our contribution includes: 1) An objective function tailored to solve the multi-vehicle CPP using QAOA. 2) Theoretical proofs guaranteeing the validity of the proposed approach. 3) Efficient construction of QAOA operators for practical implementation. 4) Resource estimation to assess the feasibility of QAOA execution. 5) Performance comparison against established algorithms like the Depth First Search. This work paves the way for leveraging quantum computing in optimizing multi-vehicle path planning, potentially leading to real-world advancements in various applications.
<div id='section'>Paperid: <span id='pid'>563, <a href='https://arxiv.org/pdf/2406.10635.pdf' target='_blank'>https://arxiv.org/pdf/2406.10635.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zijun Xu, Xuanjun Wen, Yanjie Song, Shu Yin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.10635">ROSfs: A User-Level File System for ROS</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present ROSfs, a novel user-level file system for the Robot Operating System (ROS). ROSfs interprets a robot file as a group of sub-files, with each having a distinct label. ROSfs applies a time index structure to enhance the flexible data query while the data file is under modification. It provides multi-robot systems (MRS) with prompt cross-robot data acquisition and collaboration. We implemented a ROSfs prototype and integrated it into a mainstream ROS platform. We then applied and evaluated ROSfs on real-world UAVs and data servers. Evaluation results show that compared with traditional ROS storage methods, ROSfs improves the offline query performance by up to 129x and reduces inter-robot online data query latency under a wireless network by up to 7x.
<div id='section'>Paperid: <span id='pid'>564, <a href='https://arxiv.org/pdf/2405.11659.pdf' target='_blank'>https://arxiv.org/pdf/2405.11659.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tharun V. Puthanveettil, Abhijay Singh, Yashveer Jain, Vinay Bukka, Sameer Arjun S
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.11659">Auto-Platoon : Freight by example</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The work introduces a bio-inspired leader-follower system based on an innovative mechanism proposed as software latching that aims to improve collaboration and coordination between a leader agent and the associated autonomous followers. The system utilizes software latching to establish real-time communication and synchronization between the leader and followers. A layered architecture is proposed, encompassing perception, decision-making, and control modules. Challenges such as uncertainty, dynamic environments, and communication latency are addressed using Deep learning and real-time data processing pipelines. The follower robot is equipped with sensors and communication modules that enable it to track and trace the agent of interest or avoid obstacles. The followers track the leader and dynamically avoid obstacles while maintaining a safe distance from it. The experimental results demonstrate the proposed system's effectiveness, making it a promising solution for achieving success in tasks that demand multi-robot systems capable of navigating complex dynamic environments.
<div id='section'>Paperid: <span id='pid'>565, <a href='https://arxiv.org/pdf/2405.02484.pdf' target='_blank'>https://arxiv.org/pdf/2405.02484.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sujeet Kashid, Ashwin D. Kumat
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.02484">Hierarchically Decentralized Heterogeneous Multi-Robot Task Allocation System</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With plans to send humans to the Moon and further, the supply of resources like oxygen, water, fuel, etc., can be satiated by performing In-Situ Resource Utilization (ISRU), where resources from the extra-terrestrial body are extracted to be utilized. These ISRU missions can be carried out by a Multi-Robot System (MRS). In this research, a high-level auction- based Multi-Robot Task Allocation (MRTA) system is developed for coordinating tasks amongst multiple robots with distinct capabilities. A hierarchical decentralized coordination architecture is implemented in this research to allocate the tasks amongst the robots for achieving intentional cooperation in the Multi-Robot System (MRS). 3 different policies are formulated that govern how robots should act in the multiple auction situations of the auction-based task allocation system proposed in this research, and their performance is evaluated in a 2D simulation called pyrobosim using ROS2. The decentralized coordination architecture and the auction-based MRTA make the MRS highly scalable, reliable, flexible, and robust.
<div id='section'>Paperid: <span id='pid'>566, <a href='https://arxiv.org/pdf/2405.00586.pdf' target='_blank'>https://arxiv.org/pdf/2405.00586.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gjosse Zijlstra, Karen L. Aplin, Edmund R. Hunt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.00586">Multi-Robot Strategies for Communication-Constrained Exploration and Electrostatic Anomaly Characterization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Exploration of extreme or remote environments such as Mars is often recognized as an opportunity for multi-robot systems. However, this poses challenges for maintaining robust inter-robot communication without preexisting infrastructure. It may be that robots can only share information when they are physically in close proximity with each other. At the same time, atmospheric phenomena such as dust devils are poorly understood and characterization of their electrostatic properties is of scientific interest. We perform a comparative analysis of two multi-robot communication strategies: a distributed approach, with pairwise intermittent rendezvous, and a centralized, fixed base station approach. We also introduce and evaluate the effectiveness of an algorithm designed to predict the location and strength of electrostatic anomalies, assuming robot proximity. Using an agent-based simulation, we assess the performance of these strategies in a 2D grid cell representation of a Martian environment. Results indicate that a decentralized rendezvous system consistently outperforms a fixed base station system in terms of exploration speed and in reducing the risk of data loss. We also find that inter-robot data sharing improves performance when trying to predict the location and strength of an electrostatic anomaly. These findings indicate the importance of appropriate communication strategies for efficient multi-robot science missions.
<div id='section'>Paperid: <span id='pid'>567, <a href='https://arxiv.org/pdf/2403.16489.pdf' target='_blank'>https://arxiv.org/pdf/2403.16489.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Binh Nguyen, Linh Nguyen, Truong X. Nghiem, Hung La, Jose Baca, Pablo Rangel, Miguel Cid Montoya, Thang Nguyen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.16489">Spatially temporally distributed informative path planning for multi-robot systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper investigates the problem of informative path planning for a mobile robotic sensor network in spatially temporally distributed mapping. The robots are able to gather noisy measurements from an area of interest during their movements to build a Gaussian Process (GP) model of a spatio-temporal field. The model is then utilized to predict the spatio-temporal phenomenon at different points of interest. To spatially and temporally navigate the group of robots so that they can optimally acquire maximal information gains while their connectivity is preserved, we propose a novel multistep prediction informative path planning optimization strategy employing our newly defined local cost functions. By using the dual decomposition method, it is feasible and practical to effectively solve the optimization problem in a distributed manner. The proposed method was validated through synthetic experiments utilizing real-world data sets.
<div id='section'>Paperid: <span id='pid'>568, <a href='https://arxiv.org/pdf/2403.07635.pdf' target='_blank'>https://arxiv.org/pdf/2403.07635.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Angelos Dimakos, Daniel Woodhall, Seemal Asif
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.07635">A Study on Centralised and Decentralised Swarm Robotics Architecture for Part Delivery System</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Drones are also known as UAVs are originally designed for military purposes. With the technological advances, they can be seen in most of the aspects of life from filming to logistics. The increased use of drones made it sometimes essential to form a collaboration between them to perform the task efficiently in a defined process. This paper investigates the use of a combined centralised and decentralised architecture for the collaborative operation of drones in a parts delivery scenario to enable and expedite the operation of the factories of the future. The centralised and decentralised approaches were extensively researched, with experimentation being undertaken to determine the appropriateness of each approach for this use-case. Decentralised control was utilised to remove the need for excessive communication during the operation of the drones, resulting in smoother operations. Initial results suggested that the decentralised approach is more appropriate for this use-case. The individual functionalities necessary for the implementation of a decentralised architecture were proven and assessed, determining that a combination of multiple individual functionalities, namely VSLAM, dynamic collision avoidance and object tracking, would give an appropriate solution for use in an industrial setting. A final architecture for the parts delivery system was proposed for future work, using a combined centralised and decentralised approach to combat the limitations inherent in each architecture.
<div id='section'>Paperid: <span id='pid'>569, <a href='https://arxiv.org/pdf/2403.05962.pdf' target='_blank'>https://arxiv.org/pdf/2403.05962.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tanmoy Kundu, Moshe Rafaeli, Anton Gulyaev, Vadim Indelman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.05962">Action-Consistent Decentralized Belief Space Planning with Inconsistent Beliefs and Limited Data Sharing: Framework and Simplification Algorithms with Formal Guarantees</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In multi-robot systems, ensuring safe and reliable decision making under uncertain conditions demands robust multi-robot belief space planning (MR-BSP) algorithms. While planning with multiple robots, each robot maintains a belief over the state of the environment and reasons how the belief would evolve in the future for different possible actions. However, existing MR-BSP works have a common assumption that the beliefs of different robots are same at planning time. Such an assumption is often unrealistic as it requires prohibitively extensive and frequent data sharing capabilities. In practice, robots may have limited communication capabilities, and consequently beliefs of the robots can be different. Crucially, when the robots have inconsistent beliefs, the existing approaches could result in lack of coordination between the robots and may lead to unsafe decisions. In this paper, we present decentralized MR-BSP algorithms, with performance guarantees, for tackling this crucial gap. Our algorithms leverage the notion of action preferences. The base algorithm VerifyAC guarantees a consistent joint action selection by the cooperative robots via a three-step verification. When the verification succeeds, VerifyAC finds a consistent joint action without triggering a communication; otherwise it triggers a communication. We design an extended algorithm R-VerifyAC for further reducing the number of communications, by relaxing the criteria of action consistency. Another extension R-VerifyAC-simp builds on verifying a partial set of observations and improves the computation time significantly. The theoretical performance guarantees are corroborated with simulation results in discrete setting. Furthermore, we formulate our approaches for continuous and high-dimensional state and observation spaces, and provide experimental results for active multi-robot visual SLAM with real robots.
<div id='section'>Paperid: <span id='pid'>570, <a href='https://arxiv.org/pdf/2402.13292.pdf' target='_blank'>https://arxiv.org/pdf/2402.13292.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aakash, Indranil Saha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.13292">A Conflict-Aware Optimal Goal Assignment Algorithm for Multi-Robot Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The fundamental goal assignment problem for a multi-robot application aims to assign a unique goal to each robot while ensuring collision-free paths, minimizing the total movement cost. A plausible algorithmic solution to this NP-hard problem involves an iterative process that integrates a task planner to compute the goal assignment while ignoring the collision possibilities among the robots and a multi-agent path-finding algorithm to find the collision-free trajectories for a given assignment. This procedure involves a method for computing the next best assignment given the current best assignment. A naive way of computing the next best assignment, as done in the state-of-the-art solutions, becomes a roadblock to achieving scalability in solving the overall problem. To obviate this bottleneck, we propose an efficient conflict-guided method to compute the next best assignment. Additionally, we introduce two more optimizations to the algorithm -- first for avoiding the unconstrained path computations between robot-goal pairs wherever possible, and the second to prevent duplicate constrained path computations for multiple robot-goal pairs. We extensively evaluate our algorithm for up to a hundred robots on several benchmark workspaces. The results demonstrate that the proposed algorithm achieves nearly an order of magnitude speedup over the state-of-the-art algorithm, showcasing its efficacy in real-world scenarios.
<div id='section'>Paperid: <span id='pid'>571, <a href='https://arxiv.org/pdf/2402.13201.pdf' target='_blank'>https://arxiv.org/pdf/2402.13201.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Orhan Eren AkgÃ¼n, NÃ©stor Cuevas, Matheus Farias, Daniel Garces
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.13201">Tiny Reinforcement Learning for Quadruped Locomotion using Decision Transformers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Resource-constrained robotic platforms are particularly useful for tasks that require low-cost hardware alternatives due to the risk of losing the robot, like in search-and-rescue applications, or the need for a large number of devices, like in swarm robotics. For this reason, it is crucial to find mechanisms for adapting reinforcement learning techniques to the constraints imposed by lower computational power and smaller memory capacities of these ultra low-cost robotic platforms. We try to address this need by proposing a method for making imitation learning deployable onto resource-constrained robotic platforms. Here we cast the imitation learning problem as a conditional sequence modeling task and we train a decision transformer using expert demonstrations augmented with a custom reward. Then, we compress the resulting generative model using software optimization schemes, including quantization and pruning. We test our method in simulation using Isaac Gym, a realistic physics simulation environment designed for reinforcement learning. We empirically demonstrate that our method achieves natural looking gaits for Bittle, a resource-constrained quadruped robot. We also run multiple simulations to show the effects of pruning and quantization on the performance of the model. Our results show that quantization (down to 4 bits) and pruning reduce model size by around 30\% while maintaining a competitive reward, making the model deployable in a resource-constrained system.
<div id='section'>Paperid: <span id='pid'>572, <a href='https://arxiv.org/pdf/2402.09382.pdf' target='_blank'>https://arxiv.org/pdf/2402.09382.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Luca Ballotta, Rajat Talak
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.09382">Safe Distributed Control of Multi-Robot Systems with Communication Delays</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Safe operation of multi-robot systems is critical, especially in communication-degraded environments such as underwater for seabed mapping, underground caves for navigation, and in extraterrestrial missions for assembly and construction. We address safety of networked autonomous systems where the information exchanged between robots incurs communication delays. We formalize a notion of distributed control barrier function for multi-robot systems, a safety certificate amenable to a distributed implementation, which provides formal ground to using graph neural networks to learn safe distributed controllers. Further, we observe that learning a distributed controller ignoring delays can severely degrade safety. We finally propose a predictor-based framework to train a safe distributed controller under communication delays, where the current state of nearby robots is predicted from received data and age-of-information. Numerical experiments on multi-robot collision avoidance show that our predictor-based approach can significantly improve the safety of a learned distributed controller under communication delays. A video abstract is available at https://youtu.be/Hcu1Ri32Spk.
<div id='section'>Paperid: <span id='pid'>573, <a href='https://arxiv.org/pdf/2401.13103.pdf' target='_blank'>https://arxiv.org/pdf/2401.13103.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>W. Zhu, S. Oguz, M. K. Heinrich, M. Allwright, M. Wahby, A. Lyhne Christensen, E. Garone, M. Dorigo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.13103">Self-organizing Nervous Systems for Robot Swarms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The system architecture controlling a group of robots is generally set before deployment and can be either centralized or decentralized. This dichotomy is highly constraining, because decentralized systems are typically fully self-organized and therefore difficult to design analytically, whereas centralized systems have single points of failure and limited scalability. To address this dichotomy, we present the Self-organizing Nervous System (SoNS), a novel robot swarm architecture based on self-organized hierarchy. The SoNS approach enables robots to autonomously establish, maintain, and reconfigure dynamic multi-level system architectures. For example, a robot swarm consisting of $n$ independent robots could transform into a single $n$-robot SoNS and then into several independent smaller SoNSs, where each SoNS uses a temporary and dynamic hierarchy. Leveraging the SoNS approach, we show that sensing, actuation, and decision-making can be coordinated in a locally centralized way, without sacrificing the benefits of scalability, flexibility, and fault tolerance, for which swarm robotics is usually studied. In several proof-of-concept robot missions -- including binary decision-making and search-and-rescue -- we demonstrate that the capabilities of the SoNS approach greatly advance the state of the art in swarm robotics. The missions are conducted with a real heterogeneous aerial-ground robot swarm, using a custom-developed quadrotor platform. We also demonstrate the scalability of the SoNS approach in swarms of up to 250 robots in a physics-based simulator, and demonstrate several types of system fault tolerance in simulation and reality.
<div id='section'>Paperid: <span id='pid'>574, <a href='https://arxiv.org/pdf/2312.11802.pdf' target='_blank'>https://arxiv.org/pdf/2312.11802.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sanjay Oruganti, Ramviyas Parasuraman, Ramana Pidaparti
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.11802">IKT-BT: Indirect Knowledge Transfer Behavior Tree Framework for Multi-Robot Systems Through Communication Eavesdropping</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent and multi-robot systems (MRS) often rely on direct communication for information sharing. This work explores an alternative approach inspired by eavesdropping mechanisms in nature that involves casual observation of agent interactions to enhance decentralized knowledge dissemination. We achieve this through a novel IKT-BT framework tailored for a behavior-based MRS, encapsulating knowledge and control actions in Behavior Trees (BT). We present two new BT-based modalities - eavesdrop-update (EU) and eavesdrop-buffer-update (EBU) - incorporating unique eavesdropping strategies and efficient episodic memory management suited for resource-limited swarm robots. We theoretically analyze the IKT-BT framework for an MRS and validate the performance of the proposed modalities through extensive experiments simulating a search and rescue mission. Our results reveal improvements in both global mission performance outcomes and agent-level knowledge dissemination with a reduced need for direct communication.
<div id='section'>Paperid: <span id='pid'>575, <a href='https://arxiv.org/pdf/2312.08047.pdf' target='_blank'>https://arxiv.org/pdf/2312.08047.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pete Schroepfer, Nathalie Schauffel, Jan GrÃ¼ndling, Thomas Ellwart, Benjamin Weyers, CÃ©dric Pradalier
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.08047">Trust and Acceptance of Multi-Robot Systems "in the Wild". A Roadmap exemplified within the EU-Project BugWright2</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper outlines a roadmap to effectively leverage shared mental models in multi-robot, multi-stakeholder scenarios, drawing on experiences from the BugWright2 project. The discussion centers on an autonomous multi-robot systems designed for ship inspection and maintenance. A significant challenge in the development and implementation of this system is the calibration of trust. To address this, the paper proposes that trust calibration can be managed and optimized through the creation and continual updating of shared and accurate mental models of the robots. Strategies to promote these mental models, including cross-training, briefings, debriefings, and task-specific elaboration and visualization, are examined. Additionally, the crucial role of an adaptable, distributed, and well-structured user interface (UI) is discussed.
<div id='section'>Paperid: <span id='pid'>576, <a href='https://arxiv.org/pdf/2311.08227.pdf' target='_blank'>https://arxiv.org/pdf/2311.08227.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Brian McCarthy, Aisling O'Driscoll
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.08227">Prediction of inter packet arrival times for enhanced NR-V2X sidelink scheduling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A significant limitation of the LTE-V2X and NR-V2X sidelink scheduling mechanisms is their difficulty coping with variations in inter packet arrival times, also known as aperiodic packets. This conflicts with the fundamental characteristics of most V2X services which are triggered based on an event. e.g. ETSI Cooperative Awareness Messages (CAMs) - vehicle kinematics, Cooperative Perception Messages (CPMs) - object sensing and Decentralised Event Notification Messages (DENMs) - event occurrences. Furthermore, network management techniques such as congestion control mechanisms can result in varied inter packet arrival times. To combat this, NR-V2X introduced a dynamic grant mechanism, which we show is ineffective unless there is background periodic traffic to stabilise the sensing history upon which the scheduler makes it decisions. The characteristics of V2X services make it implausible that such periodic application traffic will exist.
  To overcome this significant drawback, we demonstrate that the standardised scheduling algorithms can be made effective if the event triggered arrival rate of packets can be accurately predicted. These predictions can be used to tune the Resource Reservation Interval (RRI) parameter of the MAC scheduler to negate the negative impact of aperiodicity. Such an approach allows the scheduler to achieve comparable performance to a scenario where packets arrive periodically. To demonstrate the effectiveness of our approach, an ML model has been devised for the prediction of cooperative awareness messages, but the same principle can be abstracted to other V2X service types.
<div id='section'>Paperid: <span id='pid'>577, <a href='https://arxiv.org/pdf/2311.04126.pdf' target='_blank'>https://arxiv.org/pdf/2311.04126.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Khalid Bourr, Francesco Tiezzi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.04126">From Diagram to Deployment: Translating BPMN Collaborations into X-Klaim for Efficient Multi-Robot System Programming</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces a novel method for translating Business Process Model and Notation (BPMN) diagrams into executable X-Klaim code for Multi-Robot Systems (MRSs). Merging the clarity of BPMN with the operational strength of X-Klaim, we enable the design and execution of complex robotic interactions without requiring in-depth knowledge of the underlying programming language from the users. Our approach maintains the BPMN model's core design principles and logic in the translation to X-Klaim, thus enhancing the readability and maintainability of MRS applications. We offer a series of translated examples, address optimization strategies, and introduce the B2XKLAIM tool, which automates the conversion process. This method aims to streamline MRS programming and improve collaboration between roboticists and domain experts throughout the design and implementation stages.
<div id='section'>Paperid: <span id='pid'>578, <a href='https://arxiv.org/pdf/2310.11843.pdf' target='_blank'>https://arxiv.org/pdf/2310.11843.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jonas Kuckling, Robin Luckey, Viktor Avrutin, Andrew Vardy, Andreagiovanni Reina, Heiko Hamann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.11843">Do We Run Large-scale Multi-Robot Systems on the Edge? More Evidence for Two-Phase Performance in System Size Scaling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With increasing numbers of mobile robots arriving in real-world applications, more robots coexist in the same space, interact, and possibly collaborate. Methods to provide such systems with system size scalability are known, for example, from swarm robotics. Example strategies are self-organizing behavior, a strict decentralized approach, and limiting the robot-robot communication. Despite applying such strategies, any multi-robot system breaks above a certain critical system size (i.e., number of robots) as too many robots share a resource (e.g., space, communication channel). We provide additional evidence based on simulations, that at these critical system sizes, the system performance separates into two phases: nearly optimal and minimal performance. We speculate that in real-world applications that are configured for optimal system size, the supposedly high-performing system may actually live on borrowed time as it is on a transient to breakdown. We provide two modeling options (based on queueing theory and a population model) that may help to support this reasoning.
<div id='section'>Paperid: <span id='pid'>579, <a href='https://arxiv.org/pdf/2310.02343.pdf' target='_blank'>https://arxiv.org/pdf/2310.02343.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sven Signer, Ian Gray
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.02343">Adaptive Application Behaviour for Robot Swarms using Mixed-Criticality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Communication is a vital component for all swarm robotics applications, and even simple swarm robotics behaviours often break down when this communication is unreliable. Since wireless communications are inherently subject to interference and signal degradation, real-world swarm robotics applications will need to be able handle such scenarios. This paper argues for tighter integration of application level and network layer behaviour, so that the application can alter its behaviour in response to a degraded network. This is systematised through the implementation of a mixed-criticality system model. We compare a static application behaviour with that of an application that is able to alter its behaviour in response to the current criticality level of a mixed-criticality wireless protocol. Using simulation results we show that while a static approach is sufficient if the network conditions are known a priori, a mixed-criticality system model is able to adapt application behaviour to better support unseen or unpredictable conditions.
<div id='section'>Paperid: <span id='pid'>580, <a href='https://arxiv.org/pdf/2309.15367.pdf' target='_blank'>https://arxiv.org/pdf/2309.15367.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinran Li, Shuaikang Zheng, Pengcheng Zheng, Haifeng Zhang, Zhitian Li, Xudong Zou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.15367">Analysis on Multi-robot Relative 6-DOF Pose Estimation Error Based on UWB Range</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Relative pose estimation is the foundational requirement for multi-robot system, while it is a challenging research topic in infrastructure-free scenes. In this study, we analyze the relative 6-DOF pose estimation error of multi-robot system in GNSS-denied and anchor-free environment. An analytical lower bound of position and orientation estimation error is given under the assumption that distance between the nodes are far more than the size of robotic platform. Through simulation, impact of distance between nodes, altitudes and circumradius of tag simplex on pose estimation accuracy is discussed, which verifies the analysis results. Our analysis is expected to determine parameters (e.g. deployment of tags) of UWB based multi-robot systems.
<div id='section'>Paperid: <span id='pid'>581, <a href='https://arxiv.org/pdf/2308.11154.pdf' target='_blank'>https://arxiv.org/pdf/2308.11154.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiucheng Wang, Hongzhi Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.11154">Mobility-Aware Computation Offloading for Swarm Robotics using Deep Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Swarm robotics is envisioned to automate a large number of dirty, dangerous, and dull tasks. Robots have limited energy, computation capability, and communication resources. Therefore, current swarm robotics have a small number of robots, which can only provide limited spatio-temporal information. In this paper, we propose to leverage the mobile edge computing to alleviate the computation burden. We develop an effective solution based on a mobility-aware deep reinforcement learning model at the edge server side for computing scheduling and resource. Our results show that the proposed approach can meet delay requirements and guarantee computation precision by using minimum robot energy.
<div id='section'>Paperid: <span id='pid'>582, <a href='https://arxiv.org/pdf/2308.07071.pdf' target='_blank'>https://arxiv.org/pdf/2308.07071.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shreyash Gupta, Abhinav Kumar, Niladri S. Tripathy, Suril V. Shah
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.07071">RL-based Variable Horizon Model Predictive Control of Multi-Robot Systems using Versatile On-Demand Collision Avoidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-robot systems have become very popular in recent years because of their wide spectrum of applications, ranging from surveillance to cooperative payload transportation. Model Predictive Control (MPC) is a promising controller for multi-robot control because of its preview capability and ability to handle constraints easily. The performance of the MPC widely depends on many parameters, among which the prediction horizon is the major contributor. Increasing the prediction horizon beyond a limit drastically increases the computation cost. Tuning the value of the prediction horizon can be very time-consuming, and the tuning process must be repeated for every task. Moreover, instead of using a fixed horizon for an entire task, a better balance between performance and computation cost can be established if different prediction horizons can be employed for every robot at each time step. Further, for such variable prediction horizon MPC for multiple robots, on-demand collision avoidance is the key requirement. We propose Versatile On-demand Collision Avoidance (VODCA) strategy to comply with the variable horizon model predictive control. We also present a framework for learning the prediction horizon for the multi-robot system as a function of the states of the robots using the Soft Actor-Critic (SAC) RL algorithm. The results are illustrated and validated numerically for different multi-robot tasks.
<div id='section'>Paperid: <span id='pid'>583, <a href='https://arxiv.org/pdf/2307.14237.pdf' target='_blank'>https://arxiv.org/pdf/2307.14237.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Karl Mason, Sabine Hauert
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.14237">Evolving Multi-Objective Neural Network Controllers for Robot Swarms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Many swarm robotics tasks consist of multiple conflicting objectives. This research proposes a multi-objective evolutionary neural network approach to developing controllers for swarms of robots. The swarm robot controllers are trained in a low-fidelity Python simulator and then tested in a high-fidelity simulated environment using Webots. Simulations are then conducted to test the scalability of the evolved multi-objective robot controllers to environments with a larger number of robots. The results presented demonstrate that the proposed approach can effectively control each of the robots. The robot swarm exhibits different behaviours as the weighting for each objective is adjusted. The results also confirm that multi-objective neural network controllers evolved in a low-fidelity simulator can be transferred to high-fidelity simulated environments and that the controllers can scale to environments with a larger number of robots without further retraining needed.
<div id='section'>Paperid: <span id='pid'>584, <a href='https://arxiv.org/pdf/2307.06912.pdf' target='_blank'>https://arxiv.org/pdf/2307.06912.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ulrich Dah-Achinanon, Emir Khaled Belhaddad, Guillaume Ricard, Giovanni Beltrame
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.06912">BittyBuzz: A Swarm Robotics Runtime for Tiny Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Swarm robotics is an emerging field of research which is increasingly attracting attention thanks to the advances in robotics and its potential applications. However, despite the enthusiasm surrounding this area of research, software development for swarm robotics is still a tedious task. That fact is partly due to the lack of dedicated solutions, in particular for low-cost systems to be produced in large numbers and that can have important resource constraints. To address this issue, we introduce BittyBuzz, a novel runtime platform: it allows Buzz, a domain-specific language, to run on microcontrollers while maintaining dynamic memory management. BittyBuzz is designed to fit a flash memory as small as 32 kB (with usable space for scripts) and work with as little as 2 kB of RAM. In this work, we introduce the BittyBuzz implementation, its differences from the original Buzz virtual machine, and its advantages for swarm robotics systems. We show that BittyBuzz is successfully integrated with three robotic platforms with minimal memory footprint and conduct experiments to show computation performance of BittyBuzz. Results show that BittyBuzz can be effectively used to implement common swarm behaviors on microcontroller-based systems.
<div id='section'>Paperid: <span id='pid'>585, <a href='https://arxiv.org/pdf/2306.17703.pdf' target='_blank'>https://arxiv.org/pdf/2306.17703.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cagri Kilic, Eduardo Gutierrez, Jason N. Gross
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.17703">Evaluation of the Benefits of Zero Velocity Update in Decentralized EKF-Based Cooperative Localization Algorithms for GNSS-Denied Multi-Robot Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper proposes the cooperative use of zero velocity update (ZU) in a decentralized extended Kalman filter (DEKF) based localization algorithm for multi-robot systems. The filter utilizes inertial measurement unit (IMU), ultra-wideband (UWB), and odometry velocity measurements to improve the localization performance of the system in the presence of a GNSS-denied environment. The contribution of this work is to evaluate the benefits of using ZU in a DEKF-based localization algorithm. The algorithm is tested with real hardware in a video motion capture facility and a Robot Operating System (ROS) based simulation environment for unmanned ground vehicles (UGV). Both simulation and real-world experiments are performed to show the effectiveness of using ZU in one robot to reinstate the localization of other robots in a multi-robot system. Experimental results from GNSS-denied simulation and real-world environments show that using ZU with simple heuristics in the DEKF significantly improves the 3D localization accuracy.
<div id='section'>Paperid: <span id='pid'>586, <a href='https://arxiv.org/pdf/2306.16032.pdf' target='_blank'>https://arxiv.org/pdf/2306.16032.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Katie Clinch, Tony A. Wood, Chris Manzie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.16032">Auction algorithm sensitivity for multi-robot task allocation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We consider the problem of finding a low-cost allocation and ordering of tasks between a team of robots in a d-dimensional, uncertain, landscape, and the sensitivity of this solution to changes in the cost function. Various algorithms have been shown to give a 2-approximation to the MinSum allocation problem. By analysing such an auction algorithm, we obtain intervals on each cost, such that any fluctuation of the costs within these intervals will result in the auction algorithm outputting the same solution.
<div id='section'>Paperid: <span id='pid'>587, <a href='https://arxiv.org/pdf/2306.14489.pdf' target='_blank'>https://arxiv.org/pdf/2306.14489.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Juraj Obradovic, Marko Krizmancic, Stjepan Bogdan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.14489">Decentralized Multi-Robot Formation Control Using Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a decentralized leader-follower multi-robot formation control based on a reinforcement learning (RL) algorithm applied to a swarm of small educational Sphero robots. Since the basic Q-learning method is known to require large memory resources for Q-tables, this work implements the Double Deep Q-Network (DDQN) algorithm, which has achieved excellent results in many robotic problems. To enhance the system behavior, we trained two different DDQN models, one for reaching the formation and the other for maintaining it. The models use a discrete set of robot motions (actions) to adapt the continuous nonlinear system to the discrete nature of RL. The presented approach has been tested in simulation and real experiments which show that the multi-robot system can achieve and maintain a stable formation without the need for complex mathematical models and nonlinear control laws.
<div id='section'>Paperid: <span id='pid'>588, <a href='https://arxiv.org/pdf/2306.11936.pdf' target='_blank'>https://arxiv.org/pdf/2306.11936.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ashay Aswale, Carlo Pinciroli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.11936">Heterogeneous Coalition Formation and Scheduling with Multi-Skilled Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present an approach to task scheduling in heterogeneous multi-robot systems. In our setting, the tasks to complete require diverse skills. We assume that each robot is multi-skilled, i.e., each robot offers a subset of the possible skills. This makes the formation of heterogeneous teams (\emph{coalitions}) a requirement for task completion. We present two centralized algorithms to schedule robots across tasks and to form suitable coalitions, assuming stochastic travel times across tasks. The coalitions are dynamic, in that the robots form and disband coalitions as the schedule is executed. The first algorithm we propose guarantees optimality, but its run-time is acceptable only for small problem instances. The second algorithm we propose can tackle large problems with short run-times, and is based on a heuristic approach that typically reaches 1x-2x of the optimal solution cost.
<div id='section'>Paperid: <span id='pid'>589, <a href='https://arxiv.org/pdf/2306.07385.pdf' target='_blank'>https://arxiv.org/pdf/2306.07385.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Max Rudolph, Sean Wilson, Magnus Egerstedt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.07385">Range Limited Coverage Control using Air-Ground Multi-Robot Teams</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we investigate how heterogeneous multi-robot systems with different sensing capabilities can observe a domain with an apriori unknown density function. Common coverage control techniques are targeted towards homogeneous teams of robots and do not consider what happens when the sensing capabilities of the robots are vastly different. This work proposes an extension to Lloyd's algorithm that fuses coverage information from heterogeneous robots with differing sensing capabilities to effectively observe a domain. Namely, we study a bimodal team of robots consisting of aerial and ground agents. In our problem formulation we use aerial robots with coarse domain sensors to approximate the number of ground robots needed within their sensing region to effectively cover it. This information is relayed to ground robots, who perform an extension to the Lloyd's algorithm that balances a locally focused coverage controller with a globally focused distribution controller. The stability of the Lloyd's algorithm extension is proven and its performance is evaluated through simulation and experiments using the Robotarium, a remotely-accessible, multi-robot testbed.
<div id='section'>Paperid: <span id='pid'>590, <a href='https://arxiv.org/pdf/2305.17018.pdf' target='_blank'>https://arxiv.org/pdf/2305.17018.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Charlie Street, Masoumeh Mansouri, Bruno Lacerda
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.17018">Formal Modelling for Multi-Robot Systems Under Uncertainty</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Purpose of Review: To effectively synthesise and analyse multi-robot behaviour, we require formal task-level models which accurately capture multi-robot execution. In this paper, we review modelling formalisms for multi-robot systems under uncertainty, and discuss how they can be used for planning, reinforcement learning, model checking, and simulation.
  Recent Findings: Recent work has investigated models which more accurately capture multi-robot execution by considering different forms of uncertainty, such as temporal uncertainty and partial observability, and modelling the effects of robot interactions on action execution. Other strands of work have presented approaches for reducing the size of multi-robot models to admit more efficient solution methods. This can be achieved by decoupling the robots under independence assumptions, or reasoning over higher level macro actions.
  Summary: Existing multi-robot models demonstrate a trade off between accurately capturing robot dependencies and uncertainty, and being small enough to tractably solve real world problems. Therefore, future research should exploit realistic assumptions over multi-robot behaviour to develop smaller models which retain accurate representations of uncertainty and robot interactions; and exploit the structure of multi-robot problems, such as factored state spaces, to develop scalable solution methods.
<div id='section'>Paperid: <span id='pid'>591, <a href='https://arxiv.org/pdf/2304.09741.pdf' target='_blank'>https://arxiv.org/pdf/2304.09741.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yufan Huang, Man Li, Tao Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.09741">A Multi-robot Coverage Path Planning Algorithm Based on Improved DARP Algorithm</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The research on multi-robot coverage path planning (CPP) has been attracting more and more attention. In order to achieve efficient coverage, this paper proposes an improved DARP coverage algorithm. The improved DARP algorithm based on A* algorithm is used to assign tasks to robots and then combined with STC algorithm based on Up-First algorithm to achieve full coverage of the task area. Compared with the initial DARP algorithm, this algorithm has higher efficiency and higher coverage rate.
<div id='section'>Paperid: <span id='pid'>592, <a href='https://arxiv.org/pdf/2304.07651.pdf' target='_blank'>https://arxiv.org/pdf/2304.07651.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Eugene M. Taranta, Adam Seiwert, Anthony Goeckner, Khiem Nguyen, Erin Cherry
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.07651">From Warfighting Needs to Robot Actuation: A Complete Rapid Integration Swarming Solution</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Swarm robotics systems have the potential to transform warfighting in urban environments, but until now have not seen large-scale field testing. We present the Rapid Integration Swarming Ecosystem (RISE), a platform for future multi-agent research and deployment. RISE enables rapid integration of third-party swarm tactics and behaviors, which was demonstrated using both physical and simulated swarms. Our physical testbed is composed of more than 250 networked heterogeneous agents and has been extensively tested in mock warfare scenarios at five urban combat training ranges. RISE implements live, virtual, constructive simulation capabilities to allow the use of both virtual and physical agents simultaneously, while our "fluid fidelity" simulation enables adaptive scaling between low and high fidelity simulation levels based on dynamic runtime requirements. Both virtual and physical agents are controlled with a unified gesture-based interface that enables a greater than 150:1 agent-to-operator ratio. Through this interface, we enable efficient swarm-based mission execution. RISE translates mission needs to robot actuation with rapid tactic integration, a reliable testbed, and efficient operation.
<div id='section'>Paperid: <span id='pid'>593, <a href='https://arxiv.org/pdf/2304.03125.pdf' target='_blank'>https://arxiv.org/pdf/2304.03125.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Baudouin Saintyves, Matthew Spenko, Heinrich M. Jaeger
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.03125">A self-organizing robotic aggregate using solid and liquid-like collective states</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Designing robotic systems that can change their physical form factor as well as their compliance to adapt to environmental constraints remains a major conceptual and technical challenge. To address this, we introduce the Granulobot, a modular system that blurs the distinction between soft, modular, and swarm robotics. The system consists of gear-like units that each contain a single actuator such that units can self-assemble into larger, granular aggregates using magnetic coupling. These aggregates can reconfigure dynamically and also split up into subsystems that might later recombine. Aggregates can self-organize into collective states with solid- and liquid-like properties, thus displaying widely differing compliances. These states can be perturbed locally via actuators or externally via mechanical feedback from the environment to produce adaptive shape shifting in a decentralized manner. This in turn can generate locomotion strategies adapted to different conditions. Aggregates can move over obstacles without using external sensors or coordinate to maintain a steady gait over different surfaces without electronic communication among units. The modular design highlights a physical, morphological form of control that advances the development of resilient robotic systems with the ability to morph and adapt to different functions and conditions.
<div id='section'>Paperid: <span id='pid'>594, <a href='https://arxiv.org/pdf/2303.11443.pdf' target='_blank'>https://arxiv.org/pdf/2303.11443.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Salma Ichekhlef, Ãtienne Villemure, Shokoufeh Naderi, FranÃ§ois Ferland, Maude Blondin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.11443">Relative ultra-wideband based localization of multi-robot systems with kinematic extended Kalman filter</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Localization plays a critical role in the field of distributed swarm robotics. Previous work has highlighted the potential of relative localization for position tracking in multi-robot systems. Ultra-wideband (UWB) technology provides a good estimation of the relative position between robots but suffers from some limitations. This paper proposes improving the relative localization functionality developed in our previous work, which is based on UWB technology. Our new approach merges UWB telemetry and kinematic model into an extended Kalman filter to properly track the relative position of robots. We performed a simulation and validated the improvements in relative distance and angle accuracy for the proposed approach. An additional analysis was conducted to observe the increase in performance when the robots share their control inputs.
<div id='section'>Paperid: <span id='pid'>595, <a href='https://arxiv.org/pdf/2303.08935.pdf' target='_blank'>https://arxiv.org/pdf/2303.08935.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ahmad Bilal Asghar, Shreyas Sundaram, Stephen L. Smith
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.08935">Multi-Robot Persistent Monitoring: Minimizing Latency and Number of Robots with Recharging Constraints</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper we study multi-robot path planning for persistent monitoring tasks. We consider the case where robots have a limited battery capacity with a discharge time $D$. We represent the areas to be monitored as the vertices of a weighted graph. For each vertex, there is a constraint on the maximum allowable time between robot visits, called the latency. The objective is to find the minimum number of robots that can satisfy these latency constraints while also ensuring that the robots periodically charge at a recharging depot. The decision version of this problem is known to be PSPACE-complete. We present a $O(\frac{\log D}{\log \log D}\log Ï)$ approximation algorithm for the problem where $Ï$ is the ratio of the maximum and the minimum latency constraints. We also present an orienteering based heuristic to solve the problem and show empirically that it typically provides higher quality solutions than the approximation algorithm. We extend our results to provide an algorithm for the problem of minimizing the maximum weighted latency given a fixed number of robots. We evaluate our algorithms on large problem instances in a patrolling scenario and in a wildfire monitoring application. We also compare the algorithms with an existing solver on benchmark instances.
<div id='section'>Paperid: <span id='pid'>596, <a href='https://arxiv.org/pdf/2302.08447.pdf' target='_blank'>https://arxiv.org/pdf/2302.08447.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhan Gao, Deniz Gunduz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.08447">Graph Neural Networks over the Air for Decentralized Tasks in Wireless Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Graph neural networks (GNNs) model representations from networked data and allow for decentralized inference through localized communications. Existing GNN architectures often assume ideal communications and ignore potential channel effects, such as fading and noise, leading to performance degradation in real-world implementation. Considering a GNN implemented over nodes connected through wireless links, this paper conducts a stability analysis to study the impact of channel impairments on the performance of GNNs, and proposes graph neural networks over the air (AirGNNs), a novel GNN architecture that incorporates the communication model. AirGNNs modify graph convolutional operations that shift graph signals over random communication graphs to take into account channel fading and noise when aggregating features from neighbors, thus, improving architecture robustness to channel impairments during testing. We develop a channel-inversion signal transmission strategy for AirGNNs when channel state information (CSI) is available, and propose a stochastic gradient descent based method to train AirGNNs when CSI is unknown. The convergence analysis shows that the training procedure approaches a stationary solution of an associated stochastic optimization problem and the variance analysis characterizes the statistical behavior of the trained model. Experiments on decentralized source localization and multi-robot flocking corroborate theoretical findings and show superior performance of AirGNNs over wireless communication channels.
<div id='section'>Paperid: <span id='pid'>597, <a href='https://arxiv.org/pdf/2301.01250.pdf' target='_blank'>https://arxiv.org/pdf/2301.01250.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maxime Chaveroche, Franck Davoine, VÃ©ronique Cherfaoui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.01250">Decentralized cooperative perception for autonomous vehicles: Learning to value the unknown</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, we have been witnesses of accidents involving autonomous vehicles and their lack of sufficient information. One way to tackle this issue is to benefit from the perception of different view points, namely cooperative perception. We propose here a decentralized collaboration, i.e. peer-to-peer, in which the agents are active in their quest for full perception by asking for specific areas in their surroundings on which they would like to know more. Ultimately, we want to optimize a trade-off between the maximization of knowledge about moving objects and the minimization of the total volume of information received from others, to limit communication costs and message processing time. For this, we propose a way to learn a communication policy that reverses the usual communication paradigm by only requesting from other vehicles what is unknown to the ego-vehicle, instead of filtering on the sender side. We tested three different generative models to be taken as base for a Deep Reinforcement Learning (DRL) algorithm, and compared them to a broadcasting policy and a policy randomly selecting areas. In particular, we propose Locally Predictable VAE (LP-VAE), which appears to be producing better belief states for predictions than state-of-the-art models, both as a standalone model and in the context of DRL. Experiments were conducted in the driving simulator CARLA. Our best models reached on average a gain of 25% of the total complementary information, while only requesting about 5% of the ego-vehicle's perceptual field. This trade-off is adjustable through the interpretable hyperparameters of our reward function.
<div id='section'>Paperid: <span id='pid'>598, <a href='https://arxiv.org/pdf/2212.12724.pdf' target='_blank'>https://arxiv.org/pdf/2212.12724.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tony A. Wood, Maryam Kamgarpour
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.12724">Certification of Bottleneck Task Assignment with Shortest Path Criteria</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Minimising the longest travel distance for a group of mobile robots with interchangeable goals requires knowledge of the shortest length paths between all robots and goal destinations. Determining the exact length of the shortest paths in an environment with obstacles is NP-hard however. In this paper, we investigate when polynomial-time approximations of the shortest path search are sufficient to determine the optimal assignment of robots to goals. In particular, we propose an algorithm in which the accuracy of the path planning is iteratively increased. The approach provides a certificate when the uncertainties on estimates of the shortest paths become small enough to guarantee the optimality of the goal assignment. To this end, we apply results from assignment sensitivity assuming upper and lower bounds on the length of the shortest paths. We then provide polynomial-time methods to find such bounds by applying sampling-based path planning. The upper bounds are given by feasible paths, the lower bounds are obtained by expanding the sample set and leveraging the knowledge of the sample dispersion. We demonstrate the application of the proposed method with a multi-robot path-planning case study.
<div id='section'>Paperid: <span id='pid'>599, <a href='https://arxiv.org/pdf/2202.00756.pdf' target='_blank'>https://arxiv.org/pdf/2202.00756.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Justin Cano, Jerome Le Ny
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2202.00756">Ranging-Based Localizability Optimization for Mobile Robotic Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In robotic networks relying on noisy range measurements between agents for cooperative localization, the achievable positioning accuracy strongly strongly depends on the network geometry. This motivates the problem of planning robot trajectories in such multi-robot systems in a way that maintains high localization accuracy. We present potential-based planning methods, where localizability potentials are introduced to characterize the quality of the network geometry for cooperative position estimation. These potentials are based on Cramer Rao Lower Bounds (CRLB) and provide a theoretical lower bound on the error covariance achievable by any unbiased position estimator. In the process, we establish connections between CRLBs and the theory of graph rigidity, which has been previously used to plan the motion of robotic networks. We develop decentralized deployment algorithms appropriate for large networks, and we use equality-constrained CRLBs to extend the concept of localizability to scenarios where additional information about the relative positions of the ranging sensors is known. We illustrate the resulting robot deployment methodology through simulated examples and an experiment.
<div id='section'>Paperid: <span id='pid'>600, <a href='https://arxiv.org/pdf/2112.10578.pdf' target='_blank'>https://arxiv.org/pdf/2112.10578.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Athanasios Lentzas, Dimitris Vrakas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2112.10578">From Robot Self-Localization to Global-Localization: An RSSI Based Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Localization is a crucial task for autonomous mobile robots in order to successfully move to goal locations in their environment. Usually, this is done in a robot-centric manner, where the robot maintains a map with its body in the center. In swarm robotics applications, where a group of robots needs to coordinate in order to achieve their common goals, robot-centric localization will not suffice as each member of the swarm has its own frame of reference. One way to deal with this problem is to create, maintain and share a common map (global coordinate system), among the members of the swarm. This paper presents an approach to global localization for a group of robots in unknown, GPS and landmark free environments that extends the localization scheme of the LadyBug algorithm. The main idea relies on members of the swarm staying still and acting as beacons, emitting electromagnetic signals. These stationary robots form a global frame of reference and the rest of the group localize themselves in it using the Received Signal Strength Indicator (RSSI). The proposed method is evaluated, and the results obtained from the experiments are promising.
<div id='section'>Paperid: <span id='pid'>601, <a href='https://arxiv.org/pdf/2103.10316.pdf' target='_blank'>https://arxiv.org/pdf/2103.10316.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rohith G, Madhu Vadali
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2103.10316">A Quasi-centralized Collision-free Path Planning Approach for Multi-Robot Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a novel quasi-centralized approach for collision-free path planning of multi-robot systems (MRS) in obstacle-ridden environments. A new formation potential fields (FPF) concept is proposed around a virtual agent, located at the center of the formation which ensures self-organization and maintenance of the formation. The path of the virtual agent is centrally planned and the robots at the minima of the FPF are forced to move along with the virtual agent. In the neighborhood of obstacles, individual robots selfishly avoid collisions, thus marginally deviating from the formation. The proposed quasi-centralized approach introduces formation flexibility into the MRS, which enables MRS to effectively navigate in an obstacle-ridden workspace. Methodical analysis of the proposed approach and guidelines for selecting the FPF are presented. Results using a candidate FPF are shown that ensure a pentagonal formation effectively squeezes through a narrow passage avoiding any collisions with the walls.
<div id='section'>Paperid: <span id='pid'>602, <a href='https://arxiv.org/pdf/1904.09266.pdf' target='_blank'>https://arxiv.org/pdf/1904.09266.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Eduardo CastellÃ³ Ferrer, Thomas Hardjono, Alex 'Sandy' Pentland, Marco Dorigo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/1904.09266">Secure and secret cooperation in robotic swarms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The importance of swarm robotics systems in both academic research and real-world applications is steadily increasing. However, to reach widespread adoption, new models that ensure the secure cooperation of large groups of robots need to be developed. This work introduces a novel method to encapsulate cooperative robotic missions in an authenticated data structure known as Merkle tree. With this method, operators can provide the "blueprint" of the swarm's mission without disclosing its raw data. In other words, data verification can be separated from data itself. We propose a system where robots in a swarm, to cooperate towards mission completion, have to "prove" their integrity to their peers by exchanging cryptographic proofs. We show the implications of this approach for two different swarm robotics missions: foraging and maze formation. In both missions, swarm robots were able to cooperate and carry out sequential operations without having explicit knowledge about the mission's high-level objectives. The results presented in this work demonstrate the feasibility of using Merkle trees as a cooperation mechanism for swarm robotics systems in both simulation and real-robot experiments, which has implications for future decentralized robotics applications where security plays a crucial role such as environmental monitoring, infrastructure surveillance, and disaster management.
<div id='section'>Paperid: <span id='pid'>603, <a href='https://arxiv.org/pdf/2508.08473.pdf' target='_blank'>https://arxiv.org/pdf/2508.08473.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hossein B. Jond
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08473">A Minimal Model for Emergent Collective Behaviors in Autonomous Robotic Multi-Agent Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Collective behaviors such as swarming and flocking emerge from simple, decentralized interactions in biological systems. Existing models, such as Vicsek and Cucker-Smale, lack collision avoidance, whereas the Olfati-Saber model imposes rigid formations, limiting their applicability in swarm robotics. To address these limitations, this paper proposes a minimal yet expressive model that governs agent dynamics using relative positions, velocities, and local density, modulated by two tunable parameters: the spatial offset and kinetic offset. The model achieves spatially flexible, collision-free behaviors that reflect naturalistic group dynamics. Furthermore, we extend the framework to cognitive autonomous systems, enabling energy-aware phase transitions between swarming and flocking through adaptive control parameter tuning. This cognitively inspired approach offers a robust foundation for real-world applications in multi-robot systems, particularly autonomous aerial swarms.
<div id='section'>Paperid: <span id='pid'>604, <a href='https://arxiv.org/pdf/2508.00967.pdf' target='_blank'>https://arxiv.org/pdf/2508.00967.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Massoud Pourmandi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.00967">Cooperative Perception: A Resource-Efficient Framework for Multi-Drone 3D Scene Reconstruction Using Federated Diffusion and NeRF</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The proposal introduces an innovative drone swarm perception system that aims to solve problems related to computational limitations and low-bandwidth communication, and real-time scene reconstruction. The framework enables efficient multi-agent 3D/4D scene synthesis through federated learning of shared diffusion model and YOLOv12 lightweight semantic extraction and local NeRF updates while maintaining privacy and scalability. The framework redesigns generative diffusion models for joint scene reconstruction, and improves cooperative scene understanding, while adding semantic-aware compression protocols. The approach can be validated through simulations and potential real-world deployment on drone testbeds, positioning it as a disruptive advancement in multi-agent AI for autonomous systems.
<div id='section'>Paperid: <span id='pid'>605, <a href='https://arxiv.org/pdf/2507.07302.pdf' target='_blank'>https://arxiv.org/pdf/2507.07302.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ashish Kumar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07302">Application of LLMs to Multi-Robot Path Planning and Task Allocation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Efficient exploration is a well known problem in deep reinforcement learning and this problem is exacerbated in multi-agent reinforcement learning due the intrinsic complexities of such algorithms. There are several approaches to efficiently explore an environment to learn to solve tasks by multi-agent operating in that environment, of which, the idea of expert exploration is investigated in this work. More specifically, this work investigates the application of large-language models as expert planners for efficient exploration in planning based tasks for multiple agents.
<div id='section'>Paperid: <span id='pid'>606, <a href='https://arxiv.org/pdf/2506.09914.pdf' target='_blank'>https://arxiv.org/pdf/2506.09914.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Teng Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.09914">From Theory to Practice: Advancing Multi-Robot Path Planning Algorithms and Applications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The labeled MRPP (Multi-Robot Path Planning) problem involves routing robots from start to goal configurations efficiently while avoiding collisions. Despite progress in solution quality and runtime, its complexity and industrial relevance continue to drive research.
  This dissertation introduces scalable MRPP methods with provable guarantees and practical heuristics. First, we study dense MRPP on 2D grids, relevant to warehouse and parcel systems. We propose the Rubik Table method, achieving $(1 + Î´)$-optimal makespan (with $Î´\in (0, 0.5]$) for up to $\frac{m_1 m_2}{2}$ robots, solving large instances efficiently and setting a new theoretical benchmark.
  Next, we address real-world MRPP. We design optimal layouts for structured environments (e.g., warehouses, parking systems) and propose a puzzle-based system for dense, deadlock-free autonomous vehicle parking. We also extend MRPP to Reeds-Shepp robots, introducing motion primitives and smoothing techniques to ensure feasible, efficient paths under nonholonomic constraints. Simulations and real-world tests validate the approach in urban driving and robotic transport scenarios.
<div id='section'>Paperid: <span id='pid'>607, <a href='https://arxiv.org/pdf/2504.09577.pdf' target='_blank'>https://arxiv.org/pdf/2504.09577.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Michael P. Wozniak
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.09577">Unification of Consensus-Based Multi-Objective Optimization and Multi-Robot Path Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent systems seeking consensus may also have other objective functions to optimize, requiring the research of multi-objective optimization in consensus. Several recent publications have explored this domain using various methods such as weighted-sum optimization and penalization methods. This paper reviews the state of the art for consensus-based multi-objective optimization, poses a multi-agent lunar rover exploration problem seeking consensus and maximization of explored area, and achieves optimal edge weights and steering angles by applying SQP algorithms.
<div id='section'>Paperid: <span id='pid'>608, <a href='https://arxiv.org/pdf/2501.01531.pdf' target='_blank'>https://arxiv.org/pdf/2501.01531.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Logan Beaver
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.01531">A Global Games-Inspired Approach to Multi-Robot Task Allocation for Heterogeneous Teams</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this article we propose a game-theoretic approach to the multi-robot task allocation problem using the framework of global games. Each task is associated with a global signal, a real-valued number that captures the task execution progress and/or urgency. We propose a linear objective function for each robot in the system, which, for each task, increases with global signal and decreases with the number assigned robots. We provide conditions on the objective function hyperparameters to induce a mixed Nash equilibrium, i.e., solutions where all robots are not assigned to a single task. The resulting algorithm only requires the inversion of a matrix to determine a probability distribution over the robot assignments. We demonstrate the performance of our algorithm in simulation and provide direction for applications and future work.
<div id='section'>Paperid: <span id='pid'>609, <a href='https://arxiv.org/pdf/2501.00110.pdf' target='_blank'>https://arxiv.org/pdf/2501.00110.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andrea Giusti
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.00110">Modelling and Control of Spatial Behaviours in Multi-Agent Systems with Applications to Biology and Robotics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large-Scale Multi-Agent Systems (LS-MAS) consist of several autonomous components, interacting in a non-trivial way, so that the emerging behaviour of the ensemble depends on the individual dynamics of the components and their reciprocal interactions. These models can describe a rich variety of natural systems, as well as artificial ones, characterised by unparalleled scalability, robustness, and flexibility. Indeed, a crucial objective is devising efficient strategies to model and control the spatial behaviours of LS-MAS to achieve specific goals. However, the inherent complexity of these systems and the wide spectrum of their emerging behaviours pose significant challenges. The overarching goal of this thesis is, therefore, to advance methods for modelling, analyzing and controlling the spatial behaviours of LS-MAS, with applications to cellular populations and swarm robotics. The thesis begins with an overview of the existing Literature, and is then organized into two distinct parts. In the context of swarm robotics, Part I deals with distributed control algorithms to spatially organize agents on geometric patterns. The contribution is twofold, encompassing both the development of original control algorithms, and providing a novel formal analysis, which allows to guarantee the emergence of specific geometric patterns. In Part II, looking at the spatial behaviours of biological agents, experiments are carried out to study the movement of microorganisms and their response to light stimuli. This allows the derivation and parametrization of mathematical models that capture these behaviours, and pave the way for the development of innovative approaches for the spatial control of microorganisms. The results presented in the thesis were developed by leveraging formal analytical tools, simulations, and experiments, using innovative platforms and original computational frameworks.
<div id='section'>Paperid: <span id='pid'>610, <a href='https://arxiv.org/pdf/2412.19942.pdf' target='_blank'>https://arxiv.org/pdf/2412.19942.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>James O'Keeffe
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.19942">Detecting and Diagnosing Faults in Autonomous Robot Swarms with an Artificial Antibody Population Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>An active approach to fault tolerance, the combined processes of fault detection, diagnosis, and recovery, is essential for long term autonomy in robots -- particularly multi-robot systems and swarms. Previous efforts have primarily focussed on spontaneously occurring electro-mechanical failures in the sensors and actuators of a minority sub-population of robots. While the systems that enable this function are valuable, they have not yet considered that many failures arise from gradual wear and tear with continued operation, and that this may be more challenging to detect than sudden step changes in performance. This paper presents the Artificial Antibody Population Dynamics (AAPD) model -- an immune-inspired model for the detection and diagnosis of gradual degradation in robot swarms. The AAPD model is demonstrated to reliably detect and diagnose gradual degradation, as well as spontaneous changes in performance, among swarms of robots of varying sizes while remaining tolerant of normally behaving robots. The AAPD model is distributed, offers supervised and unsupervised configurations, and demonstrates promising scalable properties. Deploying the AAPD model on a swarm of foraging robots undergoing gradual degradation enables the swarm to operate on average at between 70% - 97% of its performance in perfect conditions and is able to prevent instances of robots failing in the field during experiments in most of the cases tested.
<div id='section'>Paperid: <span id='pid'>611, <a href='https://arxiv.org/pdf/2411.15953.pdf' target='_blank'>https://arxiv.org/pdf/2411.15953.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ankit Shaw
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.15953">Autonomous Multi-Robot Exploration Strategies for 3D Environments with Fire Detection Capabilitie</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a comprehensive overview of exploration strategies utilized in both 2D and 3D environments, focusing on autonomous multi-robot systems designed for building exploration and fire detection. We explore the limitations of traditional algorithms that rely on prior knowledge and predefined maps, emphasizing the challenges faced when environments undergo changes that invalidate these maps. Our modular approach integrates localization, mapping, and trajectory planning to facilitate effective exploration using an OctoMap framework generated from point cloud data. The exploration strategy incorporates obstacle avoidance through potential fields, ensuring safe navigation in dynamic settings. Additionally, I propose future research directions, including decentralized map creation, coordinated exploration among unmanned aerial vehicles (UAVs), and adaptations to time-varying environments. This work serves as a foundation for advancing coordinated multi-robot exploration algorithms, enhancing their applicability in real-world scenarios.
<div id='section'>Paperid: <span id='pid'>612, <a href='https://arxiv.org/pdf/2411.03203.pdf' target='_blank'>https://arxiv.org/pdf/2411.03203.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Elena Tonini
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.03203">Statistical Analysis to Support CSI-Based Sensing Methods</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Building upon the foundational work of the Bachelor's Degree Thesis titled "Analysis and Characterization of Wi-Fi Channel State Information'', this thesis significantly advances the research by conducting an in-depth analysis of CSIs, offering new insights that extend well beyond the original study. The goal of this work is to broaden the mathematical and statistical representation of a wireless channel through the study of CSI behavior and evolution over time and frequency.
  CSI provides a high-level description of the behavior of a signal propagating from a transmitter to a receiver, thereby representing the structure of the environment where the signal propagates. This knowledge can be used to perform ambvient sensing, a technique that extracts relevant information about the surroundings of the receiver from the properties of the received signal, which are affected by interactions with the surfaces of the objects within the analyzed environment. Ambient sensing already plays an essential role in new wireless networks such as 5G and Beyond 5G; its use Joint Communication and Sensing applications and for the optimization of signal propagation through beamforming could also enable the implementation of efficient cooperative ambient sensing in vehicular networks, facilitating Cooperative Perception and, consequently, increasing road safety.
  Due to the lack of research on CSI characterization, this study aims to begin analyzing the structure of CSI traces collected in a controlled experimental environment and to describe their statistical properties. The results of such characterization could provide mathematical support for environment classification and movement recognition tasks that are currently performed only through Machine Learning techniques, introducing instead efficient, dedicated algorithms.
<div id='section'>Paperid: <span id='pid'>613, <a href='https://arxiv.org/pdf/2410.03753.pdf' target='_blank'>https://arxiv.org/pdf/2410.03753.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jushan Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.03753">A Brief Tutorial on Consensus ADMM for Distributed Optimization with Applications in Robotics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a tutorial on the Consensus Alternating Direction Method of Multipliers (Consensus ADMM) for distributed optimization, with a specific focus on applications in multi-robot systems. In this tutorial, we derive the consensus ADMM algorithm, highlighting its connections to the augmented Lagrangian and primal-dual methods. Finally, we apply Consensus ADMM to an example problem for trajectory optimization of a multi-agent system.
<div id='section'>Paperid: <span id='pid'>614, <a href='https://arxiv.org/pdf/2409.04230.pdf' target='_blank'>https://arxiv.org/pdf/2409.04230.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Inmo Jang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.04230">SPACE: A Python-based Simulator for Evaluating Decentralized Multi-Robot Task Allocation Algorithms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Swarm robotics explores the coordination of multiple robots to achieve collective goals, with collective decision-making being a central focus. This process involves decentralized robots autonomously making local decisions and communicating them, which influences the overall emergent behavior. Testing such decentralized algorithms in real-world scenarios with hundreds or more robots is often impractical, underscoring the need for effective simulation tools. We propose SPACE (Swarm Planning and Control Evaluation), a Python-based simulator designed to support the research, evaluation, and comparison of decentralized Multi-Robot Task Allocation (MRTA) algorithms. SPACE streamlines core algorithmic development by allowing users to implement decision-making algorithms as Python plug-ins, easily construct agent behavior trees via an intuitive GUI, and leverage built-in support for inter-agent communication and local task awareness. To demonstrate its practical utility, we implement and evaluate CBBA and GRAPE within the simulator, comparing their performance across different metrics, particularly in scenarios with dynamically introduced tasks. This evaluation shows the usefulness of SPACE in conducting rigorous and standardized comparisons of MRTA algorithms, helping to support future research in the field.
<div id='section'>Paperid: <span id='pid'>615, <a href='https://arxiv.org/pdf/2409.01504.pdf' target='_blank'>https://arxiv.org/pdf/2409.01504.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.01504">Situation-aware Autonomous Driving Decision Making with Cooperative Perception on Demand</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper investigates the impact of cooperative perception on autonomous driving decision making on urban roads. The extended perception range contributed by the cooperative perception can be properly leveraged to address the implicit dependencies within the vehicles, thereby the vehicle decision making performance can be improved. Meanwhile, we acknowledge the inherent limitation of wireless communication and propose a Cooperative Perception on Demand (CPoD) strategy, where the cooperative perception will only be activated when the extended perception range is necessary for proper situation-awareness. The situation-aware decision making with CPoD is modeled as a Partially Observable Markov Decision Process (POMDP) and solved in an online manner. The evaluation results demonstrate that the proposed approach can function safely and efficiently for autonomous driving on urban roads.
<div id='section'>Paperid: <span id='pid'>616, <a href='https://arxiv.org/pdf/2408.14039.pdf' target='_blank'>https://arxiv.org/pdf/2408.14039.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bharath Rajiv Nair
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.14039">Collaborative Perception in Multi-Robot Systems: Case Studies in Household Cleaning and Warehouse Operations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper explores the paradigm of Collaborative Perception (CP), where multiple robots and sensors in the environment share and integrate sensor data to construct a comprehensive representation of the surroundings. By aggregating data from various sensors and utilizing advanced algorithms, the collaborative perception framework improves task efficiency, coverage, and safety. Two case studies are presented to showcase the benefits of collaborative perception in multi-robot systems. The first case study illustrates the benefits and advantages of using CP for the task of household cleaning with a team of cleaning robots. The second case study performs a comparative analysis of the performance of CP versus Standalone Perception (SP) for Autonomous Mobile Robots operating in a warehouse environment. The case studies validate the effectiveness of CP in enhancing multi-robot coordination, task completion, and overall system performance and its potential to impact operations in other applications as well. Future investigations will focus on optimizing the framework and validating its performance through empirical testing.
<div id='section'>Paperid: <span id='pid'>617, <a href='https://arxiv.org/pdf/2405.02133.pdf' target='_blank'>https://arxiv.org/pdf/2405.02133.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tanja Katharina Kaiser
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.02133">Learning from Evolution: Improving Collective Decision-Making Mechanisms using Insights from Evolutionary Robotics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Collective decision-making enables multi-robot systems to act autonomously in real-world environments. Existing collective decision-making mechanisms suffer from the so-called speed versus accuracy trade-off or rely on high complexity, e.g., by including global communication. Recent work has shown that more efficient collective decision-making mechanisms based on artificial neural networks can be generated using methods from evolutionary computation. A major drawback of these decision-making neural networks is their limited interpretability. Analyzing evolved decision-making mechanisms can help us improve the efficiency of hand-coded decision-making mechanisms while maintaining a higher interpretability. In this paper, we analyze evolved collective decision-making mechanisms in detail and hand-code two new decision-making mechanisms based on the insights gained. In benchmark experiments, we show that the newly implemented collective decision-making mechanisms are more efficient than the state-of-the-art collective decision-making mechanisms voter model and majority rule.
<div id='section'>Paperid: <span id='pid'>618, <a href='https://arxiv.org/pdf/2403.15621.pdf' target='_blank'>https://arxiv.org/pdf/2403.15621.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Logan E. Beaver
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.15621">Multi-Robot Task Allocation using Global Games with Negative Feedback: The Colony Maintenance Problem</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this article we address the multi-robot task allocation problem, where robots must cooperatively assign themselves to accomplish a set of tasks. We consider the colony maintenance problem as an example, where a team of robots are tasked with continuously maintaining the energy supply of a central colony. We model this as a global game, where each robot measures the energy level of the colony, and the current number of assigned robots, to determine whether or not to forage for energy sources. The key to our approach is introducing a negative feedback term into the robots' utility, which also eliminates the trivial solution where foraging or not foraging are strictly dominant strategies. We compare our approach qualitatively to existing an global games approach, where a positive positive feedback term admits threshold-based decision making that encourages many robots to forage. We discuss how positive feedback can lead to a cascading failure when robots are removed from the system, and we demonstrate the resilience of our approach in simulation.
<div id='section'>Paperid: <span id='pid'>619, <a href='https://arxiv.org/pdf/2312.16273.pdf' target='_blank'>https://arxiv.org/pdf/2312.16273.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Luis Paulo Reis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.16273">Coordination and Machine Learning in Multi-Robot Systems: Applications in Robotic Soccer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents the concepts of Artificial Intelligence, Multi-Agent-Systems, Coordination, Intelligent Robotics and Deep Reinforcement Learning. Emphasis is given on and how AI and DRL, may be efficiently used to create efficient robot skills and coordinated robotic teams, capable of performing very complex actions and tasks, such as playing a game of soccer. The paper also presents the concept of robotic soccer and the vision and structure of the RoboCup initiative with emphasis on the Humanoid Simulation 3D league and the new challenges this competition, poses. The final topics presented at the paper are based on the research developed/coordinated by the author throughout the last 22 years in the context of the FCPortugal project. The paper presents a short description of the coordination methodologies developed, such as: Strategy, Tactics, Formations, Setplays, and Coaching Languages and the use of Machine Learning to optimize the use of this concepts. The topics presented also include novel stochastic search algorithms for black box optimization and their use in the optimization of omnidirectional walking skills, robotic multi-agent learning and the creation of a humanoid kick with controlled distance. Finally, new applications using variations of the Proximal Policy Optimization algorithm and advanced modelling for robot and multi-robot learning are briefly explained with emphasis for our new humanoid sprinting and running skills and an amazing humanoid robot soccer dribbling skill. FCPortugal project enabled us to publish more than 100 papers and win several competitions in different leagues and many scientific awards at RoboCup. In total, our team won more than 40 awards in international competitions including a clear victory at the Simulation 3D League at RoboCup 2022 competition, scoring 84 goals and conceding only 2.
<div id='section'>Paperid: <span id='pid'>620, <a href='https://arxiv.org/pdf/2310.08599.pdf' target='_blank'>https://arxiv.org/pdf/2310.08599.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hoang-Dung Bui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.08599">A Survey of Multi-Robot Motion Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-robot Motion Planning (MRMP) is an active research field which has gained attention over the years. MRMP has significant roles to improve the efficiency and reliability of multi-robot system in a wide range of applications from delivery robots to collaborative assembly lines. This survey provides an overview of MRMP taxonomy, state-of-the-art algorithms, and approaches which have been developed for multi-robot systems. This study also discusses the strengths and limitations of each algorithm and their applications in various scenarios. Moreover, based on this, we can draw out open problems for future research.
<div id='section'>Paperid: <span id='pid'>621, <a href='https://arxiv.org/pdf/2306.01270.pdf' target='_blank'>https://arxiv.org/pdf/2306.01270.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaoming Peng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.01270">Multi-Robot Path Planning Combining Heuristics and Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-robot path finding in dynamic environments is a highly challenging classic problem. In the movement process, robots need to avoid collisions with other moving robots while minimizing their travel distance. Previous methods for this problem either continuously replan paths using heuristic search methods to avoid conflicts or choose appropriate collision avoidance strategies based on learning approaches. The former may result in long travel distances due to frequent replanning, while the latter may have low learning efficiency due to low sample exploration and utilization, and causing high training costs for the model. To address these issues, we propose a path planning method, MAPPOHR, which combines heuristic search, empirical rules, and multi-agent reinforcement learning. The method consists of two layers: a real-time planner based on the multi-agent reinforcement learning algorithm, MAPPO, which embeds empirical rules in the action output layer and reward functions, and a heuristic search planner used to create a global guiding path. During movement, the heuristic search planner replans new paths based on the instructions of the real-time planner. We tested our method in 10 different conflict scenarios. The experiments show that the planning performance of MAPPOHR is better than that of existing learning and heuristic methods. Due to the utilization of empirical knowledge and heuristic search, the learning efficiency of MAPPOHR is higher than that of existing learning methods.
<div id='section'>Paperid: <span id='pid'>622, <a href='https://arxiv.org/pdf/2304.05147.pdf' target='_blank'>https://arxiv.org/pdf/2304.05147.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Roberto Casadei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.05147">Artificial Collective Intelligence Engineering: a Survey of Concepts and Perspectives</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Collectiveness is an important property of many systems--both natural and artificial. By exploiting a large number of individuals, it is often possible to produce effects that go far beyond the capabilities of the smartest individuals, or even to produce intelligent collective behaviour out of not-so-intelligent individuals. Indeed, collective intelligence, namely the capability of a group to act collectively in a seemingly intelligent way, is increasingly often a design goal of engineered computational systems--motivated by recent techno-scientific trends like the Internet of Things, swarm robotics, and crowd computing, just to name a few. For several years, the collective intelligence observed in natural and artificial systems has served as a source of inspiration for engineering ideas, models, and mechanisms. Today, artificial and computational collective intelligence are recognised research topics, spanning various techniques, kinds of target systems, and application domains. However, there is still a lot of fragmentation in the research panorama of the topic within computer science, and the verticality of most communities and contributions makes it difficult to extract the core underlying ideas and frames of reference. The challenge is to identify, place in a common structure, and ultimately connect the different areas and methods addressing intelligent collectives. To address this gap, this paper considers a set of broad scoping questions providing a map of collective intelligence research, mostly by the point of view of computer scientists and engineers. Accordingly, it covers preliminary notions, fundamental concepts, and the main research perspectives, identifying opportunities and challenges for researchers on artificial and computational collective intelligence engineering.
<div id='section'>Paperid: <span id='pid'>623, <a href='https://arxiv.org/pdf/2302.06360.pdf' target='_blank'>https://arxiv.org/pdf/2302.06360.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>B. Udugama
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.06360">Review on Efficient Strategies for Coordinated Motion and Tracking in Swarm Robotics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Swarm robotics is a creative method of organizing multi-robot structures, consisting of many basic robots influenced by communal insects. The greatest astonishing attribute of swarm robots is their capacity to function together to accomplish a collective objective. This paper addresses the list of current surveys, problems and algorithms that were stimulated in the research of Coordinated Movement in Swarm robotics. Algorithms for swarm robotics movement are contrasted, considering the swarm micro-robots to accomplish aggregation, creation, and clamouring by contrasting the relative computational simulations between the algorithms and simulations used.
